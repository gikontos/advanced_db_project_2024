{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c8305e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5808983b-dfce-4a68-b7bb-ee7a9589dd06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adult', 121093), ('young adult', 38703), ('child', 10830), ('old', 5985)]\n",
      "Time taken: 28.52 seconds"
     ]
    }
   ],
   "source": [
    "###query 1 RDD\n",
    "\n",
    "import csv,time\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 RDD\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    # Use StringIO to treat the line as a file-like object\n",
    "    f = StringIO(line)\n",
    "    # Use csv.reader to correctly parse the CSV line\n",
    "    reader = csv.reader(f)\n",
    "    return next(reader) \n",
    "def help1(data):\n",
    "    try:\n",
    "        age=int(data)\n",
    "        if age<18 and age>0:\n",
    "            return \"child\"\n",
    "        if age<25:\n",
    "            return \"young adult\"\n",
    "        if age<65 :\n",
    "            return \"adult\"\n",
    "        if age>64:\n",
    "            return \"old\"\n",
    "        else:\n",
    "            return \"no individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "rdd1  = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\\\n",
    ".map(parse_csv_line)\n",
    "\n",
    "rdd2= sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\\\n",
    ".map(parse_csv_line)\n",
    "\n",
    "header1 = rdd1.first()\n",
    "header2 = rdd2.first()\n",
    "\n",
    "# Filter out the header\n",
    "rdd1_data = rdd1.filter(lambda line: line != header1)\n",
    "rdd2_data = rdd2.filter(lambda line: line != header2)\n",
    "\n",
    "crime_data = rdd1_data.union(rdd2_data) \\\n",
    ".filter(lambda pair: pair[9].find(\"AGGRAVATED\") != -1 ) \\\n",
    ".map(lambda data: (help1(data[11]),1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".sortBy( lambda pair : pair[1], ascending=False )\n",
    "\n",
    "print(crime_data.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e904bbc1-6e1b-46bc-b8ce-705efc077b46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|  age_group| count|\n",
      "+-----------+------+\n",
      "|      adult|121093|\n",
      "|young adult| 38703|\n",
      "|      child| 10830|\n",
      "|        old|  5985|\n",
      "+-----------+------+\n",
      "\n",
      "Time taken: 2.37 seconds"
     ]
    }
   ],
   "source": [
    "####query 1 dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 Dataframe\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "\n",
    "def age_group(age_str):\n",
    "    try:\n",
    "        age=int(age_str)\n",
    "        if age<18 and age>0:\n",
    "            return \"child\"\n",
    "        if age<25:\n",
    "            return \"young adult\"\n",
    "        if age<65 :\n",
    "            return \"adult\"\n",
    "        if age>64:\n",
    "            return \"old\"\n",
    "        else:\n",
    "            return \"no individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "age_udf=udf(age_group,StringType())\n",
    "dataframe=dataframe1.union(dataframe2)\\\n",
    ".filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED\"))\\\n",
    ".withColumn(\"age_group\",age_udf(col(\"Vict Age\")))\\\n",
    ".groupBy(\"age_group\").agg(F.count(\"*\").alias(\"count\"))\\\n",
    ".orderBy(\"count\",ascending=False)\n",
    "\n",
    "\n",
    "dataframe.show()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82388c6c-dc7d-4ed6-b1a9-8ddcc8b9461e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 9.16 seconds"
     ]
    }
   ],
   "source": [
    "####query2 dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 Dataframe\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(F.desc(\"closed_case_rate\"))\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "\n",
    "\n",
    "dataframe=dataframe1.union(dataframe2)\\\n",
    ".withColumn(\"year\",col(\"Date Rptd\").substr(7,4))\\\n",
    ".select(\"year\",\"AREA NAME\",\"Status\")\\\n",
    ".groupBy(\"year\",\"AREA NAME\").agg((count(when(col(\"Status\") != \"IC\", 1)) / count(\"*\") ).alias(\"closed_case_rate\"))\\\n",
    ".withColumn(\"#\", F.row_number().over(window_spec) )\\\n",
    ".filter(col(\"#\") <= 3)\\\n",
    ".withColumnRenamed(\"AREA NAME\", \"precinct\")\n",
    "\n",
    "dataframe.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdae980b-ed68-4d53-b0c1-700edc75a0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 18.17 seconds"
     ]
    }
   ],
   "source": [
    "###query2 spark sql api\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "dataframe=dataframe1.union(dataframe2)\n",
    "\n",
    "dataframe.createOrReplaceTempView(\"Dataset\")\n",
    "query= \"\"\"\n",
    "    WITH extracted_data AS (\n",
    "        SELECT \n",
    "            substr(`Date Rptd`, 7, 4) AS year,\n",
    "            `AREA NAME` AS precinct,\n",
    "            Status\n",
    "        FROM Dataset\n",
    "    ),\n",
    "    aggregated_data AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            COUNT(CASE WHEN Status != 'IC' THEN 1 END)  / COUNT(*) AS closed_case_rate\n",
    "        FROM extracted_data\n",
    "        GROUP BY year, precinct\n",
    "    ),\n",
    "    ranked_data AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS `#`\n",
    "        FROM aggregated_data\n",
    "    )\n",
    "    SELECT \n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        `#`\n",
    "    FROM ranked_data\n",
    "    WHERE `#` <= 3\n",
    "\"\"\"\n",
    "res=spark.sql(query)\n",
    "res.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a65462-35a1-4add-9f18-eeab06075f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####2b\n",
    "####make parquet dataset\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2b write parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "dataframe=dataframe1.union(dataframe2)\n",
    "\n",
    "dataframe.coalesce(1).write.mode(\"overwrite\").parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") ##coalesce gia 1 file\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e259e26a-6aae-4f07-96f7-0d6614118af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "|2018|   Foothill|  0.307089506550753|  1|\n",
      "|2018|    Mission|0.30690661478599224|  2|\n",
      "|2018|   Van Nuys|0.29078685730517945|  3|\n",
      "|2019|West Valley| 0.3077447195094254|  1|\n",
      "|2019|    Mission|0.30748519116855144|  2|\n",
      "|2019|   Foothill| 0.2953842186694172|  3|\n",
      "|2020|West Valley|0.31144886009717204|  1|\n",
      "|2020|    Mission| 0.3038777908343126|  2|\n",
      "|2020|     Harbor| 0.2988065750956992|  3|\n",
      "|2021|    Mission| 0.3091391367253436|  1|\n",
      "|2021|West Valley| 0.2887750349324639|  2|\n",
      "|2021|   Foothill|0.28464788732394364|  3|\n",
      "|2022|West Valley| 0.2664366494153728|  1|\n",
      "|2022|     Harbor| 0.2633405639913232|  2|\n",
      "|2022|    Topanga| 0.2627340236376948|  3|\n",
      "|2023|   Foothill|  0.268215994531784|  1|\n",
      "|2023|    Topanga|0.26407806464728606|  2|\n",
      "|2023|    Mission|0.25941195616795054|  3|\n",
      "|2024|N Hollywood|0.19514978601997146|  1|\n",
      "|2024|   Foothill|0.18531827515400412|  2|\n",
      "|2024|77th Street|0.17349137931034483|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "\n",
      "Time taken: 1.76 seconds"
     ]
    }
   ],
   "source": [
    "####2b test parquet file on 2a query dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2b test parquet for 2b Dataframe\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\n",
    "\n",
    "dataframe=dataframe.withColumn(\"year\",col(\"Date Rptd\").substr(7,4))\\\n",
    ".select(\"year\",\"AREA NAME\",\"Status\")\\\n",
    ".groupBy(\"year\",\"AREA NAME\").agg((count(when(col(\"Status\") != \"IC\", 1)) / count(\"*\") ).alias(\"closed_case_rate\"))\\\n",
    ".withColumn(\"#\", F.row_number().over(window_spec) )\\\n",
    ".filter(col(\"#\") <= 3)\\\n",
    ".withColumnRenamed(\"AREA NAME\", \"precinct\")\n",
    "\n",
    "dataframe.show(60)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb0b5b8-5e92-4bcd-b15a-1d04537696c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.19 seconds\n",
      "+-------------+-----------------+--------------------+\n",
      "|         COMM|   Average Income|   Crimes per Person|\n",
      "+-------------+-----------------+--------------------+\n",
      "| Elysian Park|36510.13935826846|0.005002928735845...|\n",
      "|  Pico Rivera|55762.81231927806|0.009708737864077669|\n",
      "|Green Meadows|30576.59351193179|0.008199889186064806|\n",
      "+-------------+-----------------+--------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "####query 3\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "census=blocks_census.select(\"COMM\",\"POP_2010\",\"ZCTA10\").na.fill({\"POP_2010\": 0}).groupBy(\"COMM\",\"ZCTA10\").agg(F.sum(\"POP_2010\").alias(\"POP_2010\"))\n",
    "\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"POP_2010\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry))\\\n",
    ".select(\"COMM\",\"POP_2010\")\\\n",
    ".groupBy(\"COMM\")\\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2,res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557e63b7-6621-4057-8df8-350d6b94a546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [COMM#2619 AS COMM#3085, 'Average Income, 'Crimes per Person]\n",
      "+- Join Inner, (COMM#2619 = COMM#3050)\n",
      "   :- Project [COMM#2619, total_population#2815L, comm_total_income#2817, (comm_total_income#2817 / cast(total_population#2815L as double)) AS Average Income#2821]\n",
      "   :  +- Aggregate [COMM#2619], [COMM#2619, sum(POP_2010#2754L) AS total_population#2815L, sum(total_income#2799) AS comm_total_income#2817]\n",
      "   :     +- Project [Zip Code#2776, Community#2777, Estimated Median Income#2782, COMM#2619, ZCTA10#2636, POP_2010#2754L, (Estimated Median Income#2782 * cast(POP_2010#2754L as float)) AS total_income#2799]\n",
      "   :        +- Join Inner, (ZCTA10#2636 = Zip Code#2776)\n",
      "   :           :- Project [Zip Code#2776, Community#2777, cast(regexp_replace(Estimated Median Income#2778, [$,], , 1) as float) AS Estimated Median Income#2782]\n",
      "   :           :  +- Relation [Zip Code#2776,Community#2777,Estimated Median Income#2778] csv\n",
      "   :           +- Aggregate [COMM#2619, ZCTA10#2636], [COMM#2619, ZCTA10#2636, sum(POP_2010#2746L) AS POP_2010#2754L]\n",
      "   :              +- Project [COMM#2619, coalesce(POP_2010#2628L, cast(0 as bigint)) AS POP_2010#2746L, ZCTA10#2636]\n",
      "   :                 +- Project [COMM#2619, POP_2010#2628L, ZCTA10#2636]\n",
      "   :                    +- Project [properties#2607.BG10 AS BG10#2612, properties#2607.BG10FIP10 AS BG10FIP10#2613, properties#2607.BG12 AS BG12#2614, properties#2607.CB10 AS CB10#2615, properties#2607.CEN_FIP13 AS CEN_FIP13#2616, properties#2607.CITY AS CITY#2617, properties#2607.CITYCOM AS CITYCOM#2618, properties#2607.COMM AS COMM#2619, properties#2607.CT10 AS CT10#2620, properties#2607.CT12 AS CT12#2621, properties#2607.CTCB10 AS CTCB10#2622, properties#2607.HD_2012 AS HD_2012#2623L, properties#2607.HD_NAME AS HD_NAME#2624, properties#2607.HOUSING10 AS HOUSING10#2625L, properties#2607.LA_FIP10 AS LA_FIP10#2626, properties#2607.OBJECTID AS OBJECTID#2627L, properties#2607.POP_2010 AS POP_2010#2628L, properties#2607.PUMA10 AS PUMA10#2629, properties#2607.SPA_2012 AS SPA_2012#2630L, properties#2607.SPA_NAME AS SPA_NAME#2631, properties#2607.SUP_DIST AS SUP_DIST#2632, properties#2607.SUP_LABEL AS SUP_LABEL#2633, properties#2607.ShapeSTArea AS ShapeSTArea#2634, properties#2607.ShapeSTLength AS ShapeSTLength#2635, ... 2 more fields]\n",
      "   :                       +- Project [features#2603.geometry AS geometry#2606, features#2603.properties AS properties#2607, features#2603.type AS type#2608]\n",
      "   :                          +- Project [features#2603]\n",
      "   :                             +- Generate explode(features#2595), false, [features#2603]\n",
      "   :                                +- Relation [crs#2594,features#2595,name#2596,type#2597] geojson\n",
      "   +- Project [COMM#3050, TotalPopulation#3028L, NumberOfCrimes#3030L, (cast(NumberOfCrimes#3030L as double) / cast(TotalPopulation#3028L as double)) AS Crimes per Person#3034]\n",
      "      +- Aggregate [COMM#3050], [COMM#3050, sum(POP_2010#3059L) AS TotalPopulation#3028L, count(1) AS NumberOfCrimes#3030L]\n",
      "         +- Project [COMM#3050, POP_2010#3059L]\n",
      "            +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "               :- Project [DR_NO#2826, Date Rptd#2827, DATE OCC#2828, TIME OCC#2829, AREA #2830, AREA NAME#2831, Rpt Dist No#2832, Part 1-2#2833, Crm Cd#2834, Crm Cd Desc#2835, Mocodes#2836, Vict Age#2837, Vict Sex#2838, Vict Descent#2839, Premis Cd#2840, Premis Desc#2841, Weapon Used Cd#2842, Weapon Desc#2843, Status#2844, Status Desc#2845, Crm Cd 1#2846, Crm Cd 2#2847, Crm Cd 3#2848, Crm Cd 4#2849, ... 5 more fields]\n",
      "               :  +- Relation [DR_NO#2826,Date Rptd#2827,DATE OCC#2828,TIME OCC#2829,AREA #2830,AREA NAME#2831,Rpt Dist No#2832,Part 1-2#2833,Crm Cd#2834,Crm Cd Desc#2835,Mocodes#2836,Vict Age#2837,Vict Sex#2838,Vict Descent#2839,Premis Cd#2840,Premis Desc#2841,Weapon Used Cd#2842,Weapon Desc#2843,Status#2844,Status Desc#2845,Crm Cd 1#2846,Crm Cd 2#2847,Crm Cd 3#2848,Crm Cd 4#2849,... 4 more fields] parquet\n",
      "               +- Project [properties#2607.BG10 AS BG10#3043, properties#2607.BG10FIP10 AS BG10FIP10#3044, properties#2607.BG12 AS BG12#3045, properties#2607.CB10 AS CB10#3046, properties#2607.CEN_FIP13 AS CEN_FIP13#3047, properties#2607.CITY AS CITY#3048, properties#2607.CITYCOM AS CITYCOM#3049, properties#2607.COMM AS COMM#3050, properties#2607.CT10 AS CT10#3051, properties#2607.CT12 AS CT12#3052, properties#2607.CTCB10 AS CTCB10#3053, properties#2607.HD_2012 AS HD_2012#3054L, properties#2607.HD_NAME AS HD_NAME#3055, properties#2607.HOUSING10 AS HOUSING10#3056L, properties#2607.LA_FIP10 AS LA_FIP10#3057, properties#2607.OBJECTID AS OBJECTID#3058L, properties#2607.POP_2010 AS POP_2010#3059L, properties#2607.PUMA10 AS PUMA10#3060, properties#2607.SPA_2012 AS SPA_2012#3061L, properties#2607.SPA_NAME AS SPA_NAME#3062, properties#2607.SUP_DIST AS SUP_DIST#3063, properties#2607.SUP_LABEL AS SUP_LABEL#3064, properties#2607.ShapeSTArea AS ShapeSTArea#3065, properties#2607.ShapeSTLength AS ShapeSTLength#3066, ... 2 more fields]\n",
      "                  +- Project [features#2603.geometry AS geometry#2606, features#2603.properties AS properties#2607, features#2603.type AS type#2608]\n",
      "                     +- Project [features#2603]\n",
      "                        +- Generate explode(features#3040), false, [features#2603]\n",
      "                           +- Relation [crs#3039,features#3040,name#3041,type#3042] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Average Income: double, Crimes per Person: double\n",
      "Project [COMM#2619 AS COMM#3085, Average Income#2821, Crimes per Person#3034]\n",
      "+- Join Inner, (COMM#2619 = COMM#3050)\n",
      "   :- Project [COMM#2619, total_population#2815L, comm_total_income#2817, (comm_total_income#2817 / cast(total_population#2815L as double)) AS Average Income#2821]\n",
      "   :  +- Aggregate [COMM#2619], [COMM#2619, sum(POP_2010#2754L) AS total_population#2815L, sum(total_income#2799) AS comm_total_income#2817]\n",
      "   :     +- Project [Zip Code#2776, Community#2777, Estimated Median Income#2782, COMM#2619, ZCTA10#2636, POP_2010#2754L, (Estimated Median Income#2782 * cast(POP_2010#2754L as float)) AS total_income#2799]\n",
      "   :        +- Join Inner, (ZCTA10#2636 = Zip Code#2776)\n",
      "   :           :- Project [Zip Code#2776, Community#2777, cast(regexp_replace(Estimated Median Income#2778, [$,], , 1) as float) AS Estimated Median Income#2782]\n",
      "   :           :  +- Relation [Zip Code#2776,Community#2777,Estimated Median Income#2778] csv\n",
      "   :           +- Aggregate [COMM#2619, ZCTA10#2636], [COMM#2619, ZCTA10#2636, sum(POP_2010#2746L) AS POP_2010#2754L]\n",
      "   :              +- Project [COMM#2619, coalesce(POP_2010#2628L, cast(0 as bigint)) AS POP_2010#2746L, ZCTA10#2636]\n",
      "   :                 +- Project [COMM#2619, POP_2010#2628L, ZCTA10#2636]\n",
      "   :                    +- Project [properties#2607.BG10 AS BG10#2612, properties#2607.BG10FIP10 AS BG10FIP10#2613, properties#2607.BG12 AS BG12#2614, properties#2607.CB10 AS CB10#2615, properties#2607.CEN_FIP13 AS CEN_FIP13#2616, properties#2607.CITY AS CITY#2617, properties#2607.CITYCOM AS CITYCOM#2618, properties#2607.COMM AS COMM#2619, properties#2607.CT10 AS CT10#2620, properties#2607.CT12 AS CT12#2621, properties#2607.CTCB10 AS CTCB10#2622, properties#2607.HD_2012 AS HD_2012#2623L, properties#2607.HD_NAME AS HD_NAME#2624, properties#2607.HOUSING10 AS HOUSING10#2625L, properties#2607.LA_FIP10 AS LA_FIP10#2626, properties#2607.OBJECTID AS OBJECTID#2627L, properties#2607.POP_2010 AS POP_2010#2628L, properties#2607.PUMA10 AS PUMA10#2629, properties#2607.SPA_2012 AS SPA_2012#2630L, properties#2607.SPA_NAME AS SPA_NAME#2631, properties#2607.SUP_DIST AS SUP_DIST#2632, properties#2607.SUP_LABEL AS SUP_LABEL#2633, properties#2607.ShapeSTArea AS ShapeSTArea#2634, properties#2607.ShapeSTLength AS ShapeSTLength#2635, ... 2 more fields]\n",
      "   :                       +- Project [features#2603.geometry AS geometry#2606, features#2603.properties AS properties#2607, features#2603.type AS type#2608]\n",
      "   :                          +- Project [features#2603]\n",
      "   :                             +- Generate explode(features#2595), false, [features#2603]\n",
      "   :                                +- Relation [crs#2594,features#2595,name#2596,type#2597] geojson\n",
      "   +- Project [COMM#3050, TotalPopulation#3028L, NumberOfCrimes#3030L, (cast(NumberOfCrimes#3030L as double) / cast(TotalPopulation#3028L as double)) AS Crimes per Person#3034]\n",
      "      +- Aggregate [COMM#3050], [COMM#3050, sum(POP_2010#3059L) AS TotalPopulation#3028L, count(1) AS NumberOfCrimes#3030L]\n",
      "         +- Project [COMM#3050, POP_2010#3059L]\n",
      "            +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "               :- Project [DR_NO#2826, Date Rptd#2827, DATE OCC#2828, TIME OCC#2829, AREA #2830, AREA NAME#2831, Rpt Dist No#2832, Part 1-2#2833, Crm Cd#2834, Crm Cd Desc#2835, Mocodes#2836, Vict Age#2837, Vict Sex#2838, Vict Descent#2839, Premis Cd#2840, Premis Desc#2841, Weapon Used Cd#2842, Weapon Desc#2843, Status#2844, Status Desc#2845, Crm Cd 1#2846, Crm Cd 2#2847, Crm Cd 3#2848, Crm Cd 4#2849, ... 5 more fields]\n",
      "               :  +- Relation [DR_NO#2826,Date Rptd#2827,DATE OCC#2828,TIME OCC#2829,AREA #2830,AREA NAME#2831,Rpt Dist No#2832,Part 1-2#2833,Crm Cd#2834,Crm Cd Desc#2835,Mocodes#2836,Vict Age#2837,Vict Sex#2838,Vict Descent#2839,Premis Cd#2840,Premis Desc#2841,Weapon Used Cd#2842,Weapon Desc#2843,Status#2844,Status Desc#2845,Crm Cd 1#2846,Crm Cd 2#2847,Crm Cd 3#2848,Crm Cd 4#2849,... 4 more fields] parquet\n",
      "               +- Project [properties#2607.BG10 AS BG10#3043, properties#2607.BG10FIP10 AS BG10FIP10#3044, properties#2607.BG12 AS BG12#3045, properties#2607.CB10 AS CB10#3046, properties#2607.CEN_FIP13 AS CEN_FIP13#3047, properties#2607.CITY AS CITY#3048, properties#2607.CITYCOM AS CITYCOM#3049, properties#2607.COMM AS COMM#3050, properties#2607.CT10 AS CT10#3051, properties#2607.CT12 AS CT12#3052, properties#2607.CTCB10 AS CTCB10#3053, properties#2607.HD_2012 AS HD_2012#3054L, properties#2607.HD_NAME AS HD_NAME#3055, properties#2607.HOUSING10 AS HOUSING10#3056L, properties#2607.LA_FIP10 AS LA_FIP10#3057, properties#2607.OBJECTID AS OBJECTID#3058L, properties#2607.POP_2010 AS POP_2010#3059L, properties#2607.PUMA10 AS PUMA10#3060, properties#2607.SPA_2012 AS SPA_2012#3061L, properties#2607.SPA_NAME AS SPA_NAME#3062, properties#2607.SUP_DIST AS SUP_DIST#3063, properties#2607.SUP_LABEL AS SUP_LABEL#3064, properties#2607.ShapeSTArea AS ShapeSTArea#3065, properties#2607.ShapeSTLength AS ShapeSTLength#3066, ... 2 more fields]\n",
      "                  +- Project [features#2603.geometry AS geometry#2606, features#2603.properties AS properties#2607, features#2603.type AS type#2608]\n",
      "                     +- Project [features#2603]\n",
      "                        +- Generate explode(features#3040), false, [features#2603]\n",
      "                           +- Relation [crs#3039,features#3040,name#3041,type#3042] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#2619, Average Income#2821, Crimes per Person#3034]\n",
      "+- Join Inner, (COMM#2619 = COMM#3050)\n",
      "   :- Aggregate [COMM#2619], [COMM#2619, (sum(total_income#2799) / cast(sum(POP_2010#2754L) as double)) AS Average Income#2821]\n",
      "   :  +- Project [COMM#2619, POP_2010#2754L, (Estimated Median Income#2782 * cast(POP_2010#2754L as float)) AS total_income#2799]\n",
      "   :     +- Join Inner, (ZCTA10#2636 = Zip Code#2776)\n",
      "   :        :- Project [Zip Code#2776, cast(regexp_replace(Estimated Median Income#2778, [$,], , 1) as float) AS Estimated Median Income#2782]\n",
      "   :        :  +- Filter isnotnull(Zip Code#2776)\n",
      "   :        :     +- Relation [Zip Code#2776,Community#2777,Estimated Median Income#2778] csv\n",
      "   :        +- Aggregate [COMM#2619, ZCTA10#2636], [COMM#2619, ZCTA10#2636, sum(POP_2010#2746L) AS POP_2010#2754L]\n",
      "   :           +- Project [features#2603.properties.COMM AS COMM#2619, coalesce(features#2603.properties.POP_2010, 0) AS POP_2010#2746L, features#2603.properties.ZCTA10 AS ZCTA10#2636]\n",
      "   :              +- Filter (isnotnull(features#2603.properties.ZCTA10) AND isnotnull(features#2603.properties.COMM))\n",
      "   :                 +- Generate explode(features#2595), [0], false, [features#2603]\n",
      "   :                    +- Project [features#2595]\n",
      "   :                       +- Filter ((size(features#2595, true) > 0) AND isnotnull(features#2595))\n",
      "   :                          +- Relation [crs#2594,features#2595,name#2596,type#2597] geojson\n",
      "   +- Aggregate [COMM#3050], [COMM#3050, (cast(count(1) as double) / cast(sum(POP_2010#3059L) as double)) AS Crimes per Person#3034]\n",
      "      +- Project [COMM#3050, POP_2010#3059L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#2882]\n",
      "            :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "            :     +- Relation [DR_NO#2826,Date Rptd#2827,DATE OCC#2828,TIME OCC#2829,AREA #2830,AREA NAME#2831,Rpt Dist No#2832,Part 1-2#2833,Crm Cd#2834,Crm Cd Desc#2835,Mocodes#2836,Vict Age#2837,Vict Sex#2838,Vict Descent#2839,Premis Cd#2840,Premis Desc#2841,Weapon Used Cd#2842,Weapon Desc#2843,Status#2844,Status Desc#2845,Crm Cd 1#2846,Crm Cd 2#2847,Crm Cd 3#2848,Crm Cd 4#2849,... 4 more fields] parquet\n",
      "            +- Project [features#2603.properties.COMM AS COMM#3050, features#2603.properties.POP_2010 AS POP_2010#3059L, features#2603.geometry AS geometry#2606]\n",
      "               +- Filter ((isnotnull(features#2603.geometry) AND isnotnull(features#2603.properties.COMM)) AND bloomfilter#3152 of [COMM#2619] filtering [features#2603.properties.COMM])\n",
      "                  :  +- Aggregate [COMM#2619, ZCTA10#2636], [COMM#2619, ZCTA10#2636, sum(POP_2010#2746L) AS POP_2010#2754L]\n",
      "                  :     +- Project [features#2603.properties.COMM AS COMM#2619, coalesce(features#2603.properties.POP_2010, 0) AS POP_2010#2746L, features#2603.properties.ZCTA10 AS ZCTA10#2636]\n",
      "                  :        +- Filter (isnotnull(features#2603.properties.ZCTA10) AND isnotnull(features#2603.properties.COMM))\n",
      "                  :           +- Generate explode(features#2595), [0], false, [features#2603]\n",
      "                  :              +- Project [features#2595]\n",
      "                  :                 +- Filter ((size(features#2595, true) > 0) AND isnotnull(features#2595))\n",
      "                  :                    +- Relation [crs#2594,features#2595,name#2596,type#2597] geojson\n",
      "                  +- Generate explode(features#3040), [0], false, [features#2603]\n",
      "                     +- Project [features#3040]\n",
      "                        +- Filter ((size(features#3040, true) > 0) AND isnotnull(features#3040))\n",
      "                           +- Relation [crs#3039,features#3040,name#3041,type#3042] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#2619, Average Income#2821, Crimes per Person#3034]\n",
      "   +- SortMergeJoin [COMM#2619], [COMM#3050], Inner\n",
      "      :- Sort [COMM#2619 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#2619], functions=[sum(total_income#2799), sum(POP_2010#2754L)], output=[COMM#2619, Average Income#2821], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#2619, 1000), ENSURE_REQUIREMENTS, [plan_id=1976]\n",
      "      :        +- HashAggregate(keys=[COMM#2619], functions=[partial_sum(total_income#2799), partial_sum(POP_2010#2754L)], output=[COMM#2619, sum#3100, sum#3102L], schema specialized)\n",
      "      :           +- Project [COMM#2619, POP_2010#2754L, (Estimated Median Income#2782 * cast(POP_2010#2754L as float)) AS total_income#2799]\n",
      "      :              +- BroadcastHashJoin [Zip Code#2776], [ZCTA10#2636], Inner, BuildLeft, false\n",
      "      :                 :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1971]\n",
      "      :                 :  +- Project [Zip Code#2776, cast(regexp_replace(Estimated Median Income#2778, [$,], , 1) as float) AS Estimated Median Income#2782]\n",
      "      :                 :     +- Filter isnotnull(Zip Code#2776)\n",
      "      :                 :        +- FileScan csv [Zip Code#2776,Estimated Median Income#2778] Batched: false, DataFilters: [isnotnull(Zip Code#2776)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                 +- HashAggregate(keys=[COMM#2619, ZCTA10#2636], functions=[sum(POP_2010#2746L)], output=[COMM#2619, ZCTA10#2636, POP_2010#2754L], schema specialized)\n",
      "      :                    +- Exchange hashpartitioning(COMM#2619, ZCTA10#2636, 1000), ENSURE_REQUIREMENTS, [plan_id=1968]\n",
      "      :                       +- HashAggregate(keys=[COMM#2619, ZCTA10#2636], functions=[partial_sum(POP_2010#2746L)], output=[COMM#2619, ZCTA10#2636, sum#3104L], schema specialized)\n",
      "      :                          +- Project [features#2603.properties.COMM AS COMM#2619, coalesce(features#2603.properties.POP_2010, 0) AS POP_2010#2746L, features#2603.properties.ZCTA10 AS ZCTA10#2636]\n",
      "      :                             +- Filter (isnotnull(features#2603.properties.ZCTA10) AND isnotnull(features#2603.properties.COMM))\n",
      "      :                                +- Generate explode(features#2595), false, [features#2603]\n",
      "      :                                   +- Filter ((size(features#2595, true) > 0) AND isnotnull(features#2595))\n",
      "      :                                      +- FileScan geojson [features#2595] Batched: false, DataFilters: [(size(features#2595, true) > 0), isnotnull(features#2595)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- Sort [COMM#3050 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#3050], functions=[count(1), sum(POP_2010#3059L)], output=[COMM#3050, Crimes per Person#3034], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#3050, 1000), ENSURE_REQUIREMENTS, [plan_id=2064]\n",
      "               +- HashAggregate(keys=[COMM#3050], functions=[partial_count(1), partial_sum(POP_2010#3059L)], output=[COMM#3050, count#3106L, sum#3108L], schema specialized)\n",
      "                  +- Project [COMM#3050, POP_2010#3059L]\n",
      "                     +- RangeJoin point#2882: geometry, geometry#2606: geometry, WITHIN\n",
      "                        :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#2882]\n",
      "                        :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "                        :     +- FileScan parquet [LAT#2852,LON#2853] Batched: true, DataFilters: [isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                        +- Project [features#2603.properties.COMM AS COMM#3050, features#2603.properties.POP_2010 AS POP_2010#3059L, features#2603.geometry AS geometry#2606]\n",
      "                           +- Filter ((isnotnull(features#2603.geometry) AND isnotnull(features#2603.properties.COMM)) AND bloomfilter#3152 of [bf3152 COMM#2619 estimatedNumRows=294857] filtering [features#2603.properties.COMM])\n",
      "                              :  +- GenerateBloomFilter bf3152, 294857, 0, false, false, false, true, [id=#2058]\n",
      "                              :     +- OutputAdapter [COMM#2619, ZCTA10#2636, sum#3104L]\n",
      "                              :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                              :           +- Exchange hashpartitioning(COMM#2619, ZCTA10#2636, 1000), ENSURE_REQUIREMENTS, [plan_id=2055]\n",
      "                              :              +- HashAggregate(keys=[COMM#2619, ZCTA10#2636], functions=[partial_sum(POP_2010#2746L)], output=[COMM#2619, ZCTA10#2636, sum#3104L], schema specialized)\n",
      "                              :                 +- Project [features#2603.properties.COMM AS COMM#2619, coalesce(features#2603.properties.POP_2010, 0) AS POP_2010#2746L, features#2603.properties.ZCTA10 AS ZCTA10#2636]\n",
      "                              :                    +- Filter (isnotnull(features#2603.properties.ZCTA10) AND isnotnull(features#2603.properties.COMM))\n",
      "                              :                       +- Generate explode(features#2595), false, [features#2603]\n",
      "                              :                          +- Filter ((size(features#2595, true) > 0) AND isnotnull(features#2595))\n",
      "                              :                             +- FileScan geojson [features#2595] Batched: false, DataFilters: [(size(features#2595, true) > 0), isnotnull(features#2595)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                              +- Generate explode(features#3040), false, [features#2603]\n",
      "                                 +- Filter ((size(features#3040, true) > 0) AND isnotnull(features#3040))\n",
      "                                    +- FileScan geojson [features#3040] Batched: false, DataFilters: [(size(features#3040, true) > 0), isnotnull(features#3040)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "res.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b40808-f476-433a-a9d2-66250b794c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
