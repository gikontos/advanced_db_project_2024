{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c8305e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adult', 121093), ('young adult', 38703), ('child', 10830), ('old', 5985)]\n",
      "Time taken: 22.19 seconds"
     ]
    }
   ],
   "source": [
    "# Query 1 RDD\n",
    "\n",
    "import csv,time\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 RDD\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    f = StringIO(line)\n",
    "    reader = csv.reader(f)\n",
    "    return next(reader) \n",
    "\n",
    "def help1(data):\n",
    "    try:\n",
    "        age=int(data)\n",
    "        if age<18 and age>0:\n",
    "            return \"child\"\n",
    "        if age<25:\n",
    "            return \"young adult\"\n",
    "        if age<65 :\n",
    "            return \"adult\"\n",
    "        if age>64:\n",
    "            return \"old\"\n",
    "        else:\n",
    "            return \"no individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "rdd1  = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\\\n",
    ".map(parse_csv_line)\n",
    "\n",
    "rdd2= sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\\\n",
    ".map(parse_csv_line)\n",
    "\n",
    "header1 = rdd1.first()\n",
    "header2 = rdd2.first()\n",
    "\n",
    "# Filter out the header\n",
    "rdd1_data = rdd1.filter(lambda line: line != header1)\n",
    "rdd2_data = rdd2.filter(lambda line: line != header2)\n",
    "\n",
    "crime_data = rdd1_data.union(rdd2_data) \\\n",
    ".filter(lambda pair: pair[9].find(\"AGGRAVATED\") != -1 ) \\\n",
    ".map(lambda data: (help1(data[11]), 1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".sortBy( lambda pair : pair[1], ascending=False )\n",
    "\n",
    "print(crime_data.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59e0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|  age_group| count|\n",
      "+-----------+------+\n",
      "|      adult|121093|\n",
      "|young adult| 38703|\n",
      "|      child| 10830|\n",
      "|        old|  5985|\n",
      "+-----------+------+\n",
      "\n",
      "Time taken: 11.91 seconds"
     ]
    }
   ],
   "source": [
    "####query 1 dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 Dataframe\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "\n",
    "def age_group(age_str):\n",
    "    try:\n",
    "        age=int(age_str)\n",
    "        if age<18 and age>0:\n",
    "            return \"child\"\n",
    "        if age<25:\n",
    "            return \"young adult\"\n",
    "        if age<65 :\n",
    "            return \"adult\"\n",
    "        if age>64:\n",
    "            return \"old\"\n",
    "        else:\n",
    "            return \"no individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "age_udf=udf(age_group,StringType())\n",
    "dataframe=dataframe1.union(dataframe2)\\\n",
    ".filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED\"))\\\n",
    ".withColumn(\"age_group\",age_udf(col(\"Vict Age\")))\\\n",
    ".groupBy(\"age_group\").agg(F.count(\"*\").alias(\"count\"))\\\n",
    ".orderBy(\"count\",ascending=False)\n",
    "\n",
    "\n",
    "dataframe.show()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45b2cff-3001-4b85-acc0-0b2308ff06e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 4.35 seconds"
     ]
    }
   ],
   "source": [
    "####query2a dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 Dataframe\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(F.desc(\"closed_case_rate\"))\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "\n",
    "\n",
    "dataframe=dataframe1.union(dataframe2)\\\n",
    ".withColumn(\"year\",col(\"Date Rptd\").substr(7,4))\\\n",
    ".select(\"year\",\"AREA NAME\",\"Status\")\\\n",
    ".groupBy(\"year\",\"AREA NAME\").agg((count(when(col(\"Status\") != \"IC\", 1)) / count(\"*\") ).alias(\"closed_case_rate\"))\\\n",
    ".withColumn(\"#\", F.row_number().over(window_spec) )\\\n",
    ".filter(col(\"#\") <= 3)\\\n",
    ".withColumnRenamed(\"AREA NAME\", \"precinct\")\n",
    "\n",
    "dataframe.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee557a1-263a-41f3-8d1b-6d755e3ee646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 3.71 seconds"
     ]
    }
   ],
   "source": [
    "###query2a spark sql api\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "dataframe=dataframe1.union(dataframe2)\n",
    "\n",
    "dataframe.createOrReplaceTempView(\"Dataset\")\n",
    "query= \"\"\"\n",
    "    WITH extracted_data AS (\n",
    "        SELECT \n",
    "            substr(`Date Rptd`, 7, 4) AS year,\n",
    "            `AREA NAME` AS precinct,\n",
    "            Status\n",
    "        FROM Dataset\n",
    "    ),\n",
    "    aggregated_data AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            COUNT(CASE WHEN Status != 'IC' THEN 1 END)  / COUNT(*) AS closed_case_rate\n",
    "        FROM extracted_data\n",
    "        GROUP BY year, precinct\n",
    "    ),\n",
    "    ranked_data AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS `#`\n",
    "        FROM aggregated_data\n",
    "    )\n",
    "    SELECT \n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        `#`\n",
    "    FROM ranked_data\n",
    "    WHERE `#` <= 3\n",
    "\"\"\"\n",
    "res=spark.sql(query)\n",
    "res.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b1466-feb2-4e67-a61f-168b7db74b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####2b\n",
    "####make parquet dataset\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2b write parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "dataframe=dataframe1.union(dataframe2)\n",
    "\n",
    "dataframe.coalesce(1).write.mode(\"overwrite\").parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") ##coalesce gia 1 file\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d0ccd6e-4f07-411a-9b81-fb53e5bd7fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 1.55 seconds"
     ]
    }
   ],
   "source": [
    "####2b test parquet file on 2a query dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2b test parquet for 2b Dataframe\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\n",
    "\n",
    "dataframe=dataframe.withColumn(\"year\",col(\"Date Rptd\").substr(7,4))\\\n",
    ".select(\"year\",\"AREA NAME\",\"Status\")\\\n",
    ".groupBy(\"year\",\"AREA NAME\").agg((count(when(col(\"Status\") != \"IC\", 1)) / count(\"*\") ).alias(\"closed_case_rate\"))\\\n",
    ".withColumn(\"#\", F.row_number().over(window_spec) )\\\n",
    ".filter(col(\"#\") <= 3)\\\n",
    ".withColumnRenamed(\"AREA NAME\", \"precinct\")\n",
    "\n",
    "dataframe.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f419353-a190-4c8f-9714-ac14210c9889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.02 seconds\n",
      "+--------------------+------------------+--------------------+\n",
      "|                COMM|    Average Income|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|      Marina del Rey| 76428.84908639747| 0.16078790655061842|\n",
      "|   Pacific Palisades| 70656.11180545464|  0.4720770986558458|\n",
      "|              Malibu|  67135.0118623962|0.003460207612456...|\n",
      "| Palisades Highlands| 66867.44038612054|  0.2055830941821028|\n",
      "|    Marina Peninsula|65235.692875259396|  0.6549938347718866|\n",
      "|             Bel Air| 63041.33942621959| 0.43199608610567514|\n",
      "|Palos Verdes Estates| 61905.61214466438|0.008547008547008548|\n",
      "|     Manhattan Beach|60985.189241497086|0.033648790746582544|\n",
      "|       Beverly Crest| 60947.48978754819| 0.37490683229813665|\n",
      "|           Brentwood| 60840.62462032012|  0.5346764258279586|\n",
      "|       Hermosa Beach| 57924.85594176151|0.016216216216216217|\n",
      "|   Mandeville Canyon| 55572.11011444479|  0.2716207559256887|\n",
      "|La Cañada Flintridge| 54900.65682110046| 0.01282051282051282|\n",
      "|       Beverly Hills| 51303.11237503298|  0.4119975639464068|\n",
      "|         Playa Vista| 50264.47143177235|  0.8175387379294857|\n",
      "|             Carthay|49841.163007975694|  1.0959014197696222|\n",
      "|      West Hollywood| 49414.42132038722| 0.13344267183539585|\n",
      "|              Venice|47614.884045977014|  1.3570434206165065|\n",
      "|       Redondo Beach| 47568.80637622101|0.050505050505050504|\n",
      "| Rancho Palos Verdes| 46151.89491631246| 0.28346456692913385|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "####query 3 NO HINT!!!!!\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .dropna(subset=[\"COMM\",\"ZCTA10\"])  \\\n",
    "\n",
    "\n",
    "census = blocks_census \\\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"HOUSING10\") \\\n",
    "    .na.fill({\"POP_2010\": 0, \"HOUSING10\": 0}) \\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"TOTAL_POP_2010\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"TOTAL_HOUSING10\")\n",
    "    )\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,.]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"TOTAL_HOUSING10\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"TOTAL_POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    ".select(\"point\")\n",
    "\n",
    "\n",
    "# res2 = crime_dataset \\\n",
    "#     .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry),)\\\n",
    "# .select(\"COMM\",\"POP_2010\")\\\n",
    "# .groupBy(\"COMM\")\\\n",
    "# .agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    "# .withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census.select(\"geometry\",\"COMM\",\"POP_2010\"), ST_Within(crime_dataset.point, blocks_census.geometry)) \\\n",
    ".groupBy(\"geometry\",\"COMM\",\"POP_2010\") \\\n",
    ".agg(F.count(\"*\").alias(\"NumberOfCrimesPerBlock\"))\\\n",
    ".groupby(\"COMM\") \\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.sum(\"NumberOfCrimesPerBlock\").alias(\"NumberOfCrimes\") ) \\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2,res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\").orderBy(col(\"Average Income\").desc())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a920b7-311d-4bba-bb79-ebe5b5ad8c05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Average Income DESC NULLS LAST], true\n",
      "+- Project [COMM#49 AS COMM#465, Average Income#289, Crimes per Person#414]\n",
      "   +- Join Inner, (COMM#49 = COMM#430)\n",
      "      :- Project [COMM#49, total_population#283L, comm_total_income#285, (comm_total_income#285 / cast(total_population#283L as double)) AS Average Income#289]\n",
      "      :  +- Aggregate [COMM#49], [COMM#49, sum(TOTAL_POP_2010#215L) AS total_population#283L, sum(total_income#265) AS comm_total_income#285]\n",
      "      :     +- Project [Zip Code#240, Community#241, Estimated Median Income#246, COMM#49, ZCTA10#66, TOTAL_POP_2010#215L, TOTAL_HOUSING10#217L, (Estimated Median Income#246 * cast(TOTAL_HOUSING10#217L as float)) AS total_income#265]\n",
      "      :        +- Join Inner, (ZCTA10#66 = Zip Code#240)\n",
      "      :           :- Project [Zip Code#240, Community#241, cast(regexp_replace(Estimated Median Income#242, [$,.], , 1) as float) AS Estimated Median Income#246]\n",
      "      :           :  +- Relation [Zip Code#240,Community#241,Estimated Median Income#242] csv\n",
      "      :           +- Aggregate [COMM#49, ZCTA10#66], [COMM#49, ZCTA10#66, sum(POP_2010#204L) AS TOTAL_POP_2010#215L, sum(HOUSING10#205L) AS TOTAL_HOUSING10#217L]\n",
      "      :              +- Project [COMM#49, coalesce(POP_2010#58L, cast(0 as bigint)) AS POP_2010#204L, ZCTA10#66, coalesce(HOUSING10#55L, cast(0 as bigint)) AS HOUSING10#205L]\n",
      "      :                 +- Project [COMM#49, POP_2010#58L, ZCTA10#66, HOUSING10#55L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#49, ZCTA10#66)\n",
      "      :                       +- Project [properties#37.BG10 AS BG10#42, properties#37.BG10FIP10 AS BG10FIP10#43, properties#37.BG12 AS BG12#44, properties#37.CB10 AS CB10#45, properties#37.CEN_FIP13 AS CEN_FIP13#46, properties#37.CITY AS CITY#47, properties#37.CITYCOM AS CITYCOM#48, properties#37.COMM AS COMM#49, properties#37.CT10 AS CT10#50, properties#37.CT12 AS CT12#51, properties#37.CTCB10 AS CTCB10#52, properties#37.HD_2012 AS HD_2012#53L, properties#37.HD_NAME AS HD_NAME#54, properties#37.HOUSING10 AS HOUSING10#55L, properties#37.LA_FIP10 AS LA_FIP10#56, properties#37.OBJECTID AS OBJECTID#57L, properties#37.POP_2010 AS POP_2010#58L, properties#37.PUMA10 AS PUMA10#59, properties#37.SPA_2012 AS SPA_2012#60L, properties#37.SPA_NAME AS SPA_NAME#61, properties#37.SUP_DIST AS SUP_DIST#62, properties#37.SUP_LABEL AS SUP_LABEL#63, properties#37.ShapeSTArea AS ShapeSTArea#64, properties#37.ShapeSTLength AS ShapeSTLength#65, ... 2 more fields]\n",
      "      :                          +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "      :                             +- Project [features#33]\n",
      "      :                                +- Generate explode(features#25), false, [features#33]\n",
      "      :                                   +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "      +- Project [COMM#430, TotalPopulation#408L, NumberOfCrimes#410L, (cast(NumberOfCrimes#410L as double) / cast(TotalPopulation#408L as double)) AS Crimes per Person#414]\n",
      "         +- Aggregate [COMM#430], [COMM#430, sum(POP_2010#439L) AS TotalPopulation#408L, sum(NumberOfCrimesPerBlock#398L) AS NumberOfCrimes#410L]\n",
      "            +- Aggregate [geometry#36, COMM#430, POP_2010#439L], [geometry#36, COMM#430, POP_2010#439L, count(1) AS NumberOfCrimesPerBlock#398L]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                  :- Project [point#351]\n",
      "                  :  +- Project [DR_NO#294, Date Rptd#295, DATE OCC#296, TIME OCC#297, AREA #298, AREA NAME#299, Rpt Dist No#300, Part 1-2#301, Crm Cd#302, Crm Cd Desc#303, Mocodes#304, Vict Age#305, Vict Sex#306, Vict Descent#307, Premis Cd#308, Premis Desc#309, Weapon Used Cd#310, Weapon Desc#311, Status#312, Status Desc#313, Crm Cd 1#314, Crm Cd 2#315, Crm Cd 3#316, Crm Cd 4#317, ... 5 more fields]\n",
      "                  :     +- Filter (NOT (cast(LON#321 as int) = 0) OR NOT (cast(LAT#320 as int) = 0))\n",
      "                  :        +- Relation [DR_NO#294,Date Rptd#295,DATE OCC#296,TIME OCC#297,AREA #298,AREA NAME#299,Rpt Dist No#300,Part 1-2#301,Crm Cd#302,Crm Cd Desc#303,Mocodes#304,Vict Age#305,Vict Sex#306,Vict Descent#307,Premis Cd#308,Premis Desc#309,Weapon Used Cd#310,Weapon Desc#311,Status#312,Status Desc#313,Crm Cd 1#314,Crm Cd 2#315,Crm Cd 3#316,Crm Cd 4#317,... 4 more fields] parquet\n",
      "                  +- Project [geometry#36, COMM#430, POP_2010#439L]\n",
      "                     +- Filter atleastnnonnulls(2, COMM#430, ZCTA10#447)\n",
      "                        +- Project [properties#37.BG10 AS BG10#423, properties#37.BG10FIP10 AS BG10FIP10#424, properties#37.BG12 AS BG12#425, properties#37.CB10 AS CB10#426, properties#37.CEN_FIP13 AS CEN_FIP13#427, properties#37.CITY AS CITY#428, properties#37.CITYCOM AS CITYCOM#429, properties#37.COMM AS COMM#430, properties#37.CT10 AS CT10#431, properties#37.CT12 AS CT12#432, properties#37.CTCB10 AS CTCB10#433, properties#37.HD_2012 AS HD_2012#434L, properties#37.HD_NAME AS HD_NAME#435, properties#37.HOUSING10 AS HOUSING10#436L, properties#37.LA_FIP10 AS LA_FIP10#437, properties#37.OBJECTID AS OBJECTID#438L, properties#37.POP_2010 AS POP_2010#439L, properties#37.PUMA10 AS PUMA10#440, properties#37.SPA_2012 AS SPA_2012#441L, properties#37.SPA_NAME AS SPA_NAME#442, properties#37.SUP_DIST AS SUP_DIST#443, properties#37.SUP_LABEL AS SUP_LABEL#444, properties#37.ShapeSTArea AS ShapeSTArea#445, properties#37.ShapeSTLength AS ShapeSTLength#446, ... 2 more fields]\n",
      "                           +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                              +- Project [features#33]\n",
      "                                 +- Generate explode(features#420), false, [features#33]\n",
      "                                    +- Relation [crs#419,features#420,name#421,type#422] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Average Income: double, Crimes per Person: double\n",
      "Sort [Average Income#289 DESC NULLS LAST], true\n",
      "+- Project [COMM#49 AS COMM#465, Average Income#289, Crimes per Person#414]\n",
      "   +- Join Inner, (COMM#49 = COMM#430)\n",
      "      :- Project [COMM#49, total_population#283L, comm_total_income#285, (comm_total_income#285 / cast(total_population#283L as double)) AS Average Income#289]\n",
      "      :  +- Aggregate [COMM#49], [COMM#49, sum(TOTAL_POP_2010#215L) AS total_population#283L, sum(total_income#265) AS comm_total_income#285]\n",
      "      :     +- Project [Zip Code#240, Community#241, Estimated Median Income#246, COMM#49, ZCTA10#66, TOTAL_POP_2010#215L, TOTAL_HOUSING10#217L, (Estimated Median Income#246 * cast(TOTAL_HOUSING10#217L as float)) AS total_income#265]\n",
      "      :        +- Join Inner, (ZCTA10#66 = Zip Code#240)\n",
      "      :           :- Project [Zip Code#240, Community#241, cast(regexp_replace(Estimated Median Income#242, [$,.], , 1) as float) AS Estimated Median Income#246]\n",
      "      :           :  +- Relation [Zip Code#240,Community#241,Estimated Median Income#242] csv\n",
      "      :           +- Aggregate [COMM#49, ZCTA10#66], [COMM#49, ZCTA10#66, sum(POP_2010#204L) AS TOTAL_POP_2010#215L, sum(HOUSING10#205L) AS TOTAL_HOUSING10#217L]\n",
      "      :              +- Project [COMM#49, coalesce(POP_2010#58L, cast(0 as bigint)) AS POP_2010#204L, ZCTA10#66, coalesce(HOUSING10#55L, cast(0 as bigint)) AS HOUSING10#205L]\n",
      "      :                 +- Project [COMM#49, POP_2010#58L, ZCTA10#66, HOUSING10#55L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#49, ZCTA10#66)\n",
      "      :                       +- Project [properties#37.BG10 AS BG10#42, properties#37.BG10FIP10 AS BG10FIP10#43, properties#37.BG12 AS BG12#44, properties#37.CB10 AS CB10#45, properties#37.CEN_FIP13 AS CEN_FIP13#46, properties#37.CITY AS CITY#47, properties#37.CITYCOM AS CITYCOM#48, properties#37.COMM AS COMM#49, properties#37.CT10 AS CT10#50, properties#37.CT12 AS CT12#51, properties#37.CTCB10 AS CTCB10#52, properties#37.HD_2012 AS HD_2012#53L, properties#37.HD_NAME AS HD_NAME#54, properties#37.HOUSING10 AS HOUSING10#55L, properties#37.LA_FIP10 AS LA_FIP10#56, properties#37.OBJECTID AS OBJECTID#57L, properties#37.POP_2010 AS POP_2010#58L, properties#37.PUMA10 AS PUMA10#59, properties#37.SPA_2012 AS SPA_2012#60L, properties#37.SPA_NAME AS SPA_NAME#61, properties#37.SUP_DIST AS SUP_DIST#62, properties#37.SUP_LABEL AS SUP_LABEL#63, properties#37.ShapeSTArea AS ShapeSTArea#64, properties#37.ShapeSTLength AS ShapeSTLength#65, ... 2 more fields]\n",
      "      :                          +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "      :                             +- Project [features#33]\n",
      "      :                                +- Generate explode(features#25), false, [features#33]\n",
      "      :                                   +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "      +- Project [COMM#430, TotalPopulation#408L, NumberOfCrimes#410L, (cast(NumberOfCrimes#410L as double) / cast(TotalPopulation#408L as double)) AS Crimes per Person#414]\n",
      "         +- Aggregate [COMM#430], [COMM#430, sum(POP_2010#439L) AS TotalPopulation#408L, sum(NumberOfCrimesPerBlock#398L) AS NumberOfCrimes#410L]\n",
      "            +- Aggregate [geometry#36, COMM#430, POP_2010#439L], [geometry#36, COMM#430, POP_2010#439L, count(1) AS NumberOfCrimesPerBlock#398L]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                  :- Project [point#351]\n",
      "                  :  +- Project [DR_NO#294, Date Rptd#295, DATE OCC#296, TIME OCC#297, AREA #298, AREA NAME#299, Rpt Dist No#300, Part 1-2#301, Crm Cd#302, Crm Cd Desc#303, Mocodes#304, Vict Age#305, Vict Sex#306, Vict Descent#307, Premis Cd#308, Premis Desc#309, Weapon Used Cd#310, Weapon Desc#311, Status#312, Status Desc#313, Crm Cd 1#314, Crm Cd 2#315, Crm Cd 3#316, Crm Cd 4#317, ... 5 more fields]\n",
      "                  :     +- Filter (NOT (cast(LON#321 as int) = 0) OR NOT (cast(LAT#320 as int) = 0))\n",
      "                  :        +- Relation [DR_NO#294,Date Rptd#295,DATE OCC#296,TIME OCC#297,AREA #298,AREA NAME#299,Rpt Dist No#300,Part 1-2#301,Crm Cd#302,Crm Cd Desc#303,Mocodes#304,Vict Age#305,Vict Sex#306,Vict Descent#307,Premis Cd#308,Premis Desc#309,Weapon Used Cd#310,Weapon Desc#311,Status#312,Status Desc#313,Crm Cd 1#314,Crm Cd 2#315,Crm Cd 3#316,Crm Cd 4#317,... 4 more fields] parquet\n",
      "                  +- Project [geometry#36, COMM#430, POP_2010#439L]\n",
      "                     +- Filter atleastnnonnulls(2, COMM#430, ZCTA10#447)\n",
      "                        +- Project [properties#37.BG10 AS BG10#423, properties#37.BG10FIP10 AS BG10FIP10#424, properties#37.BG12 AS BG12#425, properties#37.CB10 AS CB10#426, properties#37.CEN_FIP13 AS CEN_FIP13#427, properties#37.CITY AS CITY#428, properties#37.CITYCOM AS CITYCOM#429, properties#37.COMM AS COMM#430, properties#37.CT10 AS CT10#431, properties#37.CT12 AS CT12#432, properties#37.CTCB10 AS CTCB10#433, properties#37.HD_2012 AS HD_2012#434L, properties#37.HD_NAME AS HD_NAME#435, properties#37.HOUSING10 AS HOUSING10#436L, properties#37.LA_FIP10 AS LA_FIP10#437, properties#37.OBJECTID AS OBJECTID#438L, properties#37.POP_2010 AS POP_2010#439L, properties#37.PUMA10 AS PUMA10#440, properties#37.SPA_2012 AS SPA_2012#441L, properties#37.SPA_NAME AS SPA_NAME#442, properties#37.SUP_DIST AS SUP_DIST#443, properties#37.SUP_LABEL AS SUP_LABEL#444, properties#37.ShapeSTArea AS ShapeSTArea#445, properties#37.ShapeSTLength AS ShapeSTLength#446, ... 2 more fields]\n",
      "                           +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                              +- Project [features#33]\n",
      "                                 +- Generate explode(features#420), false, [features#33]\n",
      "                                    +- Relation [crs#419,features#420,name#421,type#422] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Average Income#289 DESC NULLS LAST], true\n",
      "+- Project [COMM#49, Average Income#289, Crimes per Person#414]\n",
      "   +- Join Inner, (COMM#49 = COMM#430)\n",
      "      :- Aggregate [COMM#49], [COMM#49, (sum(total_income#265) / cast(sum(TOTAL_POP_2010#215L) as double)) AS Average Income#289]\n",
      "      :  +- Project [COMM#49, TOTAL_POP_2010#215L, (Estimated Median Income#246 * cast(TOTAL_HOUSING10#217L as float)) AS total_income#265]\n",
      "      :     +- Join Inner, (ZCTA10#66 = Zip Code#240)\n",
      "      :        :- Project [Zip Code#240, cast(regexp_replace(Estimated Median Income#242, [$,.], , 1) as float) AS Estimated Median Income#246]\n",
      "      :        :  +- Filter isnotnull(Zip Code#240)\n",
      "      :        :     +- Relation [Zip Code#240,Community#241,Estimated Median Income#242] csv\n",
      "      :        +- Aggregate [COMM#49, ZCTA10#66], [COMM#49, ZCTA10#66, sum(POP_2010#204L) AS TOTAL_POP_2010#215L, sum(HOUSING10#205L) AS TOTAL_HOUSING10#217L]\n",
      "      :           +- Project [features#33.properties.COMM AS COMM#49, coalesce(features#33.properties.POP_2010, 0) AS POP_2010#204L, features#33.properties.ZCTA10 AS ZCTA10#66, coalesce(features#33.properties.HOUSING10, 0) AS HOUSING10#205L]\n",
      "      :              +- Filter (atleastnnonnulls(2, features#33.properties.COMM, features#33.properties.ZCTA10) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "      :                 +- Generate explode(features#25), [0], false, [features#33]\n",
      "      :                    +- Project [features#25]\n",
      "      :                       +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                          +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "      +- Aggregate [COMM#430], [COMM#430, (cast(sum(NumberOfCrimesPerBlock#398L) as double) / cast(sum(POP_2010#439L) as double)) AS Crimes per Person#414]\n",
      "         +- Aggregate [geometry#36, COMM#430, POP_2010#439L], [COMM#430, POP_2010#439L, count(1) AS NumberOfCrimesPerBlock#398L]\n",
      "            +- Project [geometry#36, COMM#430, POP_2010#439L]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#351]\n",
      "                  :  +- Filter ((NOT (cast(LON#321 as int) = 0) OR NOT (cast(LAT#320 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :     +- Relation [DR_NO#294,Date Rptd#295,DATE OCC#296,TIME OCC#297,AREA #298,AREA NAME#299,Rpt Dist No#300,Part 1-2#301,Crm Cd#302,Crm Cd Desc#303,Mocodes#304,Vict Age#305,Vict Sex#306,Vict Descent#307,Premis Cd#308,Premis Desc#309,Weapon Used Cd#310,Weapon Desc#311,Status#312,Status Desc#313,Crm Cd 1#314,Crm Cd 2#315,Crm Cd 3#316,Crm Cd 4#317,... 4 more fields] parquet\n",
      "                  +- Project [features#33.geometry AS geometry#36, features#33.properties.COMM AS COMM#430, features#33.properties.POP_2010 AS POP_2010#439L]\n",
      "                     +- Filter (((atleastnnonnulls(2, features#33.properties.COMM, features#33.properties.ZCTA10) AND isnotnull(features#33.geometry)) AND isnotnull(features#33.properties.COMM)) AND bloomfilter#554 of [COMM#49] filtering [features#33.properties.COMM])\n",
      "                        :  +- Aggregate [COMM#49, ZCTA10#66], [COMM#49, ZCTA10#66, sum(POP_2010#204L) AS TOTAL_POP_2010#215L, sum(HOUSING10#205L) AS TOTAL_HOUSING10#217L]\n",
      "                        :     +- Project [features#33.properties.COMM AS COMM#49, coalesce(features#33.properties.POP_2010, 0) AS POP_2010#204L, features#33.properties.ZCTA10 AS ZCTA10#66, coalesce(features#33.properties.HOUSING10, 0) AS HOUSING10#205L]\n",
      "                        :        +- Filter (atleastnnonnulls(2, features#33.properties.COMM, features#33.properties.ZCTA10) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "                        :           +- Generate explode(features#25), [0], false, [features#33]\n",
      "                        :              +- Project [features#25]\n",
      "                        :                 +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                        :                    +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "                        +- Generate explode(features#420), [0], false, [features#33]\n",
      "                           +- Project [features#420]\n",
      "                              +- Filter ((size(features#420, true) > 0) AND isnotnull(features#420))\n",
      "                                 +- Relation [crs#419,features#420,name#421,type#422] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Average Income#289 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Average Income#289 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=1170]\n",
      "      +- Project [COMM#49, Average Income#289, Crimes per Person#414]\n",
      "         +- SortMergeJoin [COMM#49], [COMM#430], Inner\n",
      "            :- Sort [COMM#49 ASC NULLS FIRST], false, 0\n",
      "            :  +- HashAggregate(keys=[COMM#49], functions=[sum(total_income#265), sum(TOTAL_POP_2010#215L)], output=[COMM#49, Average Income#289], schema specialized)\n",
      "            :     +- Exchange hashpartitioning(COMM#49, 1000), ENSURE_REQUIREMENTS, [plan_id=1045]\n",
      "            :        +- HashAggregate(keys=[COMM#49], functions=[partial_sum(total_income#265), partial_sum(TOTAL_POP_2010#215L)], output=[COMM#49, sum#480, sum#482L], schema specialized)\n",
      "            :           +- Project [COMM#49, TOTAL_POP_2010#215L, (Estimated Median Income#246 * cast(TOTAL_HOUSING10#217L as float)) AS total_income#265]\n",
      "            :              +- BroadcastHashJoin [Zip Code#240], [ZCTA10#66], Inner, BuildLeft, false\n",
      "            :                 :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1040]\n",
      "            :                 :  +- Project [Zip Code#240, cast(regexp_replace(Estimated Median Income#242, [$,.], , 1) as float) AS Estimated Median Income#246]\n",
      "            :                 :     +- Filter isnotnull(Zip Code#240)\n",
      "            :                 :        +- FileScan csv [Zip Code#240,Estimated Median Income#242] Batched: false, DataFilters: [isnotnull(Zip Code#240)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            :                 +- HashAggregate(keys=[COMM#49, ZCTA10#66], functions=[sum(POP_2010#204L), sum(HOUSING10#205L)], output=[COMM#49, ZCTA10#66, TOTAL_POP_2010#215L, TOTAL_HOUSING10#217L], schema specialized)\n",
      "            :                    +- Exchange hashpartitioning(COMM#49, ZCTA10#66, 1000), ENSURE_REQUIREMENTS, [plan_id=1037]\n",
      "            :                       +- HashAggregate(keys=[COMM#49, ZCTA10#66], functions=[partial_sum(POP_2010#204L), partial_sum(HOUSING10#205L)], output=[COMM#49, ZCTA10#66, sum#484L, sum#486L], schema specialized)\n",
      "            :                          +- Project [features#33.properties.COMM AS COMM#49, coalesce(features#33.properties.POP_2010, 0) AS POP_2010#204L, features#33.properties.ZCTA10 AS ZCTA10#66, coalesce(features#33.properties.HOUSING10, 0) AS HOUSING10#205L]\n",
      "            :                             +- Filter (atleastnnonnulls(2, features#33.properties.COMM, features#33.properties.ZCTA10) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "            :                                +- Generate explode(features#25), false, [features#33]\n",
      "            :                                   +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "            :                                      +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "            +- Sort [COMM#430 ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[COMM#430], functions=[sum(NumberOfCrimesPerBlock#398L), sum(POP_2010#439L)], output=[COMM#430, Crimes per Person#414], schema specialized)\n",
      "                  +- Exchange hashpartitioning(COMM#430, 1000), ENSURE_REQUIREMENTS, [plan_id=1165]\n",
      "                     +- HashAggregate(keys=[COMM#430], functions=[partial_sum(NumberOfCrimesPerBlock#398L), partial_sum(POP_2010#439L)], output=[COMM#430, sum#488L, sum#490L], schema specialized)\n",
      "                        +- HashAggregate(keys=[geometry#36, COMM#430, POP_2010#439L], functions=[count(1)], output=[COMM#430, POP_2010#439L, NumberOfCrimesPerBlock#398L])\n",
      "                           +- Exchange hashpartitioning(geometry#36, COMM#430, POP_2010#439L, 1000), ENSURE_REQUIREMENTS, [plan_id=1162]\n",
      "                              +- HashAggregate(keys=[geometry#36, COMM#430, POP_2010#439L], functions=[partial_count(1)], output=[geometry#36, COMM#430, POP_2010#439L, count#492L])\n",
      "                                 +- Project [geometry#36, COMM#430, POP_2010#439L]\n",
      "                                    +- RangeJoin point#351: geometry, geometry#36: geometry, WITHIN\n",
      "                                       :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#351]\n",
      "                                       :  +- Filter ((NOT (cast(LON#321 as int) = 0) OR NOT (cast(LAT#320 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                       :     +- FileScan parquet [LAT#320,LON#321] Batched: true, DataFilters: [(NOT (cast(LON#321 as int) = 0) OR NOT (cast(LAT#320 as int) = 0)), isnotnull( **org.apache.spar..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                                       +- Project [features#33.geometry AS geometry#36, features#33.properties.COMM AS COMM#430, features#33.properties.POP_2010 AS POP_2010#439L]\n",
      "                                          +- Filter (((atleastnnonnulls(2, features#33.properties.COMM, features#33.properties.ZCTA10) AND isnotnull(features#33.geometry)) AND isnotnull(features#33.properties.COMM)) AND bloomfilter#554 of [bf554 COMM#49 estimatedNumRows=294857] filtering [features#33.properties.COMM])\n",
      "                                             :  +- GenerateBloomFilter bf554, 294857, 0, false, false, false, true, [id=#1156]\n",
      "                                             :     +- OutputAdapter [COMM#49, ZCTA10#66, sum#484L, sum#486L]\n",
      "                                             :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                                             :           +- Exchange hashpartitioning(COMM#49, ZCTA10#66, 1000), ENSURE_REQUIREMENTS, [plan_id=1153]\n",
      "                                             :              +- HashAggregate(keys=[COMM#49, ZCTA10#66], functions=[partial_sum(POP_2010#204L), partial_sum(HOUSING10#205L)], output=[COMM#49, ZCTA10#66, sum#484L, sum#486L], schema specialized)\n",
      "                                             :                 +- Project [features#33.properties.COMM AS COMM#49, coalesce(features#33.properties.POP_2010, 0) AS POP_2010#204L, features#33.properties.ZCTA10 AS ZCTA10#66, coalesce(features#33.properties.HOUSING10, 0) AS HOUSING10#205L]\n",
      "                                             :                    +- Filter (atleastnnonnulls(2, features#33.properties.COMM, features#33.properties.ZCTA10) AND (isnotnull(features#33.properties.ZCTA10) AND isnotnull(features#33.properties.COMM)))\n",
      "                                             :                       +- Generate explode(features#25), false, [features#33]\n",
      "                                             :                          +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                                             :                             +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                                             +- Generate explode(features#420), false, [features#33]\n",
      "                                                +- Filter ((size(features#420, true) > 0) AND isnotnull(features#420))\n",
      "                                                   +- FileScan geojson [features#420] Batched: false, DataFilters: [(size(features#420, true) > 0), isnotnull(features#420)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "res.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e4421a8-f714-4bde-8d51-14c6fe42396a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.77 seconds\n",
      "+--------------------+------------------+--------------------+\n",
      "|                COMM|    Average Income|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|      Marina del Rey| 76428.84908639747| 0.16078790655061842|\n",
      "|   Pacific Palisades| 70656.11180545464|  0.4720770986558458|\n",
      "|              Malibu|  67135.0118623962|0.003460207612456...|\n",
      "| Palisades Highlands| 66867.44038612054|  0.2055830941821028|\n",
      "|    Marina Peninsula|65235.692875259396|  0.6549938347718866|\n",
      "|             Bel Air| 63041.33942621959| 0.43199608610567514|\n",
      "|Palos Verdes Estates| 61905.61214466438|0.008547008547008548|\n",
      "|     Manhattan Beach|60985.189241497086|0.033648790746582544|\n",
      "|       Beverly Crest| 60947.48978754819| 0.37490683229813665|\n",
      "|           Brentwood| 60840.62462032012|  0.5346764258279586|\n",
      "|       Hermosa Beach| 57924.85594176151|0.016216216216216217|\n",
      "|   Mandeville Canyon| 55572.11011444479|  0.2716207559256887|\n",
      "|La Cañada Flintridge| 54900.65682110046| 0.01282051282051282|\n",
      "|       Beverly Hills| 51303.11237503298|  0.4119975639464068|\n",
      "|         Playa Vista| 50264.47143177235|  0.8175387379294857|\n",
      "|             Carthay|49841.163007975694|  1.0959014197696222|\n",
      "|      West Hollywood| 49414.42132038722| 0.13344267183539585|\n",
      "|              Venice|47614.884045977014|  1.3570434206165065|\n",
      "|       Redondo Beach| 47568.80637622101|0.050505050505050504|\n",
      "| Rancho Palos Verdes| 46151.89491631246| 0.28346456692913385|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "####query 3 HINT SHUFFLE HASH !!!!\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .dropna(subset=[\"COMM\",\"ZCTA10\"])  \\\n",
    "\n",
    "\n",
    "census = blocks_census \\\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"HOUSING10\") \\\n",
    "    .na.fill({\"POP_2010\": 0, \"HOUSING10\": 0}) \\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"TOTAL_POP_2010\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"TOTAL_HOUSING10\")\n",
    "    )\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,.]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"TOTAL_HOUSING10\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"TOTAL_POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    ".select(\"point\")\n",
    "\n",
    "\n",
    "# res2 = crime_dataset \\\n",
    "#     .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry),)\\\n",
    "# .select(\"COMM\",\"POP_2010\")\\\n",
    "# .groupBy(\"COMM\")\\\n",
    "# .agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    "# .withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census.select(\"geometry\",\"COMM\",\"POP_2010\"), ST_Within(crime_dataset.point, blocks_census.geometry)) \\\n",
    ".groupBy(\"geometry\",\"COMM\",\"POP_2010\") \\\n",
    ".agg(F.count(\"*\").alias(\"NumberOfCrimesPerBlock\"))\\\n",
    ".groupby(\"COMM\") \\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.sum(\"NumberOfCrimesPerBlock\").alias(\"NumberOfCrimes\") ) \\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2.hint(\"SHUFFLE_HASH\"),res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\").orderBy(col(\"Average Income\").desc())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efbd3b15-2d1e-45ca-8226-2e8ea19642e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Average Income DESC NULLS LAST], true\n",
      "+- Project [COMM#6843 AS COMM#7259, Average Income#7083, Crimes per Person#7208]\n",
      "   +- Join Inner, (COMM#6843 = COMM#7224)\n",
      "      :- Project [COMM#6843, total_population#7077L, comm_total_income#7079, (comm_total_income#7079 / cast(total_population#7077L as double)) AS Average Income#7083]\n",
      "      :  +- Aggregate [COMM#6843], [COMM#6843, sum(TOTAL_POP_2010#7009L) AS total_population#7077L, sum(total_income#7059) AS comm_total_income#7079]\n",
      "      :     +- Project [Zip Code#7034, Community#7035, Estimated Median Income#7040, COMM#6843, ZCTA10#6860, TOTAL_POP_2010#7009L, TOTAL_HOUSING10#7011L, (Estimated Median Income#7040 * cast(TOTAL_HOUSING10#7011L as float)) AS total_income#7059]\n",
      "      :        +- Join Inner, (ZCTA10#6860 = Zip Code#7034)\n",
      "      :           :- Project [Zip Code#7034, Community#7035, cast(regexp_replace(Estimated Median Income#7036, [$,.], , 1) as float) AS Estimated Median Income#7040]\n",
      "      :           :  +- Relation [Zip Code#7034,Community#7035,Estimated Median Income#7036] csv\n",
      "      :           +- Aggregate [COMM#6843, ZCTA10#6860], [COMM#6843, ZCTA10#6860, sum(POP_2010#6998L) AS TOTAL_POP_2010#7009L, sum(HOUSING10#6999L) AS TOTAL_HOUSING10#7011L]\n",
      "      :              +- Project [COMM#6843, coalesce(POP_2010#6852L, cast(0 as bigint)) AS POP_2010#6998L, ZCTA10#6860, coalesce(HOUSING10#6849L, cast(0 as bigint)) AS HOUSING10#6999L]\n",
      "      :                 +- Project [COMM#6843, POP_2010#6852L, ZCTA10#6860, HOUSING10#6849L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#6843, ZCTA10#6860)\n",
      "      :                       +- Project [properties#6831.BG10 AS BG10#6836, properties#6831.BG10FIP10 AS BG10FIP10#6837, properties#6831.BG12 AS BG12#6838, properties#6831.CB10 AS CB10#6839, properties#6831.CEN_FIP13 AS CEN_FIP13#6840, properties#6831.CITY AS CITY#6841, properties#6831.CITYCOM AS CITYCOM#6842, properties#6831.COMM AS COMM#6843, properties#6831.CT10 AS CT10#6844, properties#6831.CT12 AS CT12#6845, properties#6831.CTCB10 AS CTCB10#6846, properties#6831.HD_2012 AS HD_2012#6847L, properties#6831.HD_NAME AS HD_NAME#6848, properties#6831.HOUSING10 AS HOUSING10#6849L, properties#6831.LA_FIP10 AS LA_FIP10#6850, properties#6831.OBJECTID AS OBJECTID#6851L, properties#6831.POP_2010 AS POP_2010#6852L, properties#6831.PUMA10 AS PUMA10#6853, properties#6831.SPA_2012 AS SPA_2012#6854L, properties#6831.SPA_NAME AS SPA_NAME#6855, properties#6831.SUP_DIST AS SUP_DIST#6856, properties#6831.SUP_LABEL AS SUP_LABEL#6857, properties#6831.ShapeSTArea AS ShapeSTArea#6858, properties#6831.ShapeSTLength AS ShapeSTLength#6859, ... 2 more fields]\n",
      "      :                          +- Project [features#6827.geometry AS geometry#6830, features#6827.properties AS properties#6831, features#6827.type AS type#6832]\n",
      "      :                             +- Project [features#6827]\n",
      "      :                                +- Generate explode(features#6819), false, [features#6827]\n",
      "      :                                   +- Relation [crs#6818,features#6819,name#6820,type#6821] geojson\n",
      "      +- ResolvedHint (strategy=shuffle_hash)\n",
      "         +- Project [COMM#7224, TotalPopulation#7202L, NumberOfCrimes#7204L, (cast(NumberOfCrimes#7204L as double) / cast(TotalPopulation#7202L as double)) AS Crimes per Person#7208]\n",
      "            +- Aggregate [COMM#7224], [COMM#7224, sum(POP_2010#7233L) AS TotalPopulation#7202L, sum(NumberOfCrimesPerBlock#7192L) AS NumberOfCrimes#7204L]\n",
      "               +- Aggregate [geometry#6830, COMM#7224, POP_2010#7233L], [geometry#6830, COMM#7224, POP_2010#7233L, count(1) AS NumberOfCrimesPerBlock#7192L]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                     :- Project [point#7145]\n",
      "                     :  +- Project [DR_NO#7088, Date Rptd#7089, DATE OCC#7090, TIME OCC#7091, AREA #7092, AREA NAME#7093, Rpt Dist No#7094, Part 1-2#7095, Crm Cd#7096, Crm Cd Desc#7097, Mocodes#7098, Vict Age#7099, Vict Sex#7100, Vict Descent#7101, Premis Cd#7102, Premis Desc#7103, Weapon Used Cd#7104, Weapon Desc#7105, Status#7106, Status Desc#7107, Crm Cd 1#7108, Crm Cd 2#7109, Crm Cd 3#7110, Crm Cd 4#7111, ... 5 more fields]\n",
      "                     :     +- Filter (NOT (cast(LON#7115 as int) = 0) OR NOT (cast(LAT#7114 as int) = 0))\n",
      "                     :        +- Relation [DR_NO#7088,Date Rptd#7089,DATE OCC#7090,TIME OCC#7091,AREA #7092,AREA NAME#7093,Rpt Dist No#7094,Part 1-2#7095,Crm Cd#7096,Crm Cd Desc#7097,Mocodes#7098,Vict Age#7099,Vict Sex#7100,Vict Descent#7101,Premis Cd#7102,Premis Desc#7103,Weapon Used Cd#7104,Weapon Desc#7105,Status#7106,Status Desc#7107,Crm Cd 1#7108,Crm Cd 2#7109,Crm Cd 3#7110,Crm Cd 4#7111,... 4 more fields] parquet\n",
      "                     +- Project [geometry#6830, COMM#7224, POP_2010#7233L]\n",
      "                        +- Filter atleastnnonnulls(2, COMM#7224, ZCTA10#7241)\n",
      "                           +- Project [properties#6831.BG10 AS BG10#7217, properties#6831.BG10FIP10 AS BG10FIP10#7218, properties#6831.BG12 AS BG12#7219, properties#6831.CB10 AS CB10#7220, properties#6831.CEN_FIP13 AS CEN_FIP13#7221, properties#6831.CITY AS CITY#7222, properties#6831.CITYCOM AS CITYCOM#7223, properties#6831.COMM AS COMM#7224, properties#6831.CT10 AS CT10#7225, properties#6831.CT12 AS CT12#7226, properties#6831.CTCB10 AS CTCB10#7227, properties#6831.HD_2012 AS HD_2012#7228L, properties#6831.HD_NAME AS HD_NAME#7229, properties#6831.HOUSING10 AS HOUSING10#7230L, properties#6831.LA_FIP10 AS LA_FIP10#7231, properties#6831.OBJECTID AS OBJECTID#7232L, properties#6831.POP_2010 AS POP_2010#7233L, properties#6831.PUMA10 AS PUMA10#7234, properties#6831.SPA_2012 AS SPA_2012#7235L, properties#6831.SPA_NAME AS SPA_NAME#7236, properties#6831.SUP_DIST AS SUP_DIST#7237, properties#6831.SUP_LABEL AS SUP_LABEL#7238, properties#6831.ShapeSTArea AS ShapeSTArea#7239, properties#6831.ShapeSTLength AS ShapeSTLength#7240, ... 2 more fields]\n",
      "                              +- Project [features#6827.geometry AS geometry#6830, features#6827.properties AS properties#6831, features#6827.type AS type#6832]\n",
      "                                 +- Project [features#6827]\n",
      "                                    +- Generate explode(features#7214), false, [features#6827]\n",
      "                                       +- Relation [crs#7213,features#7214,name#7215,type#7216] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Average Income: double, Crimes per Person: double\n",
      "Sort [Average Income#7083 DESC NULLS LAST], true\n",
      "+- Project [COMM#6843 AS COMM#7259, Average Income#7083, Crimes per Person#7208]\n",
      "   +- Join Inner, (COMM#6843 = COMM#7224)\n",
      "      :- Project [COMM#6843, total_population#7077L, comm_total_income#7079, (comm_total_income#7079 / cast(total_population#7077L as double)) AS Average Income#7083]\n",
      "      :  +- Aggregate [COMM#6843], [COMM#6843, sum(TOTAL_POP_2010#7009L) AS total_population#7077L, sum(total_income#7059) AS comm_total_income#7079]\n",
      "      :     +- Project [Zip Code#7034, Community#7035, Estimated Median Income#7040, COMM#6843, ZCTA10#6860, TOTAL_POP_2010#7009L, TOTAL_HOUSING10#7011L, (Estimated Median Income#7040 * cast(TOTAL_HOUSING10#7011L as float)) AS total_income#7059]\n",
      "      :        +- Join Inner, (ZCTA10#6860 = Zip Code#7034)\n",
      "      :           :- Project [Zip Code#7034, Community#7035, cast(regexp_replace(Estimated Median Income#7036, [$,.], , 1) as float) AS Estimated Median Income#7040]\n",
      "      :           :  +- Relation [Zip Code#7034,Community#7035,Estimated Median Income#7036] csv\n",
      "      :           +- Aggregate [COMM#6843, ZCTA10#6860], [COMM#6843, ZCTA10#6860, sum(POP_2010#6998L) AS TOTAL_POP_2010#7009L, sum(HOUSING10#6999L) AS TOTAL_HOUSING10#7011L]\n",
      "      :              +- Project [COMM#6843, coalesce(POP_2010#6852L, cast(0 as bigint)) AS POP_2010#6998L, ZCTA10#6860, coalesce(HOUSING10#6849L, cast(0 as bigint)) AS HOUSING10#6999L]\n",
      "      :                 +- Project [COMM#6843, POP_2010#6852L, ZCTA10#6860, HOUSING10#6849L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#6843, ZCTA10#6860)\n",
      "      :                       +- Project [properties#6831.BG10 AS BG10#6836, properties#6831.BG10FIP10 AS BG10FIP10#6837, properties#6831.BG12 AS BG12#6838, properties#6831.CB10 AS CB10#6839, properties#6831.CEN_FIP13 AS CEN_FIP13#6840, properties#6831.CITY AS CITY#6841, properties#6831.CITYCOM AS CITYCOM#6842, properties#6831.COMM AS COMM#6843, properties#6831.CT10 AS CT10#6844, properties#6831.CT12 AS CT12#6845, properties#6831.CTCB10 AS CTCB10#6846, properties#6831.HD_2012 AS HD_2012#6847L, properties#6831.HD_NAME AS HD_NAME#6848, properties#6831.HOUSING10 AS HOUSING10#6849L, properties#6831.LA_FIP10 AS LA_FIP10#6850, properties#6831.OBJECTID AS OBJECTID#6851L, properties#6831.POP_2010 AS POP_2010#6852L, properties#6831.PUMA10 AS PUMA10#6853, properties#6831.SPA_2012 AS SPA_2012#6854L, properties#6831.SPA_NAME AS SPA_NAME#6855, properties#6831.SUP_DIST AS SUP_DIST#6856, properties#6831.SUP_LABEL AS SUP_LABEL#6857, properties#6831.ShapeSTArea AS ShapeSTArea#6858, properties#6831.ShapeSTLength AS ShapeSTLength#6859, ... 2 more fields]\n",
      "      :                          +- Project [features#6827.geometry AS geometry#6830, features#6827.properties AS properties#6831, features#6827.type AS type#6832]\n",
      "      :                             +- Project [features#6827]\n",
      "      :                                +- Generate explode(features#6819), false, [features#6827]\n",
      "      :                                   +- Relation [crs#6818,features#6819,name#6820,type#6821] geojson\n",
      "      +- ResolvedHint (strategy=shuffle_hash)\n",
      "         +- Project [COMM#7224, TotalPopulation#7202L, NumberOfCrimes#7204L, (cast(NumberOfCrimes#7204L as double) / cast(TotalPopulation#7202L as double)) AS Crimes per Person#7208]\n",
      "            +- Aggregate [COMM#7224], [COMM#7224, sum(POP_2010#7233L) AS TotalPopulation#7202L, sum(NumberOfCrimesPerBlock#7192L) AS NumberOfCrimes#7204L]\n",
      "               +- Aggregate [geometry#6830, COMM#7224, POP_2010#7233L], [geometry#6830, COMM#7224, POP_2010#7233L, count(1) AS NumberOfCrimesPerBlock#7192L]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                     :- Project [point#7145]\n",
      "                     :  +- Project [DR_NO#7088, Date Rptd#7089, DATE OCC#7090, TIME OCC#7091, AREA #7092, AREA NAME#7093, Rpt Dist No#7094, Part 1-2#7095, Crm Cd#7096, Crm Cd Desc#7097, Mocodes#7098, Vict Age#7099, Vict Sex#7100, Vict Descent#7101, Premis Cd#7102, Premis Desc#7103, Weapon Used Cd#7104, Weapon Desc#7105, Status#7106, Status Desc#7107, Crm Cd 1#7108, Crm Cd 2#7109, Crm Cd 3#7110, Crm Cd 4#7111, ... 5 more fields]\n",
      "                     :     +- Filter (NOT (cast(LON#7115 as int) = 0) OR NOT (cast(LAT#7114 as int) = 0))\n",
      "                     :        +- Relation [DR_NO#7088,Date Rptd#7089,DATE OCC#7090,TIME OCC#7091,AREA #7092,AREA NAME#7093,Rpt Dist No#7094,Part 1-2#7095,Crm Cd#7096,Crm Cd Desc#7097,Mocodes#7098,Vict Age#7099,Vict Sex#7100,Vict Descent#7101,Premis Cd#7102,Premis Desc#7103,Weapon Used Cd#7104,Weapon Desc#7105,Status#7106,Status Desc#7107,Crm Cd 1#7108,Crm Cd 2#7109,Crm Cd 3#7110,Crm Cd 4#7111,... 4 more fields] parquet\n",
      "                     +- Project [geometry#6830, COMM#7224, POP_2010#7233L]\n",
      "                        +- Filter atleastnnonnulls(2, COMM#7224, ZCTA10#7241)\n",
      "                           +- Project [properties#6831.BG10 AS BG10#7217, properties#6831.BG10FIP10 AS BG10FIP10#7218, properties#6831.BG12 AS BG12#7219, properties#6831.CB10 AS CB10#7220, properties#6831.CEN_FIP13 AS CEN_FIP13#7221, properties#6831.CITY AS CITY#7222, properties#6831.CITYCOM AS CITYCOM#7223, properties#6831.COMM AS COMM#7224, properties#6831.CT10 AS CT10#7225, properties#6831.CT12 AS CT12#7226, properties#6831.CTCB10 AS CTCB10#7227, properties#6831.HD_2012 AS HD_2012#7228L, properties#6831.HD_NAME AS HD_NAME#7229, properties#6831.HOUSING10 AS HOUSING10#7230L, properties#6831.LA_FIP10 AS LA_FIP10#7231, properties#6831.OBJECTID AS OBJECTID#7232L, properties#6831.POP_2010 AS POP_2010#7233L, properties#6831.PUMA10 AS PUMA10#7234, properties#6831.SPA_2012 AS SPA_2012#7235L, properties#6831.SPA_NAME AS SPA_NAME#7236, properties#6831.SUP_DIST AS SUP_DIST#7237, properties#6831.SUP_LABEL AS SUP_LABEL#7238, properties#6831.ShapeSTArea AS ShapeSTArea#7239, properties#6831.ShapeSTLength AS ShapeSTLength#7240, ... 2 more fields]\n",
      "                              +- Project [features#6827.geometry AS geometry#6830, features#6827.properties AS properties#6831, features#6827.type AS type#6832]\n",
      "                                 +- Project [features#6827]\n",
      "                                    +- Generate explode(features#7214), false, [features#6827]\n",
      "                                       +- Relation [crs#7213,features#7214,name#7215,type#7216] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Average Income#7083 DESC NULLS LAST], true\n",
      "+- Project [COMM#6843, Average Income#7083, Crimes per Person#7208]\n",
      "   +- Join Inner, (COMM#6843 = COMM#7224), rightHint=(strategy=shuffle_hash)\n",
      "      :- Aggregate [COMM#6843], [COMM#6843, (sum(total_income#7059) / cast(sum(TOTAL_POP_2010#7009L) as double)) AS Average Income#7083]\n",
      "      :  +- Project [COMM#6843, TOTAL_POP_2010#7009L, (Estimated Median Income#7040 * cast(TOTAL_HOUSING10#7011L as float)) AS total_income#7059]\n",
      "      :     +- Join Inner, (ZCTA10#6860 = Zip Code#7034)\n",
      "      :        :- Project [Zip Code#7034, cast(regexp_replace(Estimated Median Income#7036, [$,.], , 1) as float) AS Estimated Median Income#7040]\n",
      "      :        :  +- Filter isnotnull(Zip Code#7034)\n",
      "      :        :     +- Relation [Zip Code#7034,Community#7035,Estimated Median Income#7036] csv\n",
      "      :        +- Aggregate [COMM#6843, ZCTA10#6860], [COMM#6843, ZCTA10#6860, sum(POP_2010#6998L) AS TOTAL_POP_2010#7009L, sum(HOUSING10#6999L) AS TOTAL_HOUSING10#7011L]\n",
      "      :           +- Project [features#6827.properties.COMM AS COMM#6843, coalesce(features#6827.properties.POP_2010, 0) AS POP_2010#6998L, features#6827.properties.ZCTA10 AS ZCTA10#6860, coalesce(features#6827.properties.HOUSING10, 0) AS HOUSING10#6999L]\n",
      "      :              +- Filter (atleastnnonnulls(2, features#6827.properties.COMM, features#6827.properties.ZCTA10) AND (isnotnull(features#6827.properties.ZCTA10) AND isnotnull(features#6827.properties.COMM)))\n",
      "      :                 +- Generate explode(features#6819), [0], false, [features#6827]\n",
      "      :                    +- Project [features#6819]\n",
      "      :                       +- Filter ((size(features#6819, true) > 0) AND isnotnull(features#6819))\n",
      "      :                          +- Relation [crs#6818,features#6819,name#6820,type#6821] geojson\n",
      "      +- Aggregate [COMM#7224], [COMM#7224, (cast(sum(NumberOfCrimesPerBlock#7192L) as double) / cast(sum(POP_2010#7233L) as double)) AS Crimes per Person#7208]\n",
      "         +- Aggregate [geometry#6830, COMM#7224, POP_2010#7233L], [COMM#7224, POP_2010#7233L, count(1) AS NumberOfCrimesPerBlock#7192L]\n",
      "            +- Project [geometry#6830, COMM#7224, POP_2010#7233L]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#7145]\n",
      "                  :  +- Filter ((NOT (cast(LON#7115 as int) = 0) OR NOT (cast(LAT#7114 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :     +- Relation [DR_NO#7088,Date Rptd#7089,DATE OCC#7090,TIME OCC#7091,AREA #7092,AREA NAME#7093,Rpt Dist No#7094,Part 1-2#7095,Crm Cd#7096,Crm Cd Desc#7097,Mocodes#7098,Vict Age#7099,Vict Sex#7100,Vict Descent#7101,Premis Cd#7102,Premis Desc#7103,Weapon Used Cd#7104,Weapon Desc#7105,Status#7106,Status Desc#7107,Crm Cd 1#7108,Crm Cd 2#7109,Crm Cd 3#7110,Crm Cd 4#7111,... 4 more fields] parquet\n",
      "                  +- Project [features#6827.geometry AS geometry#6830, features#6827.properties.COMM AS COMM#7224, features#6827.properties.POP_2010 AS POP_2010#7233L]\n",
      "                     +- Filter (((atleastnnonnulls(2, features#6827.properties.COMM, features#6827.properties.ZCTA10) AND isnotnull(features#6827.geometry)) AND isnotnull(features#6827.properties.COMM)) AND bloomfilter#7346 of [COMM#6843] filtering [features#6827.properties.COMM])\n",
      "                        :  +- Aggregate [COMM#6843, ZCTA10#6860], [COMM#6843, ZCTA10#6860, sum(POP_2010#6998L) AS TOTAL_POP_2010#7009L, sum(HOUSING10#6999L) AS TOTAL_HOUSING10#7011L]\n",
      "                        :     +- Project [features#6827.properties.COMM AS COMM#6843, coalesce(features#6827.properties.POP_2010, 0) AS POP_2010#6998L, features#6827.properties.ZCTA10 AS ZCTA10#6860, coalesce(features#6827.properties.HOUSING10, 0) AS HOUSING10#6999L]\n",
      "                        :        +- Filter (atleastnnonnulls(2, features#6827.properties.COMM, features#6827.properties.ZCTA10) AND (isnotnull(features#6827.properties.ZCTA10) AND isnotnull(features#6827.properties.COMM)))\n",
      "                        :           +- Generate explode(features#6819), [0], false, [features#6827]\n",
      "                        :              +- Project [features#6819]\n",
      "                        :                 +- Filter ((size(features#6819, true) > 0) AND isnotnull(features#6819))\n",
      "                        :                    +- Relation [crs#6818,features#6819,name#6820,type#6821] geojson\n",
      "                        +- Generate explode(features#7214), [0], false, [features#6827]\n",
      "                           +- Project [features#7214]\n",
      "                              +- Filter ((size(features#7214, true) > 0) AND isnotnull(features#7214))\n",
      "                                 +- Relation [crs#7213,features#7214,name#7215,type#7216] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Average Income#7083 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Average Income#7083 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=14640]\n",
      "      +- Project [COMM#6843, Average Income#7083, Crimes per Person#7208]\n",
      "         +- ShuffledHashJoin [COMM#6843], [COMM#7224], Inner, BuildRight\n",
      "            :- HashAggregate(keys=[COMM#6843], functions=[sum(total_income#7059), sum(TOTAL_POP_2010#7009L)], output=[COMM#6843, Average Income#7083], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(COMM#6843, 1000), ENSURE_REQUIREMENTS, [plan_id=14523]\n",
      "            :     +- HashAggregate(keys=[COMM#6843], functions=[partial_sum(total_income#7059), partial_sum(TOTAL_POP_2010#7009L)], output=[COMM#6843, sum#7274, sum#7276L], schema specialized)\n",
      "            :        +- Project [COMM#6843, TOTAL_POP_2010#7009L, (Estimated Median Income#7040 * cast(TOTAL_HOUSING10#7011L as float)) AS total_income#7059]\n",
      "            :           +- BroadcastHashJoin [Zip Code#7034], [ZCTA10#6860], Inner, BuildLeft, false\n",
      "            :              :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=14518]\n",
      "            :              :  +- Project [Zip Code#7034, cast(regexp_replace(Estimated Median Income#7036, [$,.], , 1) as float) AS Estimated Median Income#7040]\n",
      "            :              :     +- Filter isnotnull(Zip Code#7034)\n",
      "            :              :        +- FileScan csv [Zip Code#7034,Estimated Median Income#7036] Batched: false, DataFilters: [isnotnull(Zip Code#7034)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            :              +- HashAggregate(keys=[COMM#6843, ZCTA10#6860], functions=[sum(POP_2010#6998L), sum(HOUSING10#6999L)], output=[COMM#6843, ZCTA10#6860, TOTAL_POP_2010#7009L, TOTAL_HOUSING10#7011L], schema specialized)\n",
      "            :                 +- Exchange hashpartitioning(COMM#6843, ZCTA10#6860, 1000), ENSURE_REQUIREMENTS, [plan_id=14515]\n",
      "            :                    +- HashAggregate(keys=[COMM#6843, ZCTA10#6860], functions=[partial_sum(POP_2010#6998L), partial_sum(HOUSING10#6999L)], output=[COMM#6843, ZCTA10#6860, sum#7278L, sum#7280L], schema specialized)\n",
      "            :                       +- Project [features#6827.properties.COMM AS COMM#6843, coalesce(features#6827.properties.POP_2010, 0) AS POP_2010#6998L, features#6827.properties.ZCTA10 AS ZCTA10#6860, coalesce(features#6827.properties.HOUSING10, 0) AS HOUSING10#6999L]\n",
      "            :                          +- Filter (atleastnnonnulls(2, features#6827.properties.COMM, features#6827.properties.ZCTA10) AND (isnotnull(features#6827.properties.ZCTA10) AND isnotnull(features#6827.properties.COMM)))\n",
      "            :                             +- Generate explode(features#6819), false, [features#6827]\n",
      "            :                                +- Filter ((size(features#6819, true) > 0) AND isnotnull(features#6819))\n",
      "            :                                   +- FileScan geojson [features#6819] Batched: false, DataFilters: [(size(features#6819, true) > 0), isnotnull(features#6819)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "            +- HashAggregate(keys=[COMM#7224], functions=[sum(NumberOfCrimesPerBlock#7192L), sum(POP_2010#7233L)], output=[COMM#7224, Crimes per Person#7208], schema specialized)\n",
      "               +- Exchange hashpartitioning(COMM#7224, 1000), ENSURE_REQUIREMENTS, [plan_id=14636]\n",
      "                  +- HashAggregate(keys=[COMM#7224], functions=[partial_sum(NumberOfCrimesPerBlock#7192L), partial_sum(POP_2010#7233L)], output=[COMM#7224, sum#7282L, sum#7284L], schema specialized)\n",
      "                     +- HashAggregate(keys=[geometry#6830, COMM#7224, POP_2010#7233L], functions=[count(1)], output=[COMM#7224, POP_2010#7233L, NumberOfCrimesPerBlock#7192L])\n",
      "                        +- Exchange hashpartitioning(geometry#6830, COMM#7224, POP_2010#7233L, 1000), ENSURE_REQUIREMENTS, [plan_id=14633]\n",
      "                           +- HashAggregate(keys=[geometry#6830, COMM#7224, POP_2010#7233L], functions=[partial_count(1)], output=[geometry#6830, COMM#7224, POP_2010#7233L, count#7286L])\n",
      "                              +- Project [geometry#6830, COMM#7224, POP_2010#7233L]\n",
      "                                 +- RangeJoin point#7145: geometry, geometry#6830: geometry, WITHIN\n",
      "                                    :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#7145]\n",
      "                                    :  +- Filter ((NOT (cast(LON#7115 as int) = 0) OR NOT (cast(LAT#7114 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                    :     +- FileScan parquet [LAT#7114,LON#7115] Batched: true, DataFilters: [(NOT (cast(LON#7115 as int) = 0) OR NOT (cast(LAT#7114 as int) = 0)), isnotnull( **org.apache.sp..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                                    +- Project [features#6827.geometry AS geometry#6830, features#6827.properties.COMM AS COMM#7224, features#6827.properties.POP_2010 AS POP_2010#7233L]\n",
      "                                       +- Filter (((atleastnnonnulls(2, features#6827.properties.COMM, features#6827.properties.ZCTA10) AND isnotnull(features#6827.geometry)) AND isnotnull(features#6827.properties.COMM)) AND bloomfilter#7346 of [bf7346 COMM#6843 estimatedNumRows=294857] filtering [features#6827.properties.COMM])\n",
      "                                          :  +- GenerateBloomFilter bf7346, 294857, 0, false, false, false, true, [id=#14627]\n",
      "                                          :     +- OutputAdapter [COMM#6843, ZCTA10#6860, sum#7278L, sum#7280L]\n",
      "                                          :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                                          :           +- Exchange hashpartitioning(COMM#6843, ZCTA10#6860, 1000), ENSURE_REQUIREMENTS, [plan_id=14624]\n",
      "                                          :              +- HashAggregate(keys=[COMM#6843, ZCTA10#6860], functions=[partial_sum(POP_2010#6998L), partial_sum(HOUSING10#6999L)], output=[COMM#6843, ZCTA10#6860, sum#7278L, sum#7280L], schema specialized)\n",
      "                                          :                 +- Project [features#6827.properties.COMM AS COMM#6843, coalesce(features#6827.properties.POP_2010, 0) AS POP_2010#6998L, features#6827.properties.ZCTA10 AS ZCTA10#6860, coalesce(features#6827.properties.HOUSING10, 0) AS HOUSING10#6999L]\n",
      "                                          :                    +- Filter (atleastnnonnulls(2, features#6827.properties.COMM, features#6827.properties.ZCTA10) AND (isnotnull(features#6827.properties.ZCTA10) AND isnotnull(features#6827.properties.COMM)))\n",
      "                                          :                       +- Generate explode(features#6819), false, [features#6827]\n",
      "                                          :                          +- Filter ((size(features#6819, true) > 0) AND isnotnull(features#6819))\n",
      "                                          :                             +- FileScan geojson [features#6819] Batched: false, DataFilters: [(size(features#6819, true) > 0), isnotnull(features#6819)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                                          +- Generate explode(features#7214), false, [features#6827]\n",
      "                                             +- Filter ((size(features#7214, true) > 0) AND isnotnull(features#7214))\n",
      "                                                +- FileScan geojson [features#7214] Batched: false, DataFilters: [(size(features#7214, true) > 0), isnotnull(features#7214)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "res.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "676a25f0-f0ef-4472-82e4-d9a9ab035061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.73 seconds\n",
      "+--------------------+------------------+--------------------+\n",
      "|                COMM|    Average Income|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|      Marina del Rey| 76428.84908639747| 0.16078790655061842|\n",
      "|   Pacific Palisades| 70656.11180545464|  0.4720770986558458|\n",
      "|              Malibu|  67135.0118623962|0.003460207612456...|\n",
      "| Palisades Highlands| 66867.44038612054|  0.2055830941821028|\n",
      "|    Marina Peninsula|65235.692875259396|  0.6549938347718866|\n",
      "|             Bel Air| 63041.33942621959| 0.43199608610567514|\n",
      "|Palos Verdes Estates| 61905.61214466438|0.008547008547008548|\n",
      "|     Manhattan Beach|60985.189241497086|0.033648790746582544|\n",
      "|       Beverly Crest| 60947.48978754819| 0.37490683229813665|\n",
      "|           Brentwood| 60840.62462032012|  0.5346764258279586|\n",
      "|       Hermosa Beach| 57924.85594176151|0.016216216216216217|\n",
      "|   Mandeville Canyon| 55572.11011444479|  0.2716207559256887|\n",
      "|La Cañada Flintridge| 54900.65682110046| 0.01282051282051282|\n",
      "|       Beverly Hills| 51303.11237503298|  0.4119975639464068|\n",
      "|         Playa Vista| 50264.47143177235|  0.8175387379294857|\n",
      "|             Carthay|49841.163007975694|  1.0959014197696222|\n",
      "|      West Hollywood| 49414.42132038722| 0.13344267183539585|\n",
      "|              Venice|47614.884045977014|  1.3570434206165065|\n",
      "|       Redondo Beach| 47568.80637622101|0.050505050505050504|\n",
      "| Rancho Palos Verdes| 46151.89491631246| 0.28346456692913385|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "####query 3 HINT BROADCAST\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .dropna(subset=[\"COMM\",\"ZCTA10\"])  \\\n",
    "\n",
    "\n",
    "census = blocks_census \\\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"HOUSING10\") \\\n",
    "    .na.fill({\"POP_2010\": 0, \"HOUSING10\": 0}) \\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"TOTAL_POP_2010\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"TOTAL_HOUSING10\")\n",
    "    )\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,.]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"TOTAL_HOUSING10\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"TOTAL_POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    ".select(\"point\")\n",
    "\n",
    "\n",
    "# res2 = crime_dataset \\\n",
    "#     .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry),)\\\n",
    "# .select(\"COMM\",\"POP_2010\")\\\n",
    "# .groupBy(\"COMM\")\\\n",
    "# .agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    "# .withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census.select(\"geometry\",\"COMM\",\"POP_2010\"), ST_Within(crime_dataset.point, blocks_census.geometry)) \\\n",
    ".groupBy(\"geometry\",\"COMM\",\"POP_2010\") \\\n",
    ".agg(F.count(\"*\").alias(\"NumberOfCrimesPerBlock\"))\\\n",
    ".groupby(\"COMM\") \\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.sum(\"NumberOfCrimesPerBlock\").alias(\"NumberOfCrimes\") ) \\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2.hint(\"BROADCAST\"),res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\").orderBy(col(\"Average Income\").desc())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c4c25a5-36b1-4ac7-a23b-e65629a18f0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Average Income DESC NULLS LAST], true\n",
      "+- Project [COMM#5133 AS COMM#5549, Average Income#5373, Crimes per Person#5498]\n",
      "   +- Join Inner, (COMM#5133 = COMM#5514)\n",
      "      :- Project [COMM#5133, total_population#5367L, comm_total_income#5369, (comm_total_income#5369 / cast(total_population#5367L as double)) AS Average Income#5373]\n",
      "      :  +- Aggregate [COMM#5133], [COMM#5133, sum(TOTAL_POP_2010#5299L) AS total_population#5367L, sum(total_income#5349) AS comm_total_income#5369]\n",
      "      :     +- Project [Zip Code#5324, Community#5325, Estimated Median Income#5330, COMM#5133, ZCTA10#5150, TOTAL_POP_2010#5299L, TOTAL_HOUSING10#5301L, (Estimated Median Income#5330 * cast(TOTAL_HOUSING10#5301L as float)) AS total_income#5349]\n",
      "      :        +- Join Inner, (ZCTA10#5150 = Zip Code#5324)\n",
      "      :           :- Project [Zip Code#5324, Community#5325, cast(regexp_replace(Estimated Median Income#5326, [$,.], , 1) as float) AS Estimated Median Income#5330]\n",
      "      :           :  +- Relation [Zip Code#5324,Community#5325,Estimated Median Income#5326] csv\n",
      "      :           +- Aggregate [COMM#5133, ZCTA10#5150], [COMM#5133, ZCTA10#5150, sum(POP_2010#5288L) AS TOTAL_POP_2010#5299L, sum(HOUSING10#5289L) AS TOTAL_HOUSING10#5301L]\n",
      "      :              +- Project [COMM#5133, coalesce(POP_2010#5142L, cast(0 as bigint)) AS POP_2010#5288L, ZCTA10#5150, coalesce(HOUSING10#5139L, cast(0 as bigint)) AS HOUSING10#5289L]\n",
      "      :                 +- Project [COMM#5133, POP_2010#5142L, ZCTA10#5150, HOUSING10#5139L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#5133, ZCTA10#5150)\n",
      "      :                       +- Project [properties#5121.BG10 AS BG10#5126, properties#5121.BG10FIP10 AS BG10FIP10#5127, properties#5121.BG12 AS BG12#5128, properties#5121.CB10 AS CB10#5129, properties#5121.CEN_FIP13 AS CEN_FIP13#5130, properties#5121.CITY AS CITY#5131, properties#5121.CITYCOM AS CITYCOM#5132, properties#5121.COMM AS COMM#5133, properties#5121.CT10 AS CT10#5134, properties#5121.CT12 AS CT12#5135, properties#5121.CTCB10 AS CTCB10#5136, properties#5121.HD_2012 AS HD_2012#5137L, properties#5121.HD_NAME AS HD_NAME#5138, properties#5121.HOUSING10 AS HOUSING10#5139L, properties#5121.LA_FIP10 AS LA_FIP10#5140, properties#5121.OBJECTID AS OBJECTID#5141L, properties#5121.POP_2010 AS POP_2010#5142L, properties#5121.PUMA10 AS PUMA10#5143, properties#5121.SPA_2012 AS SPA_2012#5144L, properties#5121.SPA_NAME AS SPA_NAME#5145, properties#5121.SUP_DIST AS SUP_DIST#5146, properties#5121.SUP_LABEL AS SUP_LABEL#5147, properties#5121.ShapeSTArea AS ShapeSTArea#5148, properties#5121.ShapeSTLength AS ShapeSTLength#5149, ... 2 more fields]\n",
      "      :                          +- Project [features#5117.geometry AS geometry#5120, features#5117.properties AS properties#5121, features#5117.type AS type#5122]\n",
      "      :                             +- Project [features#5117]\n",
      "      :                                +- Generate explode(features#5109), false, [features#5117]\n",
      "      :                                   +- Relation [crs#5108,features#5109,name#5110,type#5111] geojson\n",
      "      +- ResolvedHint (strategy=broadcast)\n",
      "         +- Project [COMM#5514, TotalPopulation#5492L, NumberOfCrimes#5494L, (cast(NumberOfCrimes#5494L as double) / cast(TotalPopulation#5492L as double)) AS Crimes per Person#5498]\n",
      "            +- Aggregate [COMM#5514], [COMM#5514, sum(POP_2010#5523L) AS TotalPopulation#5492L, sum(NumberOfCrimesPerBlock#5482L) AS NumberOfCrimes#5494L]\n",
      "               +- Aggregate [geometry#5120, COMM#5514, POP_2010#5523L], [geometry#5120, COMM#5514, POP_2010#5523L, count(1) AS NumberOfCrimesPerBlock#5482L]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                     :- Project [point#5435]\n",
      "                     :  +- Project [DR_NO#5378, Date Rptd#5379, DATE OCC#5380, TIME OCC#5381, AREA #5382, AREA NAME#5383, Rpt Dist No#5384, Part 1-2#5385, Crm Cd#5386, Crm Cd Desc#5387, Mocodes#5388, Vict Age#5389, Vict Sex#5390, Vict Descent#5391, Premis Cd#5392, Premis Desc#5393, Weapon Used Cd#5394, Weapon Desc#5395, Status#5396, Status Desc#5397, Crm Cd 1#5398, Crm Cd 2#5399, Crm Cd 3#5400, Crm Cd 4#5401, ... 5 more fields]\n",
      "                     :     +- Filter (NOT (cast(LON#5405 as int) = 0) OR NOT (cast(LAT#5404 as int) = 0))\n",
      "                     :        +- Relation [DR_NO#5378,Date Rptd#5379,DATE OCC#5380,TIME OCC#5381,AREA #5382,AREA NAME#5383,Rpt Dist No#5384,Part 1-2#5385,Crm Cd#5386,Crm Cd Desc#5387,Mocodes#5388,Vict Age#5389,Vict Sex#5390,Vict Descent#5391,Premis Cd#5392,Premis Desc#5393,Weapon Used Cd#5394,Weapon Desc#5395,Status#5396,Status Desc#5397,Crm Cd 1#5398,Crm Cd 2#5399,Crm Cd 3#5400,Crm Cd 4#5401,... 4 more fields] parquet\n",
      "                     +- Project [geometry#5120, COMM#5514, POP_2010#5523L]\n",
      "                        +- Filter atleastnnonnulls(2, COMM#5514, ZCTA10#5531)\n",
      "                           +- Project [properties#5121.BG10 AS BG10#5507, properties#5121.BG10FIP10 AS BG10FIP10#5508, properties#5121.BG12 AS BG12#5509, properties#5121.CB10 AS CB10#5510, properties#5121.CEN_FIP13 AS CEN_FIP13#5511, properties#5121.CITY AS CITY#5512, properties#5121.CITYCOM AS CITYCOM#5513, properties#5121.COMM AS COMM#5514, properties#5121.CT10 AS CT10#5515, properties#5121.CT12 AS CT12#5516, properties#5121.CTCB10 AS CTCB10#5517, properties#5121.HD_2012 AS HD_2012#5518L, properties#5121.HD_NAME AS HD_NAME#5519, properties#5121.HOUSING10 AS HOUSING10#5520L, properties#5121.LA_FIP10 AS LA_FIP10#5521, properties#5121.OBJECTID AS OBJECTID#5522L, properties#5121.POP_2010 AS POP_2010#5523L, properties#5121.PUMA10 AS PUMA10#5524, properties#5121.SPA_2012 AS SPA_2012#5525L, properties#5121.SPA_NAME AS SPA_NAME#5526, properties#5121.SUP_DIST AS SUP_DIST#5527, properties#5121.SUP_LABEL AS SUP_LABEL#5528, properties#5121.ShapeSTArea AS ShapeSTArea#5529, properties#5121.ShapeSTLength AS ShapeSTLength#5530, ... 2 more fields]\n",
      "                              +- Project [features#5117.geometry AS geometry#5120, features#5117.properties AS properties#5121, features#5117.type AS type#5122]\n",
      "                                 +- Project [features#5117]\n",
      "                                    +- Generate explode(features#5504), false, [features#5117]\n",
      "                                       +- Relation [crs#5503,features#5504,name#5505,type#5506] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Average Income: double, Crimes per Person: double\n",
      "Sort [Average Income#5373 DESC NULLS LAST], true\n",
      "+- Project [COMM#5133 AS COMM#5549, Average Income#5373, Crimes per Person#5498]\n",
      "   +- Join Inner, (COMM#5133 = COMM#5514)\n",
      "      :- Project [COMM#5133, total_population#5367L, comm_total_income#5369, (comm_total_income#5369 / cast(total_population#5367L as double)) AS Average Income#5373]\n",
      "      :  +- Aggregate [COMM#5133], [COMM#5133, sum(TOTAL_POP_2010#5299L) AS total_population#5367L, sum(total_income#5349) AS comm_total_income#5369]\n",
      "      :     +- Project [Zip Code#5324, Community#5325, Estimated Median Income#5330, COMM#5133, ZCTA10#5150, TOTAL_POP_2010#5299L, TOTAL_HOUSING10#5301L, (Estimated Median Income#5330 * cast(TOTAL_HOUSING10#5301L as float)) AS total_income#5349]\n",
      "      :        +- Join Inner, (ZCTA10#5150 = Zip Code#5324)\n",
      "      :           :- Project [Zip Code#5324, Community#5325, cast(regexp_replace(Estimated Median Income#5326, [$,.], , 1) as float) AS Estimated Median Income#5330]\n",
      "      :           :  +- Relation [Zip Code#5324,Community#5325,Estimated Median Income#5326] csv\n",
      "      :           +- Aggregate [COMM#5133, ZCTA10#5150], [COMM#5133, ZCTA10#5150, sum(POP_2010#5288L) AS TOTAL_POP_2010#5299L, sum(HOUSING10#5289L) AS TOTAL_HOUSING10#5301L]\n",
      "      :              +- Project [COMM#5133, coalesce(POP_2010#5142L, cast(0 as bigint)) AS POP_2010#5288L, ZCTA10#5150, coalesce(HOUSING10#5139L, cast(0 as bigint)) AS HOUSING10#5289L]\n",
      "      :                 +- Project [COMM#5133, POP_2010#5142L, ZCTA10#5150, HOUSING10#5139L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#5133, ZCTA10#5150)\n",
      "      :                       +- Project [properties#5121.BG10 AS BG10#5126, properties#5121.BG10FIP10 AS BG10FIP10#5127, properties#5121.BG12 AS BG12#5128, properties#5121.CB10 AS CB10#5129, properties#5121.CEN_FIP13 AS CEN_FIP13#5130, properties#5121.CITY AS CITY#5131, properties#5121.CITYCOM AS CITYCOM#5132, properties#5121.COMM AS COMM#5133, properties#5121.CT10 AS CT10#5134, properties#5121.CT12 AS CT12#5135, properties#5121.CTCB10 AS CTCB10#5136, properties#5121.HD_2012 AS HD_2012#5137L, properties#5121.HD_NAME AS HD_NAME#5138, properties#5121.HOUSING10 AS HOUSING10#5139L, properties#5121.LA_FIP10 AS LA_FIP10#5140, properties#5121.OBJECTID AS OBJECTID#5141L, properties#5121.POP_2010 AS POP_2010#5142L, properties#5121.PUMA10 AS PUMA10#5143, properties#5121.SPA_2012 AS SPA_2012#5144L, properties#5121.SPA_NAME AS SPA_NAME#5145, properties#5121.SUP_DIST AS SUP_DIST#5146, properties#5121.SUP_LABEL AS SUP_LABEL#5147, properties#5121.ShapeSTArea AS ShapeSTArea#5148, properties#5121.ShapeSTLength AS ShapeSTLength#5149, ... 2 more fields]\n",
      "      :                          +- Project [features#5117.geometry AS geometry#5120, features#5117.properties AS properties#5121, features#5117.type AS type#5122]\n",
      "      :                             +- Project [features#5117]\n",
      "      :                                +- Generate explode(features#5109), false, [features#5117]\n",
      "      :                                   +- Relation [crs#5108,features#5109,name#5110,type#5111] geojson\n",
      "      +- ResolvedHint (strategy=broadcast)\n",
      "         +- Project [COMM#5514, TotalPopulation#5492L, NumberOfCrimes#5494L, (cast(NumberOfCrimes#5494L as double) / cast(TotalPopulation#5492L as double)) AS Crimes per Person#5498]\n",
      "            +- Aggregate [COMM#5514], [COMM#5514, sum(POP_2010#5523L) AS TotalPopulation#5492L, sum(NumberOfCrimesPerBlock#5482L) AS NumberOfCrimes#5494L]\n",
      "               +- Aggregate [geometry#5120, COMM#5514, POP_2010#5523L], [geometry#5120, COMM#5514, POP_2010#5523L, count(1) AS NumberOfCrimesPerBlock#5482L]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                     :- Project [point#5435]\n",
      "                     :  +- Project [DR_NO#5378, Date Rptd#5379, DATE OCC#5380, TIME OCC#5381, AREA #5382, AREA NAME#5383, Rpt Dist No#5384, Part 1-2#5385, Crm Cd#5386, Crm Cd Desc#5387, Mocodes#5388, Vict Age#5389, Vict Sex#5390, Vict Descent#5391, Premis Cd#5392, Premis Desc#5393, Weapon Used Cd#5394, Weapon Desc#5395, Status#5396, Status Desc#5397, Crm Cd 1#5398, Crm Cd 2#5399, Crm Cd 3#5400, Crm Cd 4#5401, ... 5 more fields]\n",
      "                     :     +- Filter (NOT (cast(LON#5405 as int) = 0) OR NOT (cast(LAT#5404 as int) = 0))\n",
      "                     :        +- Relation [DR_NO#5378,Date Rptd#5379,DATE OCC#5380,TIME OCC#5381,AREA #5382,AREA NAME#5383,Rpt Dist No#5384,Part 1-2#5385,Crm Cd#5386,Crm Cd Desc#5387,Mocodes#5388,Vict Age#5389,Vict Sex#5390,Vict Descent#5391,Premis Cd#5392,Premis Desc#5393,Weapon Used Cd#5394,Weapon Desc#5395,Status#5396,Status Desc#5397,Crm Cd 1#5398,Crm Cd 2#5399,Crm Cd 3#5400,Crm Cd 4#5401,... 4 more fields] parquet\n",
      "                     +- Project [geometry#5120, COMM#5514, POP_2010#5523L]\n",
      "                        +- Filter atleastnnonnulls(2, COMM#5514, ZCTA10#5531)\n",
      "                           +- Project [properties#5121.BG10 AS BG10#5507, properties#5121.BG10FIP10 AS BG10FIP10#5508, properties#5121.BG12 AS BG12#5509, properties#5121.CB10 AS CB10#5510, properties#5121.CEN_FIP13 AS CEN_FIP13#5511, properties#5121.CITY AS CITY#5512, properties#5121.CITYCOM AS CITYCOM#5513, properties#5121.COMM AS COMM#5514, properties#5121.CT10 AS CT10#5515, properties#5121.CT12 AS CT12#5516, properties#5121.CTCB10 AS CTCB10#5517, properties#5121.HD_2012 AS HD_2012#5518L, properties#5121.HD_NAME AS HD_NAME#5519, properties#5121.HOUSING10 AS HOUSING10#5520L, properties#5121.LA_FIP10 AS LA_FIP10#5521, properties#5121.OBJECTID AS OBJECTID#5522L, properties#5121.POP_2010 AS POP_2010#5523L, properties#5121.PUMA10 AS PUMA10#5524, properties#5121.SPA_2012 AS SPA_2012#5525L, properties#5121.SPA_NAME AS SPA_NAME#5526, properties#5121.SUP_DIST AS SUP_DIST#5527, properties#5121.SUP_LABEL AS SUP_LABEL#5528, properties#5121.ShapeSTArea AS ShapeSTArea#5529, properties#5121.ShapeSTLength AS ShapeSTLength#5530, ... 2 more fields]\n",
      "                              +- Project [features#5117.geometry AS geometry#5120, features#5117.properties AS properties#5121, features#5117.type AS type#5122]\n",
      "                                 +- Project [features#5117]\n",
      "                                    +- Generate explode(features#5504), false, [features#5117]\n",
      "                                       +- Relation [crs#5503,features#5504,name#5505,type#5506] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Average Income#5373 DESC NULLS LAST], true\n",
      "+- Project [COMM#5133, Average Income#5373, Crimes per Person#5498]\n",
      "   +- Join Inner, (COMM#5133 = COMM#5514), rightHint=(strategy=broadcast)\n",
      "      :- Aggregate [COMM#5133], [COMM#5133, (sum(total_income#5349) / cast(sum(TOTAL_POP_2010#5299L) as double)) AS Average Income#5373]\n",
      "      :  +- Project [COMM#5133, TOTAL_POP_2010#5299L, (Estimated Median Income#5330 * cast(TOTAL_HOUSING10#5301L as float)) AS total_income#5349]\n",
      "      :     +- Join Inner, (ZCTA10#5150 = Zip Code#5324)\n",
      "      :        :- Project [Zip Code#5324, cast(regexp_replace(Estimated Median Income#5326, [$,.], , 1) as float) AS Estimated Median Income#5330]\n",
      "      :        :  +- Filter isnotnull(Zip Code#5324)\n",
      "      :        :     +- Relation [Zip Code#5324,Community#5325,Estimated Median Income#5326] csv\n",
      "      :        +- Aggregate [COMM#5133, ZCTA10#5150], [COMM#5133, ZCTA10#5150, sum(POP_2010#5288L) AS TOTAL_POP_2010#5299L, sum(HOUSING10#5289L) AS TOTAL_HOUSING10#5301L]\n",
      "      :           +- Project [features#5117.properties.COMM AS COMM#5133, coalesce(features#5117.properties.POP_2010, 0) AS POP_2010#5288L, features#5117.properties.ZCTA10 AS ZCTA10#5150, coalesce(features#5117.properties.HOUSING10, 0) AS HOUSING10#5289L]\n",
      "      :              +- Filter (atleastnnonnulls(2, features#5117.properties.COMM, features#5117.properties.ZCTA10) AND (isnotnull(features#5117.properties.ZCTA10) AND isnotnull(features#5117.properties.COMM)))\n",
      "      :                 +- Generate explode(features#5109), [0], false, [features#5117]\n",
      "      :                    +- Project [features#5109]\n",
      "      :                       +- Filter ((size(features#5109, true) > 0) AND isnotnull(features#5109))\n",
      "      :                          +- Relation [crs#5108,features#5109,name#5110,type#5111] geojson\n",
      "      +- Aggregate [COMM#5514], [COMM#5514, (cast(sum(NumberOfCrimesPerBlock#5482L) as double) / cast(sum(POP_2010#5523L) as double)) AS Crimes per Person#5498]\n",
      "         +- Aggregate [geometry#5120, COMM#5514, POP_2010#5523L], [COMM#5514, POP_2010#5523L, count(1) AS NumberOfCrimesPerBlock#5482L]\n",
      "            +- Project [geometry#5120, COMM#5514, POP_2010#5523L]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#5435]\n",
      "                  :  +- Filter ((NOT (cast(LON#5405 as int) = 0) OR NOT (cast(LAT#5404 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :     +- Relation [DR_NO#5378,Date Rptd#5379,DATE OCC#5380,TIME OCC#5381,AREA #5382,AREA NAME#5383,Rpt Dist No#5384,Part 1-2#5385,Crm Cd#5386,Crm Cd Desc#5387,Mocodes#5388,Vict Age#5389,Vict Sex#5390,Vict Descent#5391,Premis Cd#5392,Premis Desc#5393,Weapon Used Cd#5394,Weapon Desc#5395,Status#5396,Status Desc#5397,Crm Cd 1#5398,Crm Cd 2#5399,Crm Cd 3#5400,Crm Cd 4#5401,... 4 more fields] parquet\n",
      "                  +- Project [features#5117.geometry AS geometry#5120, features#5117.properties.COMM AS COMM#5514, features#5117.properties.POP_2010 AS POP_2010#5523L]\n",
      "                     +- Filter (((atleastnnonnulls(2, features#5117.properties.COMM, features#5117.properties.ZCTA10) AND isnotnull(features#5117.geometry)) AND isnotnull(features#5117.properties.COMM)) AND bloomfilter#5636 of [COMM#5133] filtering [features#5117.properties.COMM])\n",
      "                        :  +- Aggregate [COMM#5133, ZCTA10#5150], [COMM#5133, ZCTA10#5150, sum(POP_2010#5288L) AS TOTAL_POP_2010#5299L, sum(HOUSING10#5289L) AS TOTAL_HOUSING10#5301L]\n",
      "                        :     +- Project [features#5117.properties.COMM AS COMM#5133, coalesce(features#5117.properties.POP_2010, 0) AS POP_2010#5288L, features#5117.properties.ZCTA10 AS ZCTA10#5150, coalesce(features#5117.properties.HOUSING10, 0) AS HOUSING10#5289L]\n",
      "                        :        +- Filter (atleastnnonnulls(2, features#5117.properties.COMM, features#5117.properties.ZCTA10) AND (isnotnull(features#5117.properties.ZCTA10) AND isnotnull(features#5117.properties.COMM)))\n",
      "                        :           +- Generate explode(features#5109), [0], false, [features#5117]\n",
      "                        :              +- Project [features#5109]\n",
      "                        :                 +- Filter ((size(features#5109, true) > 0) AND isnotnull(features#5109))\n",
      "                        :                    +- Relation [crs#5108,features#5109,name#5110,type#5111] geojson\n",
      "                        +- Generate explode(features#5504), [0], false, [features#5117]\n",
      "                           +- Project [features#5504]\n",
      "                              +- Filter ((size(features#5504, true) > 0) AND isnotnull(features#5504))\n",
      "                                 +- Relation [crs#5503,features#5504,name#5505,type#5506] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Average Income#5373 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Average Income#5373 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=11166]\n",
      "      +- Project [COMM#5133, Average Income#5373, Crimes per Person#5498]\n",
      "         +- BroadcastHashJoin [COMM#5133], [COMM#5514], Inner, BuildRight, false\n",
      "            :- HashAggregate(keys=[COMM#5133], functions=[sum(total_income#5349), sum(TOTAL_POP_2010#5299L)], output=[COMM#5133, Average Income#5373], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(COMM#5133, 1000), ENSURE_REQUIREMENTS, [plan_id=11046]\n",
      "            :     +- HashAggregate(keys=[COMM#5133], functions=[partial_sum(total_income#5349), partial_sum(TOTAL_POP_2010#5299L)], output=[COMM#5133, sum#5564, sum#5566L], schema specialized)\n",
      "            :        +- Project [COMM#5133, TOTAL_POP_2010#5299L, (Estimated Median Income#5330 * cast(TOTAL_HOUSING10#5301L as float)) AS total_income#5349]\n",
      "            :           +- BroadcastHashJoin [Zip Code#5324], [ZCTA10#5150], Inner, BuildLeft, false\n",
      "            :              :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=11041]\n",
      "            :              :  +- Project [Zip Code#5324, cast(regexp_replace(Estimated Median Income#5326, [$,.], , 1) as float) AS Estimated Median Income#5330]\n",
      "            :              :     +- Filter isnotnull(Zip Code#5324)\n",
      "            :              :        +- FileScan csv [Zip Code#5324,Estimated Median Income#5326] Batched: false, DataFilters: [isnotnull(Zip Code#5324)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            :              +- HashAggregate(keys=[COMM#5133, ZCTA10#5150], functions=[sum(POP_2010#5288L), sum(HOUSING10#5289L)], output=[COMM#5133, ZCTA10#5150, TOTAL_POP_2010#5299L, TOTAL_HOUSING10#5301L], schema specialized)\n",
      "            :                 +- Exchange hashpartitioning(COMM#5133, ZCTA10#5150, 1000), ENSURE_REQUIREMENTS, [plan_id=11038]\n",
      "            :                    +- HashAggregate(keys=[COMM#5133, ZCTA10#5150], functions=[partial_sum(POP_2010#5288L), partial_sum(HOUSING10#5289L)], output=[COMM#5133, ZCTA10#5150, sum#5568L, sum#5570L], schema specialized)\n",
      "            :                       +- Project [features#5117.properties.COMM AS COMM#5133, coalesce(features#5117.properties.POP_2010, 0) AS POP_2010#5288L, features#5117.properties.ZCTA10 AS ZCTA10#5150, coalesce(features#5117.properties.HOUSING10, 0) AS HOUSING10#5289L]\n",
      "            :                          +- Filter (atleastnnonnulls(2, features#5117.properties.COMM, features#5117.properties.ZCTA10) AND (isnotnull(features#5117.properties.ZCTA10) AND isnotnull(features#5117.properties.COMM)))\n",
      "            :                             +- Generate explode(features#5109), false, [features#5117]\n",
      "            :                                +- Filter ((size(features#5109, true) > 0) AND isnotnull(features#5109))\n",
      "            :                                   +- FileScan geojson [features#5109] Batched: false, DataFilters: [(size(features#5109, true) > 0), isnotnull(features#5109)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=11163]\n",
      "               +- HashAggregate(keys=[COMM#5514], functions=[sum(NumberOfCrimesPerBlock#5482L), sum(POP_2010#5523L)], output=[COMM#5514, Crimes per Person#5498], schema specialized)\n",
      "                  +- Exchange hashpartitioning(COMM#5514, 1000), ENSURE_REQUIREMENTS, [plan_id=11161]\n",
      "                     +- HashAggregate(keys=[COMM#5514], functions=[partial_sum(NumberOfCrimesPerBlock#5482L), partial_sum(POP_2010#5523L)], output=[COMM#5514, sum#5572L, sum#5574L], schema specialized)\n",
      "                        +- HashAggregate(keys=[geometry#5120, COMM#5514, POP_2010#5523L], functions=[count(1)], output=[COMM#5514, POP_2010#5523L, NumberOfCrimesPerBlock#5482L])\n",
      "                           +- Exchange hashpartitioning(geometry#5120, COMM#5514, POP_2010#5523L, 1000), ENSURE_REQUIREMENTS, [plan_id=11158]\n",
      "                              +- HashAggregate(keys=[geometry#5120, COMM#5514, POP_2010#5523L], functions=[partial_count(1)], output=[geometry#5120, COMM#5514, POP_2010#5523L, count#5576L])\n",
      "                                 +- Project [geometry#5120, COMM#5514, POP_2010#5523L]\n",
      "                                    +- RangeJoin point#5435: geometry, geometry#5120: geometry, WITHIN\n",
      "                                       :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#5435]\n",
      "                                       :  +- Filter ((NOT (cast(LON#5405 as int) = 0) OR NOT (cast(LAT#5404 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                       :     +- FileScan parquet [LAT#5404,LON#5405] Batched: true, DataFilters: [(NOT (cast(LON#5405 as int) = 0) OR NOT (cast(LAT#5404 as int) = 0)), isnotnull( **org.apache.sp..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                                       +- Project [features#5117.geometry AS geometry#5120, features#5117.properties.COMM AS COMM#5514, features#5117.properties.POP_2010 AS POP_2010#5523L]\n",
      "                                          +- Filter (((atleastnnonnulls(2, features#5117.properties.COMM, features#5117.properties.ZCTA10) AND isnotnull(features#5117.geometry)) AND isnotnull(features#5117.properties.COMM)) AND bloomfilter#5636 of [bf5636 COMM#5133 estimatedNumRows=294857] filtering [features#5117.properties.COMM])\n",
      "                                             :  +- GenerateBloomFilter bf5636, 294857, 0, false, false, false, true, [id=#11152]\n",
      "                                             :     +- OutputAdapter [COMM#5133, ZCTA10#5150, sum#5568L, sum#5570L]\n",
      "                                             :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                                             :           +- Exchange hashpartitioning(COMM#5133, ZCTA10#5150, 1000), ENSURE_REQUIREMENTS, [plan_id=11149]\n",
      "                                             :              +- HashAggregate(keys=[COMM#5133, ZCTA10#5150], functions=[partial_sum(POP_2010#5288L), partial_sum(HOUSING10#5289L)], output=[COMM#5133, ZCTA10#5150, sum#5568L, sum#5570L], schema specialized)\n",
      "                                             :                 +- Project [features#5117.properties.COMM AS COMM#5133, coalesce(features#5117.properties.POP_2010, 0) AS POP_2010#5288L, features#5117.properties.ZCTA10 AS ZCTA10#5150, coalesce(features#5117.properties.HOUSING10, 0) AS HOUSING10#5289L]\n",
      "                                             :                    +- Filter (atleastnnonnulls(2, features#5117.properties.COMM, features#5117.properties.ZCTA10) AND (isnotnull(features#5117.properties.ZCTA10) AND isnotnull(features#5117.properties.COMM)))\n",
      "                                             :                       +- Generate explode(features#5109), false, [features#5117]\n",
      "                                             :                          +- Filter ((size(features#5109, true) > 0) AND isnotnull(features#5109))\n",
      "                                             :                             +- FileScan geojson [features#5109] Batched: false, DataFilters: [(size(features#5109, true) > 0), isnotnull(features#5109)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                                             +- Generate explode(features#5504), false, [features#5117]\n",
      "                                                +- Filter ((size(features#5504, true) > 0) AND isnotnull(features#5504))\n",
      "                                                   +- FileScan geojson [features#5504] Batched: false, DataFilters: [(size(features#5504, true) > 0), isnotnull(features#5504)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "res.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "160e9d33-09a3-4062-870a-e8a66c8d53d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.05 seconds\n",
      "+--------------------+------------------+--------------------+\n",
      "|                COMM|    Average Income|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|      Marina del Rey| 76428.84908639747| 0.16078790655061842|\n",
      "|   Pacific Palisades| 70656.11180545464|  0.4720770986558458|\n",
      "|              Malibu|  67135.0118623962|0.003460207612456...|\n",
      "| Palisades Highlands| 66867.44038612054|  0.2055830941821028|\n",
      "|    Marina Peninsula|65235.692875259396|  0.6549938347718866|\n",
      "|             Bel Air| 63041.33942621959| 0.43199608610567514|\n",
      "|Palos Verdes Estates| 61905.61214466438|0.008547008547008548|\n",
      "|     Manhattan Beach|60985.189241497086|0.033648790746582544|\n",
      "|       Beverly Crest| 60947.48978754819| 0.37490683229813665|\n",
      "|           Brentwood| 60840.62462032012|  0.5346764258279586|\n",
      "|       Hermosa Beach| 57924.85594176151|0.016216216216216217|\n",
      "|   Mandeville Canyon| 55572.11011444479|  0.2716207559256887|\n",
      "|La Cañada Flintridge| 54900.65682110046| 0.01282051282051282|\n",
      "|       Beverly Hills| 51303.11237503298|  0.4119975639464068|\n",
      "|         Playa Vista| 50264.47143177235|  0.8175387379294857|\n",
      "|             Carthay|49841.163007975694|  1.0959014197696222|\n",
      "|      West Hollywood| 49414.42132038722| 0.13344267183539585|\n",
      "|              Venice|47614.884045977014|  1.3570434206165065|\n",
      "|       Redondo Beach| 47568.80637622101|0.050505050505050504|\n",
      "| Rancho Palos Verdes| 46151.89491631246| 0.28346456692913385|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "####query 3 HINT SHUFFLE REPLICATE NL\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .dropna(subset=[\"COMM\",\"ZCTA10\"])  \\\n",
    "\n",
    "\n",
    "census = blocks_census \\\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"HOUSING10\") \\\n",
    "    .na.fill({\"POP_2010\": 0, \"HOUSING10\": 0}) \\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"TOTAL_POP_2010\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"TOTAL_HOUSING10\")\n",
    "    )\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,.]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"TOTAL_HOUSING10\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"TOTAL_POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    ".select(\"point\")\n",
    "\n",
    "\n",
    "# res2 = crime_dataset \\\n",
    "#     .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry),)\\\n",
    "# .select(\"COMM\",\"POP_2010\")\\\n",
    "# .groupBy(\"COMM\")\\\n",
    "# .agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    "# .withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census.select(\"geometry\",\"COMM\",\"POP_2010\"), ST_Within(crime_dataset.point, blocks_census.geometry)) \\\n",
    ".groupBy(\"geometry\",\"COMM\",\"POP_2010\") \\\n",
    ".agg(F.count(\"*\").alias(\"NumberOfCrimesPerBlock\"))\\\n",
    ".groupby(\"COMM\") \\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.sum(\"NumberOfCrimesPerBlock\").alias(\"NumberOfCrimes\") ) \\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2.hint(\"SHUFFLE_REPLICATE_NL\"),res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\").orderBy(col(\"Average Income\").desc())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "affb2fe7-b57a-44c0-8817-ce1f36d4a418",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Average Income DESC NULLS LAST], true\n",
      "+- Project [COMM#9637 AS COMM#10053, Average Income#9877, Crimes per Person#10002]\n",
      "   +- Join Inner, (COMM#9637 = COMM#10018)\n",
      "      :- Project [COMM#9637, total_population#9871L, comm_total_income#9873, (comm_total_income#9873 / cast(total_population#9871L as double)) AS Average Income#9877]\n",
      "      :  +- Aggregate [COMM#9637], [COMM#9637, sum(TOTAL_POP_2010#9803L) AS total_population#9871L, sum(total_income#9853) AS comm_total_income#9873]\n",
      "      :     +- Project [Zip Code#9828, Community#9829, Estimated Median Income#9834, COMM#9637, ZCTA10#9654, TOTAL_POP_2010#9803L, TOTAL_HOUSING10#9805L, (Estimated Median Income#9834 * cast(TOTAL_HOUSING10#9805L as float)) AS total_income#9853]\n",
      "      :        +- Join Inner, (ZCTA10#9654 = Zip Code#9828)\n",
      "      :           :- Project [Zip Code#9828, Community#9829, cast(regexp_replace(Estimated Median Income#9830, [$,.], , 1) as float) AS Estimated Median Income#9834]\n",
      "      :           :  +- Relation [Zip Code#9828,Community#9829,Estimated Median Income#9830] csv\n",
      "      :           +- Aggregate [COMM#9637, ZCTA10#9654], [COMM#9637, ZCTA10#9654, sum(POP_2010#9792L) AS TOTAL_POP_2010#9803L, sum(HOUSING10#9793L) AS TOTAL_HOUSING10#9805L]\n",
      "      :              +- Project [COMM#9637, coalesce(POP_2010#9646L, cast(0 as bigint)) AS POP_2010#9792L, ZCTA10#9654, coalesce(HOUSING10#9643L, cast(0 as bigint)) AS HOUSING10#9793L]\n",
      "      :                 +- Project [COMM#9637, POP_2010#9646L, ZCTA10#9654, HOUSING10#9643L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#9637, ZCTA10#9654)\n",
      "      :                       +- Project [properties#9625.BG10 AS BG10#9630, properties#9625.BG10FIP10 AS BG10FIP10#9631, properties#9625.BG12 AS BG12#9632, properties#9625.CB10 AS CB10#9633, properties#9625.CEN_FIP13 AS CEN_FIP13#9634, properties#9625.CITY AS CITY#9635, properties#9625.CITYCOM AS CITYCOM#9636, properties#9625.COMM AS COMM#9637, properties#9625.CT10 AS CT10#9638, properties#9625.CT12 AS CT12#9639, properties#9625.CTCB10 AS CTCB10#9640, properties#9625.HD_2012 AS HD_2012#9641L, properties#9625.HD_NAME AS HD_NAME#9642, properties#9625.HOUSING10 AS HOUSING10#9643L, properties#9625.LA_FIP10 AS LA_FIP10#9644, properties#9625.OBJECTID AS OBJECTID#9645L, properties#9625.POP_2010 AS POP_2010#9646L, properties#9625.PUMA10 AS PUMA10#9647, properties#9625.SPA_2012 AS SPA_2012#9648L, properties#9625.SPA_NAME AS SPA_NAME#9649, properties#9625.SUP_DIST AS SUP_DIST#9650, properties#9625.SUP_LABEL AS SUP_LABEL#9651, properties#9625.ShapeSTArea AS ShapeSTArea#9652, properties#9625.ShapeSTLength AS ShapeSTLength#9653, ... 2 more fields]\n",
      "      :                          +- Project [features#9621.geometry AS geometry#9624, features#9621.properties AS properties#9625, features#9621.type AS type#9626]\n",
      "      :                             +- Project [features#9621]\n",
      "      :                                +- Generate explode(features#9613), false, [features#9621]\n",
      "      :                                   +- Relation [crs#9612,features#9613,name#9614,type#9615] geojson\n",
      "      +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "         +- Project [COMM#10018, TotalPopulation#9996L, NumberOfCrimes#9998L, (cast(NumberOfCrimes#9998L as double) / cast(TotalPopulation#9996L as double)) AS Crimes per Person#10002]\n",
      "            +- Aggregate [COMM#10018], [COMM#10018, sum(POP_2010#10027L) AS TotalPopulation#9996L, sum(NumberOfCrimesPerBlock#9986L) AS NumberOfCrimes#9998L]\n",
      "               +- Aggregate [geometry#9624, COMM#10018, POP_2010#10027L], [geometry#9624, COMM#10018, POP_2010#10027L, count(1) AS NumberOfCrimesPerBlock#9986L]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                     :- Project [point#9939]\n",
      "                     :  +- Project [DR_NO#9882, Date Rptd#9883, DATE OCC#9884, TIME OCC#9885, AREA #9886, AREA NAME#9887, Rpt Dist No#9888, Part 1-2#9889, Crm Cd#9890, Crm Cd Desc#9891, Mocodes#9892, Vict Age#9893, Vict Sex#9894, Vict Descent#9895, Premis Cd#9896, Premis Desc#9897, Weapon Used Cd#9898, Weapon Desc#9899, Status#9900, Status Desc#9901, Crm Cd 1#9902, Crm Cd 2#9903, Crm Cd 3#9904, Crm Cd 4#9905, ... 5 more fields]\n",
      "                     :     +- Filter (NOT (cast(LON#9909 as int) = 0) OR NOT (cast(LAT#9908 as int) = 0))\n",
      "                     :        +- Relation [DR_NO#9882,Date Rptd#9883,DATE OCC#9884,TIME OCC#9885,AREA #9886,AREA NAME#9887,Rpt Dist No#9888,Part 1-2#9889,Crm Cd#9890,Crm Cd Desc#9891,Mocodes#9892,Vict Age#9893,Vict Sex#9894,Vict Descent#9895,Premis Cd#9896,Premis Desc#9897,Weapon Used Cd#9898,Weapon Desc#9899,Status#9900,Status Desc#9901,Crm Cd 1#9902,Crm Cd 2#9903,Crm Cd 3#9904,Crm Cd 4#9905,... 4 more fields] parquet\n",
      "                     +- Project [geometry#9624, COMM#10018, POP_2010#10027L]\n",
      "                        +- Filter atleastnnonnulls(2, COMM#10018, ZCTA10#10035)\n",
      "                           +- Project [properties#9625.BG10 AS BG10#10011, properties#9625.BG10FIP10 AS BG10FIP10#10012, properties#9625.BG12 AS BG12#10013, properties#9625.CB10 AS CB10#10014, properties#9625.CEN_FIP13 AS CEN_FIP13#10015, properties#9625.CITY AS CITY#10016, properties#9625.CITYCOM AS CITYCOM#10017, properties#9625.COMM AS COMM#10018, properties#9625.CT10 AS CT10#10019, properties#9625.CT12 AS CT12#10020, properties#9625.CTCB10 AS CTCB10#10021, properties#9625.HD_2012 AS HD_2012#10022L, properties#9625.HD_NAME AS HD_NAME#10023, properties#9625.HOUSING10 AS HOUSING10#10024L, properties#9625.LA_FIP10 AS LA_FIP10#10025, properties#9625.OBJECTID AS OBJECTID#10026L, properties#9625.POP_2010 AS POP_2010#10027L, properties#9625.PUMA10 AS PUMA10#10028, properties#9625.SPA_2012 AS SPA_2012#10029L, properties#9625.SPA_NAME AS SPA_NAME#10030, properties#9625.SUP_DIST AS SUP_DIST#10031, properties#9625.SUP_LABEL AS SUP_LABEL#10032, properties#9625.ShapeSTArea AS ShapeSTArea#10033, properties#9625.ShapeSTLength AS ShapeSTLength#10034, ... 2 more fields]\n",
      "                              +- Project [features#9621.geometry AS geometry#9624, features#9621.properties AS properties#9625, features#9621.type AS type#9626]\n",
      "                                 +- Project [features#9621]\n",
      "                                    +- Generate explode(features#10008), false, [features#9621]\n",
      "                                       +- Relation [crs#10007,features#10008,name#10009,type#10010] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Average Income: double, Crimes per Person: double\n",
      "Sort [Average Income#9877 DESC NULLS LAST], true\n",
      "+- Project [COMM#9637 AS COMM#10053, Average Income#9877, Crimes per Person#10002]\n",
      "   +- Join Inner, (COMM#9637 = COMM#10018)\n",
      "      :- Project [COMM#9637, total_population#9871L, comm_total_income#9873, (comm_total_income#9873 / cast(total_population#9871L as double)) AS Average Income#9877]\n",
      "      :  +- Aggregate [COMM#9637], [COMM#9637, sum(TOTAL_POP_2010#9803L) AS total_population#9871L, sum(total_income#9853) AS comm_total_income#9873]\n",
      "      :     +- Project [Zip Code#9828, Community#9829, Estimated Median Income#9834, COMM#9637, ZCTA10#9654, TOTAL_POP_2010#9803L, TOTAL_HOUSING10#9805L, (Estimated Median Income#9834 * cast(TOTAL_HOUSING10#9805L as float)) AS total_income#9853]\n",
      "      :        +- Join Inner, (ZCTA10#9654 = Zip Code#9828)\n",
      "      :           :- Project [Zip Code#9828, Community#9829, cast(regexp_replace(Estimated Median Income#9830, [$,.], , 1) as float) AS Estimated Median Income#9834]\n",
      "      :           :  +- Relation [Zip Code#9828,Community#9829,Estimated Median Income#9830] csv\n",
      "      :           +- Aggregate [COMM#9637, ZCTA10#9654], [COMM#9637, ZCTA10#9654, sum(POP_2010#9792L) AS TOTAL_POP_2010#9803L, sum(HOUSING10#9793L) AS TOTAL_HOUSING10#9805L]\n",
      "      :              +- Project [COMM#9637, coalesce(POP_2010#9646L, cast(0 as bigint)) AS POP_2010#9792L, ZCTA10#9654, coalesce(HOUSING10#9643L, cast(0 as bigint)) AS HOUSING10#9793L]\n",
      "      :                 +- Project [COMM#9637, POP_2010#9646L, ZCTA10#9654, HOUSING10#9643L]\n",
      "      :                    +- Filter atleastnnonnulls(2, COMM#9637, ZCTA10#9654)\n",
      "      :                       +- Project [properties#9625.BG10 AS BG10#9630, properties#9625.BG10FIP10 AS BG10FIP10#9631, properties#9625.BG12 AS BG12#9632, properties#9625.CB10 AS CB10#9633, properties#9625.CEN_FIP13 AS CEN_FIP13#9634, properties#9625.CITY AS CITY#9635, properties#9625.CITYCOM AS CITYCOM#9636, properties#9625.COMM AS COMM#9637, properties#9625.CT10 AS CT10#9638, properties#9625.CT12 AS CT12#9639, properties#9625.CTCB10 AS CTCB10#9640, properties#9625.HD_2012 AS HD_2012#9641L, properties#9625.HD_NAME AS HD_NAME#9642, properties#9625.HOUSING10 AS HOUSING10#9643L, properties#9625.LA_FIP10 AS LA_FIP10#9644, properties#9625.OBJECTID AS OBJECTID#9645L, properties#9625.POP_2010 AS POP_2010#9646L, properties#9625.PUMA10 AS PUMA10#9647, properties#9625.SPA_2012 AS SPA_2012#9648L, properties#9625.SPA_NAME AS SPA_NAME#9649, properties#9625.SUP_DIST AS SUP_DIST#9650, properties#9625.SUP_LABEL AS SUP_LABEL#9651, properties#9625.ShapeSTArea AS ShapeSTArea#9652, properties#9625.ShapeSTLength AS ShapeSTLength#9653, ... 2 more fields]\n",
      "      :                          +- Project [features#9621.geometry AS geometry#9624, features#9621.properties AS properties#9625, features#9621.type AS type#9626]\n",
      "      :                             +- Project [features#9621]\n",
      "      :                                +- Generate explode(features#9613), false, [features#9621]\n",
      "      :                                   +- Relation [crs#9612,features#9613,name#9614,type#9615] geojson\n",
      "      +- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "         +- Project [COMM#10018, TotalPopulation#9996L, NumberOfCrimes#9998L, (cast(NumberOfCrimes#9998L as double) / cast(TotalPopulation#9996L as double)) AS Crimes per Person#10002]\n",
      "            +- Aggregate [COMM#10018], [COMM#10018, sum(POP_2010#10027L) AS TotalPopulation#9996L, sum(NumberOfCrimesPerBlock#9986L) AS NumberOfCrimes#9998L]\n",
      "               +- Aggregate [geometry#9624, COMM#10018, POP_2010#10027L], [geometry#9624, COMM#10018, POP_2010#10027L, count(1) AS NumberOfCrimesPerBlock#9986L]\n",
      "                  +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                     :- Project [point#9939]\n",
      "                     :  +- Project [DR_NO#9882, Date Rptd#9883, DATE OCC#9884, TIME OCC#9885, AREA #9886, AREA NAME#9887, Rpt Dist No#9888, Part 1-2#9889, Crm Cd#9890, Crm Cd Desc#9891, Mocodes#9892, Vict Age#9893, Vict Sex#9894, Vict Descent#9895, Premis Cd#9896, Premis Desc#9897, Weapon Used Cd#9898, Weapon Desc#9899, Status#9900, Status Desc#9901, Crm Cd 1#9902, Crm Cd 2#9903, Crm Cd 3#9904, Crm Cd 4#9905, ... 5 more fields]\n",
      "                     :     +- Filter (NOT (cast(LON#9909 as int) = 0) OR NOT (cast(LAT#9908 as int) = 0))\n",
      "                     :        +- Relation [DR_NO#9882,Date Rptd#9883,DATE OCC#9884,TIME OCC#9885,AREA #9886,AREA NAME#9887,Rpt Dist No#9888,Part 1-2#9889,Crm Cd#9890,Crm Cd Desc#9891,Mocodes#9892,Vict Age#9893,Vict Sex#9894,Vict Descent#9895,Premis Cd#9896,Premis Desc#9897,Weapon Used Cd#9898,Weapon Desc#9899,Status#9900,Status Desc#9901,Crm Cd 1#9902,Crm Cd 2#9903,Crm Cd 3#9904,Crm Cd 4#9905,... 4 more fields] parquet\n",
      "                     +- Project [geometry#9624, COMM#10018, POP_2010#10027L]\n",
      "                        +- Filter atleastnnonnulls(2, COMM#10018, ZCTA10#10035)\n",
      "                           +- Project [properties#9625.BG10 AS BG10#10011, properties#9625.BG10FIP10 AS BG10FIP10#10012, properties#9625.BG12 AS BG12#10013, properties#9625.CB10 AS CB10#10014, properties#9625.CEN_FIP13 AS CEN_FIP13#10015, properties#9625.CITY AS CITY#10016, properties#9625.CITYCOM AS CITYCOM#10017, properties#9625.COMM AS COMM#10018, properties#9625.CT10 AS CT10#10019, properties#9625.CT12 AS CT12#10020, properties#9625.CTCB10 AS CTCB10#10021, properties#9625.HD_2012 AS HD_2012#10022L, properties#9625.HD_NAME AS HD_NAME#10023, properties#9625.HOUSING10 AS HOUSING10#10024L, properties#9625.LA_FIP10 AS LA_FIP10#10025, properties#9625.OBJECTID AS OBJECTID#10026L, properties#9625.POP_2010 AS POP_2010#10027L, properties#9625.PUMA10 AS PUMA10#10028, properties#9625.SPA_2012 AS SPA_2012#10029L, properties#9625.SPA_NAME AS SPA_NAME#10030, properties#9625.SUP_DIST AS SUP_DIST#10031, properties#9625.SUP_LABEL AS SUP_LABEL#10032, properties#9625.ShapeSTArea AS ShapeSTArea#10033, properties#9625.ShapeSTLength AS ShapeSTLength#10034, ... 2 more fields]\n",
      "                              +- Project [features#9621.geometry AS geometry#9624, features#9621.properties AS properties#9625, features#9621.type AS type#9626]\n",
      "                                 +- Project [features#9621]\n",
      "                                    +- Generate explode(features#10008), false, [features#9621]\n",
      "                                       +- Relation [crs#10007,features#10008,name#10009,type#10010] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Average Income#9877 DESC NULLS LAST], true\n",
      "+- Project [COMM#9637, Average Income#9877, Crimes per Person#10002]\n",
      "   +- Join Inner, (COMM#9637 = COMM#10018), rightHint=(strategy=shuffle_replicate_nl)\n",
      "      :- Aggregate [COMM#9637], [COMM#9637, (sum(total_income#9853) / cast(sum(TOTAL_POP_2010#9803L) as double)) AS Average Income#9877]\n",
      "      :  +- Project [COMM#9637, TOTAL_POP_2010#9803L, (Estimated Median Income#9834 * cast(TOTAL_HOUSING10#9805L as float)) AS total_income#9853]\n",
      "      :     +- Join Inner, (ZCTA10#9654 = Zip Code#9828)\n",
      "      :        :- Project [Zip Code#9828, cast(regexp_replace(Estimated Median Income#9830, [$,.], , 1) as float) AS Estimated Median Income#9834]\n",
      "      :        :  +- Filter isnotnull(Zip Code#9828)\n",
      "      :        :     +- Relation [Zip Code#9828,Community#9829,Estimated Median Income#9830] csv\n",
      "      :        +- Aggregate [COMM#9637, ZCTA10#9654], [COMM#9637, ZCTA10#9654, sum(POP_2010#9792L) AS TOTAL_POP_2010#9803L, sum(HOUSING10#9793L) AS TOTAL_HOUSING10#9805L]\n",
      "      :           +- Project [features#9621.properties.COMM AS COMM#9637, coalesce(features#9621.properties.POP_2010, 0) AS POP_2010#9792L, features#9621.properties.ZCTA10 AS ZCTA10#9654, coalesce(features#9621.properties.HOUSING10, 0) AS HOUSING10#9793L]\n",
      "      :              +- Filter (atleastnnonnulls(2, features#9621.properties.COMM, features#9621.properties.ZCTA10) AND (isnotnull(features#9621.properties.ZCTA10) AND isnotnull(features#9621.properties.COMM)))\n",
      "      :                 +- Generate explode(features#9613), [0], false, [features#9621]\n",
      "      :                    +- Project [features#9613]\n",
      "      :                       +- Filter ((size(features#9613, true) > 0) AND isnotnull(features#9613))\n",
      "      :                          +- Relation [crs#9612,features#9613,name#9614,type#9615] geojson\n",
      "      +- Aggregate [COMM#10018], [COMM#10018, (cast(sum(NumberOfCrimesPerBlock#9986L) as double) / cast(sum(POP_2010#10027L) as double)) AS Crimes per Person#10002]\n",
      "         +- Aggregate [geometry#9624, COMM#10018, POP_2010#10027L], [COMM#10018, POP_2010#10027L, count(1) AS NumberOfCrimesPerBlock#9986L]\n",
      "            +- Project [geometry#9624, COMM#10018, POP_2010#10027L]\n",
      "               +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "                  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#9939]\n",
      "                  :  +- Filter ((NOT (cast(LON#9909 as int) = 0) OR NOT (cast(LAT#9908 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                  :     +- Relation [DR_NO#9882,Date Rptd#9883,DATE OCC#9884,TIME OCC#9885,AREA #9886,AREA NAME#9887,Rpt Dist No#9888,Part 1-2#9889,Crm Cd#9890,Crm Cd Desc#9891,Mocodes#9892,Vict Age#9893,Vict Sex#9894,Vict Descent#9895,Premis Cd#9896,Premis Desc#9897,Weapon Used Cd#9898,Weapon Desc#9899,Status#9900,Status Desc#9901,Crm Cd 1#9902,Crm Cd 2#9903,Crm Cd 3#9904,Crm Cd 4#9905,... 4 more fields] parquet\n",
      "                  +- Project [features#9621.geometry AS geometry#9624, features#9621.properties.COMM AS COMM#10018, features#9621.properties.POP_2010 AS POP_2010#10027L]\n",
      "                     +- Filter (((atleastnnonnulls(2, features#9621.properties.COMM, features#9621.properties.ZCTA10) AND isnotnull(features#9621.geometry)) AND isnotnull(features#9621.properties.COMM)) AND bloomfilter#10140 of [COMM#9637] filtering [features#9621.properties.COMM])\n",
      "                        :  +- Aggregate [COMM#9637, ZCTA10#9654], [COMM#9637, ZCTA10#9654, sum(POP_2010#9792L) AS TOTAL_POP_2010#9803L, sum(HOUSING10#9793L) AS TOTAL_HOUSING10#9805L]\n",
      "                        :     +- Project [features#9621.properties.COMM AS COMM#9637, coalesce(features#9621.properties.POP_2010, 0) AS POP_2010#9792L, features#9621.properties.ZCTA10 AS ZCTA10#9654, coalesce(features#9621.properties.HOUSING10, 0) AS HOUSING10#9793L]\n",
      "                        :        +- Filter (atleastnnonnulls(2, features#9621.properties.COMM, features#9621.properties.ZCTA10) AND (isnotnull(features#9621.properties.ZCTA10) AND isnotnull(features#9621.properties.COMM)))\n",
      "                        :           +- Generate explode(features#9613), [0], false, [features#9621]\n",
      "                        :              +- Project [features#9613]\n",
      "                        :                 +- Filter ((size(features#9613, true) > 0) AND isnotnull(features#9613))\n",
      "                        :                    +- Relation [crs#9612,features#9613,name#9614,type#9615] geojson\n",
      "                        +- Generate explode(features#10008), [0], false, [features#9621]\n",
      "                           +- Project [features#10008]\n",
      "                              +- Filter ((size(features#10008, true) > 0) AND isnotnull(features#10008))\n",
      "                                 +- Relation [crs#10007,features#10008,name#10009,type#10010] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Average Income#9877 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Average Income#9877 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=19831]\n",
      "      +- Project [COMM#9637, Average Income#9877, Crimes per Person#10002]\n",
      "         +- CartesianProduct (COMM#9637 = COMM#10018)\n",
      "            :- HashAggregate(keys=[COMM#9637], functions=[sum(total_income#9853), sum(TOTAL_POP_2010#9803L)], output=[COMM#9637, Average Income#9877], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(COMM#9637, 1000), ENSURE_REQUIREMENTS, [plan_id=19715]\n",
      "            :     +- HashAggregate(keys=[COMM#9637], functions=[partial_sum(total_income#9853), partial_sum(TOTAL_POP_2010#9803L)], output=[COMM#9637, sum#10068, sum#10070L], schema specialized)\n",
      "            :        +- Project [COMM#9637, TOTAL_POP_2010#9803L, (Estimated Median Income#9834 * cast(TOTAL_HOUSING10#9805L as float)) AS total_income#9853]\n",
      "            :           +- BroadcastHashJoin [Zip Code#9828], [ZCTA10#9654], Inner, BuildLeft, false\n",
      "            :              :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=19710]\n",
      "            :              :  +- Project [Zip Code#9828, cast(regexp_replace(Estimated Median Income#9830, [$,.], , 1) as float) AS Estimated Median Income#9834]\n",
      "            :              :     +- Filter isnotnull(Zip Code#9828)\n",
      "            :              :        +- FileScan csv [Zip Code#9828,Estimated Median Income#9830] Batched: false, DataFilters: [isnotnull(Zip Code#9828)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "            :              +- HashAggregate(keys=[COMM#9637, ZCTA10#9654], functions=[sum(POP_2010#9792L), sum(HOUSING10#9793L)], output=[COMM#9637, ZCTA10#9654, TOTAL_POP_2010#9803L, TOTAL_HOUSING10#9805L], schema specialized)\n",
      "            :                 +- Exchange hashpartitioning(COMM#9637, ZCTA10#9654, 1000), ENSURE_REQUIREMENTS, [plan_id=19707]\n",
      "            :                    +- HashAggregate(keys=[COMM#9637, ZCTA10#9654], functions=[partial_sum(POP_2010#9792L), partial_sum(HOUSING10#9793L)], output=[COMM#9637, ZCTA10#9654, sum#10072L, sum#10074L], schema specialized)\n",
      "            :                       +- Project [features#9621.properties.COMM AS COMM#9637, coalesce(features#9621.properties.POP_2010, 0) AS POP_2010#9792L, features#9621.properties.ZCTA10 AS ZCTA10#9654, coalesce(features#9621.properties.HOUSING10, 0) AS HOUSING10#9793L]\n",
      "            :                          +- Filter (atleastnnonnulls(2, features#9621.properties.COMM, features#9621.properties.ZCTA10) AND (isnotnull(features#9621.properties.ZCTA10) AND isnotnull(features#9621.properties.COMM)))\n",
      "            :                             +- Generate explode(features#9613), false, [features#9621]\n",
      "            :                                +- Filter ((size(features#9613, true) > 0) AND isnotnull(features#9613))\n",
      "            :                                   +- FileScan geojson [features#9613] Batched: false, DataFilters: [(size(features#9613, true) > 0), isnotnull(features#9613)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "            +- HashAggregate(keys=[COMM#10018], functions=[sum(NumberOfCrimesPerBlock#9986L), sum(POP_2010#10027L)], output=[COMM#10018, Crimes per Person#10002], schema specialized)\n",
      "               +- Exchange hashpartitioning(COMM#10018, 1000), ENSURE_REQUIREMENTS, [plan_id=19827]\n",
      "                  +- HashAggregate(keys=[COMM#10018], functions=[partial_sum(NumberOfCrimesPerBlock#9986L), partial_sum(POP_2010#10027L)], output=[COMM#10018, sum#10076L, sum#10078L], schema specialized)\n",
      "                     +- HashAggregate(keys=[geometry#9624, COMM#10018, POP_2010#10027L], functions=[count(1)], output=[COMM#10018, POP_2010#10027L, NumberOfCrimesPerBlock#9986L])\n",
      "                        +- Exchange hashpartitioning(geometry#9624, COMM#10018, POP_2010#10027L, 1000), ENSURE_REQUIREMENTS, [plan_id=19824]\n",
      "                           +- HashAggregate(keys=[geometry#9624, COMM#10018, POP_2010#10027L], functions=[partial_count(1)], output=[geometry#9624, COMM#10018, POP_2010#10027L, count#10080L])\n",
      "                              +- Project [geometry#9624, COMM#10018, POP_2010#10027L]\n",
      "                                 +- RangeJoin point#9939: geometry, geometry#9624: geometry, WITHIN\n",
      "                                    :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#9939]\n",
      "                                    :  +- Filter ((NOT (cast(LON#9909 as int) = 0) OR NOT (cast(LAT#9908 as int) = 0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                    :     +- FileScan parquet [LAT#9908,LON#9909] Batched: true, DataFilters: [(NOT (cast(LON#9909 as int) = 0) OR NOT (cast(LAT#9908 as int) = 0)), isnotnull( **org.apache.sp..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                                    +- Project [features#9621.geometry AS geometry#9624, features#9621.properties.COMM AS COMM#10018, features#9621.properties.POP_2010 AS POP_2010#10027L]\n",
      "                                       +- Filter (((atleastnnonnulls(2, features#9621.properties.COMM, features#9621.properties.ZCTA10) AND isnotnull(features#9621.geometry)) AND isnotnull(features#9621.properties.COMM)) AND bloomfilter#10140 of [bf10140 COMM#9637 estimatedNumRows=294857] filtering [features#9621.properties.COMM])\n",
      "                                          :  +- GenerateBloomFilter bf10140, 294857, 0, false, false, false, true, [id=#19818]\n",
      "                                          :     +- OutputAdapter [COMM#9637, ZCTA10#9654, sum#10072L, sum#10074L]\n",
      "                                          :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                                          :           +- Exchange hashpartitioning(COMM#9637, ZCTA10#9654, 1000), ENSURE_REQUIREMENTS, [plan_id=19815]\n",
      "                                          :              +- HashAggregate(keys=[COMM#9637, ZCTA10#9654], functions=[partial_sum(POP_2010#9792L), partial_sum(HOUSING10#9793L)], output=[COMM#9637, ZCTA10#9654, sum#10072L, sum#10074L], schema specialized)\n",
      "                                          :                 +- Project [features#9621.properties.COMM AS COMM#9637, coalesce(features#9621.properties.POP_2010, 0) AS POP_2010#9792L, features#9621.properties.ZCTA10 AS ZCTA10#9654, coalesce(features#9621.properties.HOUSING10, 0) AS HOUSING10#9793L]\n",
      "                                          :                    +- Filter (atleastnnonnulls(2, features#9621.properties.COMM, features#9621.properties.ZCTA10) AND (isnotnull(features#9621.properties.ZCTA10) AND isnotnull(features#9621.properties.COMM)))\n",
      "                                          :                       +- Generate explode(features#9613), false, [features#9621]\n",
      "                                          :                          +- Filter ((size(features#9613, true) > 0) AND isnotnull(features#9613))\n",
      "                                          :                             +- FileScan geojson [features#9613] Batched: false, DataFilters: [(size(features#9613, true) > 0), isnotnull(features#9613)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                                          +- Generate explode(features#10008), false, [features#9621]\n",
      "                                             +- Filter ((size(features#10008, true) > 0) AND isnotnull(features#10008))\n",
      "                                                +- FileScan geojson [features#10008] Batched: false, DataFilters: [(size(features#10008, true) > 0), isnotnull(features#10008)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "res.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499cf80-3189-4d86-94bb-7f417d0d50d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c22e0-f8a3-408a-b03e-ed448bb4598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8899b63d-5913-44ab-bcb2-c5debeecf66b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|544|\n",
      "|               Other| 73|\n",
      "|Hispanic/Latin/Me...| 60|\n",
      "|             Unknown| 41|\n",
      "|               Black| 37|\n",
      "|                NULL| 25|\n",
      "|         Other Asian| 15|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 15.70 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 1 max income\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col,year\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Marina del Rey\",\"Pacific Palisades\",\"Malibu\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter(col(\"DATE OCC\").substr(7,4) == \"2015\") \\\n",
    ".select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f54d4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|544|\n",
      "|               Other| 73|\n",
      "|Hispanic/Latin/Me...| 60|\n",
      "|             Unknown| 41|\n",
      "|               Black| 37|\n",
      "|                NULL| 25|\n",
      "|         Other Asian| 15|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 15.72 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 2 max income\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col,year\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Marina del Rey\",\"Pacific Palisades\",\"Malibu\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter(col(\"DATE OCC\").substr(7,4) == \"2015\") \\\n",
    ".select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3c56370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|544|\n",
      "|               Other| 73|\n",
      "|Hispanic/Latin/Me...| 60|\n",
      "|             Unknown| 41|\n",
      "|               Black| 37|\n",
      "|                NULL| 25|\n",
      "|         Other Asian| 15|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 15.38 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 3 max income\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col,year\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"8\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Marina del Rey\",\"Pacific Palisades\",\"Malibu\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter(col(\"DATE OCC\").substr(7,4) == \"2015\") \\\n",
    ".select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c96baeb-48f6-4f4f-9aa1-8564f23624d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.09 seconds\n",
      "+------------------+------------------+--------------------+\n",
      "|              COMM|    Average Income|   Crimes per Person|\n",
      "+------------------+------------------+--------------------+\n",
      "|            Vernon| 4406.446428571428|                NULL|\n",
      "|    Vernon Central| 6624.199339771586|  0.7617435282016115|\n",
      "|   University Park| 6863.013283954593|  1.0671493587091436|\n",
      "|        South Park| 6943.255677324596|  0.9406721227928171|\n",
      "|           Central| 6972.518209022642|  0.7312082894274675|\n",
      "|Wholesale District|7728.6860264829365|   2.229077179830921|\n",
      "|             Watts|  7754.24222709736|  1.0362929698784107|\n",
      "|Florence-Firestone| 7859.978450089285|  0.9326086563330861|\n",
      "|     Green Meadows| 8027.096463346955|  1.4156235238545112|\n",
      "|    Athens Village| 8294.286744092911| 0.02976190476190476|\n",
      "|    Vermont Square| 8329.565933286018|  0.8801749726605218|\n",
      "|     Vermont Vista|  8361.68308921438|  1.4987153441682601|\n",
      "|       Walnut Park| 8422.475760992109|0.004938271604938...|\n",
      "|     Boyle Heights|  8434.46544538141|  0.7476719848600685|\n",
      "|Century Palms/Cove| 8552.219764107911|  1.4366962305986697|\n",
      "|       West Vernon| 8722.734433399602|   1.140709776848861|\n",
      "|           Maywood| 8775.741595181602|0.005208333333333333|\n",
      "|            Lennox| 8780.936140289192|                64.0|\n",
      "|   Adams-Normandie| 8791.458301453711|  0.8118853610564766|\n",
      "|      Bell Gardens| 8848.470907016543|0.004986149584487534|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "####query 3 calculate lowest income communities!!!!!\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .dropna(subset=[\"COMM\",\"ZCTA10\"])  \\\n",
    "\n",
    "\n",
    "census = blocks_census \\\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"HOUSING10\") \\\n",
    "    .na.fill({\"POP_2010\": 0, \"HOUSING10\": 0}) \\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"TOTAL_POP_2010\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"TOTAL_HOUSING10\")\n",
    "    )\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,.]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"TOTAL_HOUSING10\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"TOTAL_POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\")) \\\n",
    ".select(\"point\")\n",
    "\n",
    "\n",
    "# res2 = crime_dataset \\\n",
    "#     .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry),)\\\n",
    "# .select(\"COMM\",\"POP_2010\")\\\n",
    "# .groupBy(\"COMM\")\\\n",
    "# .agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    "# .withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census.select(\"geometry\",\"COMM\",\"POP_2010\"), ST_Within(crime_dataset.point, blocks_census.geometry)) \\\n",
    ".groupBy(\"geometry\",\"COMM\",\"POP_2010\") \\\n",
    ".agg(F.count(\"*\").alias(\"NumberOfCrimesPerBlock\"))\\\n",
    ".groupby(\"COMM\") \\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.sum(\"NumberOfCrimesPerBlock\").alias(\"NumberOfCrimes\") ) \\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2,res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\").orderBy(col(\"Average Income\").asc())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1844aafe-6d01-43e0-846f-0d31217a9a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|2113|\n",
      "|               Black| 651|\n",
      "|                NULL| 414|\n",
      "|               White| 389|\n",
      "|               Other| 192|\n",
      "|         Other Asian| 137|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  23|\n",
      "|              Korean|   4|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken: 15.24 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 1 min income\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col,year\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Vernon\",\"Vernon Central\",\"University Park\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter(col(\"DATE OCC\").substr(7,4) == \"2015\") \\\n",
    ".select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a58dedef-92a0-409b-8e68-eab8b00b2159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|2113|\n",
      "|               Black| 651|\n",
      "|                NULL| 414|\n",
      "|               White| 389|\n",
      "|               Other| 192|\n",
      "|         Other Asian| 137|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  23|\n",
      "|              Korean|   4|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken: 15.88 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 2 min income\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col,year\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Vernon\",\"Vernon Central\",\"University Park\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter(col(\"DATE OCC\").substr(7,4) == \"2015\") \\\n",
    ".select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20dee69a-3f80-4196-8c34-c47d33e5e1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|      Victim Descent|   #|\n",
      "+--------------------+----+\n",
      "|Hispanic/Latin/Me...|2113|\n",
      "|               Black| 651|\n",
      "|                NULL| 414|\n",
      "|               White| 389|\n",
      "|               Other| 192|\n",
      "|         Other Asian| 137|\n",
      "|             Unknown|  26|\n",
      "|American Indian/A...|  23|\n",
      "|              Korean|   4|\n",
      "|             Chinese|   3|\n",
      "|            Filipino|   2|\n",
      "|         AsianIndian|   1|\n",
      "+--------------------+----+\n",
      "\n",
      "Time taken: 15.64 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 3 min income\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col,year\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Vernon\",\"Vernon Central\",\"University Park\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter(col(\"DATE OCC\").substr(7,4) == \"2015\") \\\n",
    ".select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407fbce-18bc-4596-8da3-3296fc10e173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201e8f2-cdf1-4ead-b3bf-02f0087244f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecbc3a-5fef-4147-a3f6-70242faf693f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90e7c4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----+\n",
      "|        division|      avg_distance|   #|\n",
      "+----------------+------------------+----+\n",
      "|         PACIFIC|3617.7734337150914|7654|\n",
      "|        FOOTHILL| 4811.394973345948|7318|\n",
      "|     WEST VALLEY|3363.5469483212823|6469|\n",
      "|        VAN NUYS| 3430.765144800873|6453|\n",
      "|WEST LOS ANGELES| 3950.890550890165|6193|\n",
      "|         TOPANGA| 3818.025338208304|5971|\n",
      "| NORTH HOLLYWOOD|3194.6900764893703|5748|\n",
      "|        WILSHIRE| 2556.598961706193|5731|\n",
      "|       NORTHEAST|3695.2562270997964|5726|\n",
      "|          HARBOR|4047.4501072731864|5554|\n",
      "|         MISSION| 3553.593615521547|5415|\n",
      "|       HOLLYWOOD| 2840.164535299212|4933|\n",
      "|      HOLLENBECK|3365.6869973683088|4618|\n",
      "|       SOUTHWEST|2295.1702816435145|4482|\n",
      "|      DEVONSHIRE|3083.4413145949866|4437|\n",
      "|       SOUTHEAST| 3321.595088585187|4347|\n",
      "|     77TH STREET|1931.1552614873922|3370|\n",
      "|          NEWTON|1638.2180223355483|3009|\n",
      "|         RAMPART|1926.0054134765985|2817|\n",
      "|         OLYMPIC| 1726.857324744103|2760|\n",
      "+----------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 181.06 seconds"
     ]
    }
   ],
   "source": [
    "# query 5 conf 1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .appName(\"Query 5\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#Precint data handling\n",
    "precincts_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True)\\\n",
    "    .withColumnRenamed(\"DIVISION\",\"division\")\\\n",
    "    .withColumn(\"geom\", ST_Point(F.col(\"X\"),F.col(\"Y\")))\\\n",
    "    .select(\"division\",\"geom\")\n",
    "\n",
    "\n",
    "#Crime data handling\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(F.col(\"LON\"),F.col(\"LAT\")))\\\n",
    "    .select(\"crime_geom\")\n",
    "\n",
    "#Calculating the distance of each case with each precinct\n",
    "joined_df = precincts_df.crossJoin(crime_df) \\\n",
    "    .withColumn(\"distance\", ST_DistanceSphere(col(\"geom\"), F.col(\"crime_geom\")))\n",
    "\n",
    "#Finding the closest precinct to each crime\n",
    "window_spec = Window.partitionBy(\"crime_geom\").orderBy(F.col(\"distance\"))\n",
    "\n",
    "closest_division_df = joined_df.withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"division\", \"crime_geom\", \"distance\") \n",
    "    \n",
    "result_df = closest_division_df.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        F.count(\"crime_geom\").alias(\"#\"),\n",
    "    )\\\n",
    "    .orderBy(\"#\",ascending=False)    \n",
    " \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b05781cc-5230-44b0-b848-9cf78b72f0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----+\n",
      "|        division|      avg_distance|   #|\n",
      "+----------------+------------------+----+\n",
      "|         PACIFIC|3617.7734337150914|7654|\n",
      "|        FOOTHILL| 4811.394973345948|7318|\n",
      "|     WEST VALLEY|3363.5469483212823|6469|\n",
      "|        VAN NUYS| 3430.765144800873|6453|\n",
      "|WEST LOS ANGELES| 3950.890550890165|6193|\n",
      "|         TOPANGA| 3818.025338208304|5971|\n",
      "| NORTH HOLLYWOOD|3194.6900764893703|5748|\n",
      "|        WILSHIRE| 2556.598961706193|5731|\n",
      "|       NORTHEAST|3695.2562270997964|5726|\n",
      "|          HARBOR|4047.4501072731864|5554|\n",
      "|         MISSION| 3553.593615521547|5415|\n",
      "|       HOLLYWOOD| 2840.164535299212|4933|\n",
      "|      HOLLENBECK|3365.6869973683088|4618|\n",
      "|       SOUTHWEST|2295.1702816435145|4482|\n",
      "|      DEVONSHIRE|3083.4413145949866|4437|\n",
      "|       SOUTHEAST| 3321.595088585187|4347|\n",
      "|     77TH STREET|1931.1552614873922|3370|\n",
      "|          NEWTON|1638.2180223355483|3009|\n",
      "|         RAMPART|1926.0054134765985|2817|\n",
      "|         OLYMPIC| 1726.857324744103|2760|\n",
      "+----------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 160.16 seconds"
     ]
    }
   ],
   "source": [
    "# query 5 conf 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"4\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .appName(\"Query 5\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#Precint data handling\n",
    "precincts_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True)\\\n",
    "    .withColumnRenamed(\"DIVISION\",\"division\")\\\n",
    "    .withColumn(\"geom\", ST_Point(F.col(\"X\"),F.col(\"Y\")))\\\n",
    "    .select(\"division\",\"geom\")\n",
    "\n",
    "\n",
    "#Crime data handling\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(F.col(\"LON\"),F.col(\"LAT\")))\\\n",
    "    .select(\"crime_geom\")\n",
    "\n",
    "#Calculating the distance of each case with each precinct\n",
    "joined_df = precincts_df.crossJoin(crime_df) \\\n",
    "    .withColumn(\"distance\", ST_DistanceSphere(col(\"geom\"), F.col(\"crime_geom\")))\n",
    "\n",
    "#Finding the closest precinct to each crime\n",
    "window_spec = Window.partitionBy(\"crime_geom\").orderBy(F.col(\"distance\"))\n",
    "\n",
    "closest_division_df = joined_df.withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"division\", \"crime_geom\", \"distance\") \n",
    "    \n",
    "result_df = closest_division_df.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        F.count(\"crime_geom\").alias(\"#\"),\n",
    "    )\\\n",
    "    .orderBy(\"#\",ascending=False)    \n",
    " \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37acac94-cf22-4fd5-9f54-afd3d048adde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----+\n",
      "|        division|      avg_distance|   #|\n",
      "+----------------+------------------+----+\n",
      "|         PACIFIC|3617.7734337150914|7654|\n",
      "|        FOOTHILL| 4811.394973345948|7318|\n",
      "|     WEST VALLEY|3363.5469483212823|6469|\n",
      "|        VAN NUYS| 3430.765144800873|6453|\n",
      "|WEST LOS ANGELES| 3950.890550890165|6193|\n",
      "|         TOPANGA| 3818.025338208304|5971|\n",
      "| NORTH HOLLYWOOD|3194.6900764893703|5748|\n",
      "|        WILSHIRE| 2556.598961706193|5731|\n",
      "|       NORTHEAST|3695.2562270997964|5726|\n",
      "|          HARBOR|4047.4501072731864|5554|\n",
      "|         MISSION| 3553.593615521547|5415|\n",
      "|       HOLLYWOOD| 2840.164535299212|4933|\n",
      "|      HOLLENBECK|3365.6869973683088|4618|\n",
      "|       SOUTHWEST|2295.1702816435145|4482|\n",
      "|      DEVONSHIRE|3083.4413145949866|4437|\n",
      "|       SOUTHEAST| 3321.595088585187|4347|\n",
      "|     77TH STREET|1931.1552614873922|3370|\n",
      "|          NEWTON|1638.2180223355483|3009|\n",
      "|         RAMPART|1926.0054134765985|2817|\n",
      "|         OLYMPIC| 1726.857324744103|2760|\n",
      "+----------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 159.71 seconds"
     ]
    }
   ],
   "source": [
    "# query 5 conf 3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"8\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .appName(\"Query 5\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#Precint data handling\n",
    "precincts_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True)\\\n",
    "    .withColumnRenamed(\"DIVISION\",\"division\")\\\n",
    "    .withColumn(\"geom\", ST_Point(F.col(\"X\"),F.col(\"Y\")))\\\n",
    "    .select(\"division\",\"geom\")\n",
    "\n",
    "\n",
    "#Crime data handling\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter((col(\"LON\") != 0) | (col(\"LAT\") != 0))  \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(F.col(\"LON\"),F.col(\"LAT\")))\\\n",
    "    .select(\"crime_geom\")\n",
    "\n",
    "#Calculating the distance of each case with each precinct\n",
    "joined_df = precincts_df.crossJoin(crime_df) \\\n",
    "    .withColumn(\"distance\", ST_DistanceSphere(col(\"geom\"), F.col(\"crime_geom\")))\n",
    "\n",
    "#Finding the closest precinct to each crime\n",
    "window_spec = Window.partitionBy(\"crime_geom\").orderBy(F.col(\"distance\"))\n",
    "\n",
    "closest_division_df = joined_df.withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"division\", \"crime_geom\", \"distance\") \n",
    "    \n",
    "result_df = closest_division_df.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        F.count(\"crime_geom\").alias(\"#\"),\n",
    "    )\\\n",
    "    .orderBy(\"#\",ascending=False)    \n",
    " \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0d8bc4a-751c-401b-8bfb-d7b798d718b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----+\n",
      "|        division|      avg_distance|   #|\n",
      "+----------------+------------------+----+\n",
      "|          HARBOR|4047.4501072731864|5554|\n",
      "|        FOOTHILL|4811.3949733459485|7318|\n",
      "|     77TH STREET|1931.1552614873917|3370|\n",
      "|      HOLLENBECK| 3365.686997368309|4618|\n",
      "|         MISSION|  3553.59361552155|5415|\n",
      "|         OLYMPIC| 1726.857324744103|2760|\n",
      "|         RAMPART|1926.0054134765985|2817|\n",
      "|       SOUTHWEST| 2295.170281643515|4482|\n",
      "|WEST LOS ANGELES|3950.8905508901653|6193|\n",
      "|         CENTRAL|1438.9179813974613|2055|\n",
      "|         TOPANGA|3818.0253382083033|5971|\n",
      "|         PACIFIC|3617.7734337150905|7654|\n",
      "|       NORTHEAST|3695.2562270997955|5726|\n",
      "|          NEWTON|1638.2180223355472|3009|\n",
      "|       HOLLYWOOD| 2840.164535299211|4933|\n",
      "| NORTH HOLLYWOOD|3194.6900764893708|5748|\n",
      "|        VAN NUYS| 3430.765144800873|6453|\n",
      "|        WILSHIRE|2556.5989617061946|5731|\n",
      "|     WEST VALLEY|3363.5469483212823|6469|\n",
      "|       SOUTHEAST|3321.5950885851876|4347|\n",
      "+----------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 66.34 seconds"
     ]
    }
   ],
   "source": [
    "##attempt to increase the performanse for query 5\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"8\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .appName(\"Query 5\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Precinct data handling\n",
    "precincts_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True) \\\n",
    "    .withColumnRenamed(\"DIVISION\", \"division\") \\\n",
    "    .withColumn(\"geom\", ST_Point(F.col(\"X\"), F.col(\"Y\"))) \\\n",
    "    .select(\"division\", \"geom\") \\\n",
    "    .repartition(16)  \n",
    "\n",
    "# Crime data handling\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .filter((col(\"LON\") != 0) | (col(\"LAT\") != 0)) \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(F.col(\"LON\"), F.col(\"LAT\"))) \\\n",
    "    .select(\"crime_geom\") \\\n",
    "    .repartition(16)  \n",
    "\n",
    "# Calculating the distance of each case with each precinct\n",
    "joined_df = precincts_df.crossJoin(crime_df) \\\n",
    "    .withColumn(\"distance\", ST_DistanceSphere(col(\"geom\"), F.col(\"crime_geom\"))) \\\n",
    "    .repartition(16)  \n",
    "\n",
    "# Finding the closest precinct to each crime\n",
    "window_spec = Window.partitionBy(\"crime_geom\").orderBy(F.col(\"distance\"))\n",
    "\n",
    "closest_division_df = joined_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"division\", \"crime_geom\", \"distance\") \\\n",
    "    .repartition(16)  \n",
    "\n",
    "result_df = closest_division_df.groupBy(\"division\") \\\n",
    "    .agg(\n",
    "        F.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        F.count(\"crime_geom\").alias(\"#\"),\n",
    "    ) \\\n",
    "    .orderBy(\"#\", ascending=False) \\\n",
    "    .repartition(16)  \n",
    "\n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd285f38-e5e8-4ced-8601-48d81b63deb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
