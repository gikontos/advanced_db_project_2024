{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c8305e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adult', 121093), ('young adult', 38703), ('child', 10830), ('old', 5985)]\n",
      "Time taken: 22.19 seconds"
     ]
    }
   ],
   "source": [
    "# Query 1 RDD\n",
    "\n",
    "import csv,time\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 RDD\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    f = StringIO(line)\n",
    "    reader = csv.reader(f)\n",
    "    return next(reader) \n",
    "\n",
    "def help1(data):\n",
    "    try:\n",
    "        age=int(data)\n",
    "        if age<18 and age>0:\n",
    "            return \"child\"\n",
    "        if age<25:\n",
    "            return \"young adult\"\n",
    "        if age<65 :\n",
    "            return \"adult\"\n",
    "        if age>64:\n",
    "            return \"old\"\n",
    "        else:\n",
    "            return \"no individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "rdd1  = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\\\n",
    ".map(parse_csv_line)\n",
    "\n",
    "rdd2= sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\\\n",
    ".map(parse_csv_line)\n",
    "\n",
    "header1 = rdd1.first()\n",
    "header2 = rdd2.first()\n",
    "\n",
    "# Filter out the header\n",
    "rdd1_data = rdd1.filter(lambda line: line != header1)\n",
    "rdd2_data = rdd2.filter(lambda line: line != header2)\n",
    "\n",
    "crime_data = rdd1_data.union(rdd2_data) \\\n",
    ".filter(lambda pair: pair[9].find(\"AGGRAVATED\") != -1 ) \\\n",
    ".map(lambda data: (help1(data[11]), 1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".sortBy( lambda pair : pair[1], ascending=False )\n",
    "\n",
    "print(crime_data.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59e0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|  age_group| count|\n",
      "+-----------+------+\n",
      "|      adult|121093|\n",
      "|young adult| 38703|\n",
      "|      child| 10830|\n",
      "|        old|  5985|\n",
      "+-----------+------+\n",
      "\n",
      "Time taken: 11.91 seconds"
     ]
    }
   ],
   "source": [
    "####query 1 dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1 Dataframe\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "\n",
    "def age_group(age_str):\n",
    "    try:\n",
    "        age=int(age_str)\n",
    "        if age<18 and age>0:\n",
    "            return \"child\"\n",
    "        if age<25:\n",
    "            return \"young adult\"\n",
    "        if age<65 :\n",
    "            return \"adult\"\n",
    "        if age>64:\n",
    "            return \"old\"\n",
    "        else:\n",
    "            return \"no individual victim\"\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "age_udf=udf(age_group,StringType())\n",
    "dataframe=dataframe1.union(dataframe2)\\\n",
    ".filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED\"))\\\n",
    ".withColumn(\"age_group\",age_udf(col(\"Vict Age\")))\\\n",
    ".groupBy(\"age_group\").agg(F.count(\"*\").alias(\"count\"))\\\n",
    ".orderBy(\"count\",ascending=False)\n",
    "\n",
    "\n",
    "dataframe.show()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45b2cff-3001-4b85-acc0-0b2308ff06e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 4.35 seconds"
     ]
    }
   ],
   "source": [
    "####query2a dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 Dataframe\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(F.desc(\"closed_case_rate\"))\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "\n",
    "\n",
    "dataframe=dataframe1.union(dataframe2)\\\n",
    ".withColumn(\"year\",col(\"Date Rptd\").substr(7,4))\\\n",
    ".select(\"year\",\"AREA NAME\",\"Status\")\\\n",
    ".groupBy(\"year\",\"AREA NAME\").agg((count(when(col(\"Status\") != \"IC\", 1)) / count(\"*\") ).alias(\"closed_case_rate\"))\\\n",
    ".withColumn(\"#\", F.row_number().over(window_spec) )\\\n",
    ".filter(col(\"#\") <= 3)\\\n",
    ".withColumnRenamed(\"AREA NAME\", \"precinct\")\n",
    "\n",
    "dataframe.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee557a1-263a-41f3-8d1b-6d755e3ee646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 3.71 seconds"
     ]
    }
   ],
   "source": [
    "###query2a spark sql api\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 SQL API\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "\n",
    "dataframe=dataframe1.union(dataframe2)\n",
    "\n",
    "dataframe.createOrReplaceTempView(\"Dataset\")\n",
    "query= \"\"\"\n",
    "    WITH extracted_data AS (\n",
    "        SELECT \n",
    "            substr(`Date Rptd`, 7, 4) AS year,\n",
    "            `AREA NAME` AS precinct,\n",
    "            Status\n",
    "        FROM Dataset\n",
    "    ),\n",
    "    aggregated_data AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            COUNT(CASE WHEN Status != 'IC' THEN 1 END)  / COUNT(*) AS closed_case_rate\n",
    "        FROM extracted_data\n",
    "        GROUP BY year, precinct\n",
    "    ),\n",
    "    ranked_data AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            precinct,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS `#`\n",
    "        FROM aggregated_data\n",
    "    )\n",
    "    SELECT \n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        `#`\n",
    "    FROM ranked_data\n",
    "    WHERE `#` <= 3\n",
    "\"\"\"\n",
    "res=spark.sql(query)\n",
    "res.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b1466-feb2-4e67-a61f-168b7db74b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####2b\n",
    "####make parquet dataset\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2b write parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dataframe1= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True)\n",
    "dataframe2= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",header=True)\n",
    "dataframe=dataframe1.union(dataframe2)\n",
    "\n",
    "dataframe.coalesce(1).write.mode(\"overwrite\").parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") ##coalesce gia 1 file\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d0ccd6e-4f07-411a-9b81-fb53e5bd7fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|0.32947355855318133|  1|\n",
      "|2010|    Olympic|0.31962706191728424|  2|\n",
      "|2010|     Harbor| 0.2963203463203463|  3|\n",
      "|2011|    Olympic|0.35212167689161555|  1|\n",
      "|2011|    Rampart|0.32511779630300836|  2|\n",
      "|2011|     Harbor| 0.2865220520201501|  3|\n",
      "|2012|    Olympic| 0.3441481831052383|  1|\n",
      "|2012|    Rampart|  0.329464181029429|  2|\n",
      "|2012|     Harbor| 0.2981513327601032|  3|\n",
      "|2013|    Olympic| 0.3352812271731191|  1|\n",
      "|2013|    Rampart| 0.3208287360549221|  2|\n",
      "|2013|     Harbor| 0.2916422459266206|  3|\n",
      "|2014|   Van Nuys| 0.3180567315834039|  1|\n",
      "|2014|West Valley| 0.3131198995605775|  2|\n",
      "|2014|    Mission| 0.3116279069767442|  3|\n",
      "|2015|   Van Nuys| 0.3264134698172773|  1|\n",
      "|2015|West Valley| 0.3027597402597403|  2|\n",
      "|2015|    Mission|0.30179460678380154|  3|\n",
      "|2016|   Van Nuys|0.31880755720117726|  1|\n",
      "|2016|West Valley| 0.3154798761609907|  2|\n",
      "|2016|   Foothill| 0.2987029184335246|  3|\n",
      "|2017|   Van Nuys|0.32020342117429496|  1|\n",
      "|2017|    Mission| 0.3103892518634398|  2|\n",
      "|2017|   Foothill|0.30469226081657524|  3|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 24 rows\n",
      "\n",
      "Time taken: 1.55 seconds"
     ]
    }
   ],
   "source": [
    "####2b test parquet file on 2a query dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import udf, col,count,when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2b test parquet for 2b Dataframe\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\n",
    "\n",
    "dataframe=dataframe.withColumn(\"year\",col(\"Date Rptd\").substr(7,4))\\\n",
    ".select(\"year\",\"AREA NAME\",\"Status\")\\\n",
    ".groupBy(\"year\",\"AREA NAME\").agg((count(when(col(\"Status\") != \"IC\", 1)) / count(\"*\") ).alias(\"closed_case_rate\"))\\\n",
    ".withColumn(\"#\", F.row_number().over(window_spec) )\\\n",
    ".filter(col(\"#\") <= 3)\\\n",
    ".withColumnRenamed(\"AREA NAME\", \"precinct\")\n",
    "\n",
    "dataframe.show(24)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f419353-a190-4c8f-9714-ac14210c9889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.05 seconds\n",
      "+--------------------+------------------+--------------------+\n",
      "|                COMM|    Average Income|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|      Marina del Rey| 76428.84908639747|0.030683159228987282|\n",
      "|   Pacific Palisades| 70656.11180545464|0.004497808363497533|\n",
      "|              Malibu|  67135.0118623962|0.003460207612456...|\n",
      "| Palisades Highlands| 66867.44038612054|0.003936594845458679|\n",
      "|    Marina Peninsula|65235.692875259396| 0.01562518384299514|\n",
      "|             Bel Air| 63041.33942621959|0.001519214928910...|\n",
      "|Palos Verdes Estates| 61905.61214466438|0.008547008547008548|\n",
      "|     Manhattan Beach|60985.189241497086|0.030103480714957668|\n",
      "|       Beverly Crest| 60947.48978754819|0.002445942879372386|\n",
      "|           Brentwood| 60840.62462032012|0.003654077071904...|\n",
      "|       Hermosa Beach| 57924.85594176151|0.010483401281304601|\n",
      "|   Mandeville Canyon| 55572.11011444479|0.004121446588871122|\n",
      "|La Cañada Flintridge| 54900.65682110046| 0.01282051282051282|\n",
      "|       Beverly Hills| 51303.11237503298|0.015908663343053336|\n",
      "|         Playa Vista| 50264.47143177235|0.004342404013547871|\n",
      "|             Carthay|49841.163007975694| 0.00871403852694195|\n",
      "|      West Hollywood| 49414.42132038722|0.005652750584222826|\n",
      "|              Venice|47614.884045977014|  0.0109285970918646|\n",
      "|       Redondo Beach| 47568.80637622101|0.050505050505050504|\n",
      "| Rancho Palos Verdes| 46151.89491631246| 0.02393418100224383|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "####query 3\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "start_time=time.time()\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "blocks_census = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .dropna(subset=[\"COMM\",\"ZCTA10\"])  \\\n",
    "\n",
    "\n",
    "census = blocks_census \\\n",
    "    .select(\"COMM\", \"POP_2010\", \"ZCTA10\", \"HOUSING10\") \\\n",
    "    .na.fill({\"POP_2010\": 0, \"HOUSING10\": 0}) \\\n",
    "    .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "    .agg(\n",
    "        F.sum(\"POP_2010\").alias(\"TOTAL_POP_2010\"),\n",
    "        F.sum(\"HOUSING10\").alias(\"TOTAL_HOUSING10\")\n",
    "    )\n",
    "\n",
    "income= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True)\n",
    "\n",
    "res1= income.withColumn( \"Estimated Median Income\", F.regexp_replace(col(\"Estimated Median Income\"), \"[$,.]\", \"\").cast(\"float\"))\\\n",
    ".join(census,census[\"ZCTA10\"]==income[\"Zip Code\"])\\\n",
    ".withColumn(\"total_income\",col(\"Estimated Median Income\")*col(\"TOTAL_HOUSING10\") )\\\n",
    ".groupBy(\"COMM\").agg(\n",
    "                    F.sum(\"TOTAL_POP_2010\").alias(\"total_population\"),\n",
    "                    F.sum(\"total_income\").alias(\"comm_total_income\"))\\\n",
    ".withColumn(\"Average Income\", col(\"comm_total_income\")/col(\"total_population\"))\n",
    "\n",
    "\n",
    "crime_dataset = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\")\\\n",
    ".withColumn(\"point\",ST_Point(\"LON\", \"LAT\"))\n",
    "\n",
    "res2 = crime_dataset \\\n",
    "    .join(blocks_census, ST_Within(crime_dataset.point, blocks_census.geometry))\\\n",
    ".select(\"COMM\",\"POP_2010\")\\\n",
    ".groupBy(\"COMM\")\\\n",
    ".agg(F.sum(\"POP_2010\").alias(\"TotalPopulation\"),F.count(\"*\").alias(\"NumberOfCrimes\"))\\\n",
    ".withColumn(\"Crimes per Person\",col(\"NumberOfCrimes\")/col(\"TotalPopulation\"))\n",
    "\n",
    "res=res1.join(res2,res1[\"COMM\"]==res2[\"COMM\"]).select(res1[\"COMM\"].alias(\"COMM\"),\"Average Income\",\"Crimes per Person\").orderBy(col(\"Average Income\").desc())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70a920b7-311d-4bba-bb79-ebe5b5ad8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [COMM#24143 AS COMM#24609, 'Average Income, 'Crimes per Person]\n",
      "+- Join Inner, (COMM#24143 = COMM#24574)\n",
      "   :- Project [COMM#24143, total_population#24339L, comm_total_income#24341, (comm_total_income#24341 / cast(total_population#24339L as double)) AS Average Income#24345]\n",
      "   :  +- Aggregate [COMM#24143], [COMM#24143, sum(POP_2010#24278L) AS total_population#24339L, sum(total_income#24323) AS comm_total_income#24341]\n",
      "   :     +- Project [Zip Code#24300, Community#24301, Estimated Median Income#24306, COMM#24143, ZCTA10#24160, POP_2010#24278L, (Estimated Median Income#24306 * cast(POP_2010#24278L as float)) AS total_income#24323]\n",
      "   :        +- Join Inner, (ZCTA10#24160 = Zip Code#24300)\n",
      "   :           :- Project [Zip Code#24300, Community#24301, cast(regexp_replace(Estimated Median Income#24302, [$,], , 1) as float) AS Estimated Median Income#24306]\n",
      "   :           :  +- Relation [Zip Code#24300,Community#24301,Estimated Median Income#24302] csv\n",
      "   :           +- Aggregate [COMM#24143, ZCTA10#24160], [COMM#24143, ZCTA10#24160, sum(POP_2010#24270L) AS POP_2010#24278L]\n",
      "   :              +- Project [COMM#24143, coalesce(POP_2010#24152L, cast(0 as bigint)) AS POP_2010#24270L, ZCTA10#24160]\n",
      "   :                 +- Project [COMM#24143, POP_2010#24152L, ZCTA10#24160]\n",
      "   :                    +- Project [properties#24131.BG10 AS BG10#24136, properties#24131.BG10FIP10 AS BG10FIP10#24137, properties#24131.BG12 AS BG12#24138, properties#24131.CB10 AS CB10#24139, properties#24131.CEN_FIP13 AS CEN_FIP13#24140, properties#24131.CITY AS CITY#24141, properties#24131.CITYCOM AS CITYCOM#24142, properties#24131.COMM AS COMM#24143, properties#24131.CT10 AS CT10#24144, properties#24131.CT12 AS CT12#24145, properties#24131.CTCB10 AS CTCB10#24146, properties#24131.HD_2012 AS HD_2012#24147L, properties#24131.HD_NAME AS HD_NAME#24148, properties#24131.HOUSING10 AS HOUSING10#24149L, properties#24131.LA_FIP10 AS LA_FIP10#24150, properties#24131.OBJECTID AS OBJECTID#24151L, properties#24131.POP_2010 AS POP_2010#24152L, properties#24131.PUMA10 AS PUMA10#24153, properties#24131.SPA_2012 AS SPA_2012#24154L, properties#24131.SPA_NAME AS SPA_NAME#24155, properties#24131.SUP_DIST AS SUP_DIST#24156, properties#24131.SUP_LABEL AS SUP_LABEL#24157, properties#24131.ShapeSTArea AS ShapeSTArea#24158, properties#24131.ShapeSTLength AS ShapeSTLength#24159, ... 2 more fields]\n",
      "   :                       +- Project [features#24127.geometry AS geometry#24130, features#24127.properties AS properties#24131, features#24127.type AS type#24132]\n",
      "   :                          +- Project [features#24127]\n",
      "   :                             +- Generate explode(features#24119), false, [features#24127]\n",
      "   :                                +- Relation [crs#24118,features#24119,name#24120,type#24121] geojson\n",
      "   +- Project [COMM#24574, TotalPopulation#24552L, NumberOfCrimes#24554L, (cast(NumberOfCrimes#24554L as double) / cast(TotalPopulation#24552L as double)) AS Crimes per Person#24558]\n",
      "      +- Aggregate [COMM#24574], [COMM#24574, sum(POP_2010#24583L) AS TotalPopulation#24552L, count(1) AS NumberOfCrimes#24554L]\n",
      "         +- Project [COMM#24574, POP_2010#24583L]\n",
      "            +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "               :- Project [DR_NO#24350, Date Rptd#24351, DATE OCC#24352, TIME OCC#24353, AREA #24354, AREA NAME#24355, Rpt Dist No#24356, Part 1-2#24357, Crm Cd#24358, Crm Cd Desc#24359, Mocodes#24360, Vict Age#24361, Vict Sex#24362, Vict Descent#24363, Premis Cd#24364, Premis Desc#24365, Weapon Used Cd#24366, Weapon Desc#24367, Status#24368, Status Desc#24369, Crm Cd 1#24370, Crm Cd 2#24371, Crm Cd 3#24372, Crm Cd 4#24373, ... 5 more fields]\n",
      "               :  +- Relation [DR_NO#24350,Date Rptd#24351,DATE OCC#24352,TIME OCC#24353,AREA #24354,AREA NAME#24355,Rpt Dist No#24356,Part 1-2#24357,Crm Cd#24358,Crm Cd Desc#24359,Mocodes#24360,Vict Age#24361,Vict Sex#24362,Vict Descent#24363,Premis Cd#24364,Premis Desc#24365,Weapon Used Cd#24366,Weapon Desc#24367,Status#24368,Status Desc#24369,Crm Cd 1#24370,Crm Cd 2#24371,Crm Cd 3#24372,Crm Cd 4#24373,... 4 more fields] parquet\n",
      "               +- Project [properties#24131.BG10 AS BG10#24567, properties#24131.BG10FIP10 AS BG10FIP10#24568, properties#24131.BG12 AS BG12#24569, properties#24131.CB10 AS CB10#24570, properties#24131.CEN_FIP13 AS CEN_FIP13#24571, properties#24131.CITY AS CITY#24572, properties#24131.CITYCOM AS CITYCOM#24573, properties#24131.COMM AS COMM#24574, properties#24131.CT10 AS CT10#24575, properties#24131.CT12 AS CT12#24576, properties#24131.CTCB10 AS CTCB10#24577, properties#24131.HD_2012 AS HD_2012#24578L, properties#24131.HD_NAME AS HD_NAME#24579, properties#24131.HOUSING10 AS HOUSING10#24580L, properties#24131.LA_FIP10 AS LA_FIP10#24581, properties#24131.OBJECTID AS OBJECTID#24582L, properties#24131.POP_2010 AS POP_2010#24583L, properties#24131.PUMA10 AS PUMA10#24584, properties#24131.SPA_2012 AS SPA_2012#24585L, properties#24131.SPA_NAME AS SPA_NAME#24586, properties#24131.SUP_DIST AS SUP_DIST#24587, properties#24131.SUP_LABEL AS SUP_LABEL#24588, properties#24131.ShapeSTArea AS ShapeSTArea#24589, properties#24131.ShapeSTLength AS ShapeSTLength#24590, ... 2 more fields]\n",
      "                  +- Project [features#24127.geometry AS geometry#24130, features#24127.properties AS properties#24131, features#24127.type AS type#24132]\n",
      "                     +- Project [features#24127]\n",
      "                        +- Generate explode(features#24564), false, [features#24127]\n",
      "                           +- Relation [crs#24563,features#24564,name#24565,type#24566] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Average Income: double, Crimes per Person: double\n",
      "Project [COMM#24143 AS COMM#24609, Average Income#24345, Crimes per Person#24558]\n",
      "+- Join Inner, (COMM#24143 = COMM#24574)\n",
      "   :- Project [COMM#24143, total_population#24339L, comm_total_income#24341, (comm_total_income#24341 / cast(total_population#24339L as double)) AS Average Income#24345]\n",
      "   :  +- Aggregate [COMM#24143], [COMM#24143, sum(POP_2010#24278L) AS total_population#24339L, sum(total_income#24323) AS comm_total_income#24341]\n",
      "   :     +- Project [Zip Code#24300, Community#24301, Estimated Median Income#24306, COMM#24143, ZCTA10#24160, POP_2010#24278L, (Estimated Median Income#24306 * cast(POP_2010#24278L as float)) AS total_income#24323]\n",
      "   :        +- Join Inner, (ZCTA10#24160 = Zip Code#24300)\n",
      "   :           :- Project [Zip Code#24300, Community#24301, cast(regexp_replace(Estimated Median Income#24302, [$,], , 1) as float) AS Estimated Median Income#24306]\n",
      "   :           :  +- Relation [Zip Code#24300,Community#24301,Estimated Median Income#24302] csv\n",
      "   :           +- Aggregate [COMM#24143, ZCTA10#24160], [COMM#24143, ZCTA10#24160, sum(POP_2010#24270L) AS POP_2010#24278L]\n",
      "   :              +- Project [COMM#24143, coalesce(POP_2010#24152L, cast(0 as bigint)) AS POP_2010#24270L, ZCTA10#24160]\n",
      "   :                 +- Project [COMM#24143, POP_2010#24152L, ZCTA10#24160]\n",
      "   :                    +- Project [properties#24131.BG10 AS BG10#24136, properties#24131.BG10FIP10 AS BG10FIP10#24137, properties#24131.BG12 AS BG12#24138, properties#24131.CB10 AS CB10#24139, properties#24131.CEN_FIP13 AS CEN_FIP13#24140, properties#24131.CITY AS CITY#24141, properties#24131.CITYCOM AS CITYCOM#24142, properties#24131.COMM AS COMM#24143, properties#24131.CT10 AS CT10#24144, properties#24131.CT12 AS CT12#24145, properties#24131.CTCB10 AS CTCB10#24146, properties#24131.HD_2012 AS HD_2012#24147L, properties#24131.HD_NAME AS HD_NAME#24148, properties#24131.HOUSING10 AS HOUSING10#24149L, properties#24131.LA_FIP10 AS LA_FIP10#24150, properties#24131.OBJECTID AS OBJECTID#24151L, properties#24131.POP_2010 AS POP_2010#24152L, properties#24131.PUMA10 AS PUMA10#24153, properties#24131.SPA_2012 AS SPA_2012#24154L, properties#24131.SPA_NAME AS SPA_NAME#24155, properties#24131.SUP_DIST AS SUP_DIST#24156, properties#24131.SUP_LABEL AS SUP_LABEL#24157, properties#24131.ShapeSTArea AS ShapeSTArea#24158, properties#24131.ShapeSTLength AS ShapeSTLength#24159, ... 2 more fields]\n",
      "   :                       +- Project [features#24127.geometry AS geometry#24130, features#24127.properties AS properties#24131, features#24127.type AS type#24132]\n",
      "   :                          +- Project [features#24127]\n",
      "   :                             +- Generate explode(features#24119), false, [features#24127]\n",
      "   :                                +- Relation [crs#24118,features#24119,name#24120,type#24121] geojson\n",
      "   +- Project [COMM#24574, TotalPopulation#24552L, NumberOfCrimes#24554L, (cast(NumberOfCrimes#24554L as double) / cast(TotalPopulation#24552L as double)) AS Crimes per Person#24558]\n",
      "      +- Aggregate [COMM#24574], [COMM#24574, sum(POP_2010#24583L) AS TotalPopulation#24552L, count(1) AS NumberOfCrimes#24554L]\n",
      "         +- Project [COMM#24574, POP_2010#24583L]\n",
      "            +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "               :- Project [DR_NO#24350, Date Rptd#24351, DATE OCC#24352, TIME OCC#24353, AREA #24354, AREA NAME#24355, Rpt Dist No#24356, Part 1-2#24357, Crm Cd#24358, Crm Cd Desc#24359, Mocodes#24360, Vict Age#24361, Vict Sex#24362, Vict Descent#24363, Premis Cd#24364, Premis Desc#24365, Weapon Used Cd#24366, Weapon Desc#24367, Status#24368, Status Desc#24369, Crm Cd 1#24370, Crm Cd 2#24371, Crm Cd 3#24372, Crm Cd 4#24373, ... 5 more fields]\n",
      "               :  +- Relation [DR_NO#24350,Date Rptd#24351,DATE OCC#24352,TIME OCC#24353,AREA #24354,AREA NAME#24355,Rpt Dist No#24356,Part 1-2#24357,Crm Cd#24358,Crm Cd Desc#24359,Mocodes#24360,Vict Age#24361,Vict Sex#24362,Vict Descent#24363,Premis Cd#24364,Premis Desc#24365,Weapon Used Cd#24366,Weapon Desc#24367,Status#24368,Status Desc#24369,Crm Cd 1#24370,Crm Cd 2#24371,Crm Cd 3#24372,Crm Cd 4#24373,... 4 more fields] parquet\n",
      "               +- Project [properties#24131.BG10 AS BG10#24567, properties#24131.BG10FIP10 AS BG10FIP10#24568, properties#24131.BG12 AS BG12#24569, properties#24131.CB10 AS CB10#24570, properties#24131.CEN_FIP13 AS CEN_FIP13#24571, properties#24131.CITY AS CITY#24572, properties#24131.CITYCOM AS CITYCOM#24573, properties#24131.COMM AS COMM#24574, properties#24131.CT10 AS CT10#24575, properties#24131.CT12 AS CT12#24576, properties#24131.CTCB10 AS CTCB10#24577, properties#24131.HD_2012 AS HD_2012#24578L, properties#24131.HD_NAME AS HD_NAME#24579, properties#24131.HOUSING10 AS HOUSING10#24580L, properties#24131.LA_FIP10 AS LA_FIP10#24581, properties#24131.OBJECTID AS OBJECTID#24582L, properties#24131.POP_2010 AS POP_2010#24583L, properties#24131.PUMA10 AS PUMA10#24584, properties#24131.SPA_2012 AS SPA_2012#24585L, properties#24131.SPA_NAME AS SPA_NAME#24586, properties#24131.SUP_DIST AS SUP_DIST#24587, properties#24131.SUP_LABEL AS SUP_LABEL#24588, properties#24131.ShapeSTArea AS ShapeSTArea#24589, properties#24131.ShapeSTLength AS ShapeSTLength#24590, ... 2 more fields]\n",
      "                  +- Project [features#24127.geometry AS geometry#24130, features#24127.properties AS properties#24131, features#24127.type AS type#24132]\n",
      "                     +- Project [features#24127]\n",
      "                        +- Generate explode(features#24564), false, [features#24127]\n",
      "                           +- Relation [crs#24563,features#24564,name#24565,type#24566] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#24143, Average Income#24345, Crimes per Person#24558]\n",
      "+- Join Inner, (COMM#24143 = COMM#24574)\n",
      "   :- Aggregate [COMM#24143], [COMM#24143, (sum(total_income#24323) / cast(sum(POP_2010#24278L) as double)) AS Average Income#24345]\n",
      "   :  +- Project [COMM#24143, POP_2010#24278L, (Estimated Median Income#24306 * cast(POP_2010#24278L as float)) AS total_income#24323]\n",
      "   :     +- Join Inner, (ZCTA10#24160 = Zip Code#24300)\n",
      "   :        :- Project [Zip Code#24300, cast(regexp_replace(Estimated Median Income#24302, [$,], , 1) as float) AS Estimated Median Income#24306]\n",
      "   :        :  +- Filter isnotnull(Zip Code#24300)\n",
      "   :        :     +- Relation [Zip Code#24300,Community#24301,Estimated Median Income#24302] csv\n",
      "   :        +- Aggregate [COMM#24143, ZCTA10#24160], [COMM#24143, ZCTA10#24160, sum(POP_2010#24270L) AS POP_2010#24278L]\n",
      "   :           +- Project [features#24127.properties.COMM AS COMM#24143, coalesce(features#24127.properties.POP_2010, 0) AS POP_2010#24270L, features#24127.properties.ZCTA10 AS ZCTA10#24160]\n",
      "   :              +- Filter (isnotnull(features#24127.properties.ZCTA10) AND isnotnull(features#24127.properties.COMM))\n",
      "   :                 +- Generate explode(features#24119), [0], false, [features#24127]\n",
      "   :                    +- Project [features#24119]\n",
      "   :                       +- Filter ((size(features#24119, true) > 0) AND isnotnull(features#24119))\n",
      "   :                          +- Relation [crs#24118,features#24119,name#24120,type#24121] geojson\n",
      "   +- Aggregate [COMM#24574], [COMM#24574, (cast(count(1) as double) / cast(sum(POP_2010#24583L) as double)) AS Crimes per Person#24558]\n",
      "      +- Project [COMM#24574, POP_2010#24583L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#24406]\n",
      "            :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "            :     +- Relation [DR_NO#24350,Date Rptd#24351,DATE OCC#24352,TIME OCC#24353,AREA #24354,AREA NAME#24355,Rpt Dist No#24356,Part 1-2#24357,Crm Cd#24358,Crm Cd Desc#24359,Mocodes#24360,Vict Age#24361,Vict Sex#24362,Vict Descent#24363,Premis Cd#24364,Premis Desc#24365,Weapon Used Cd#24366,Weapon Desc#24367,Status#24368,Status Desc#24369,Crm Cd 1#24370,Crm Cd 2#24371,Crm Cd 3#24372,Crm Cd 4#24373,... 4 more fields] parquet\n",
      "            +- Project [features#24127.properties.COMM AS COMM#24574, features#24127.properties.POP_2010 AS POP_2010#24583L, features#24127.geometry AS geometry#24130]\n",
      "               +- Filter ((isnotnull(features#24127.geometry) AND isnotnull(features#24127.properties.COMM)) AND bloomfilter#24676 of [COMM#24143] filtering [features#24127.properties.COMM])\n",
      "                  :  +- Aggregate [COMM#24143, ZCTA10#24160], [COMM#24143, ZCTA10#24160, sum(POP_2010#24270L) AS POP_2010#24278L]\n",
      "                  :     +- Project [features#24127.properties.COMM AS COMM#24143, coalesce(features#24127.properties.POP_2010, 0) AS POP_2010#24270L, features#24127.properties.ZCTA10 AS ZCTA10#24160]\n",
      "                  :        +- Filter (isnotnull(features#24127.properties.ZCTA10) AND isnotnull(features#24127.properties.COMM))\n",
      "                  :           +- Generate explode(features#24119), [0], false, [features#24127]\n",
      "                  :              +- Project [features#24119]\n",
      "                  :                 +- Filter ((size(features#24119, true) > 0) AND isnotnull(features#24119))\n",
      "                  :                    +- Relation [crs#24118,features#24119,name#24120,type#24121] geojson\n",
      "                  +- Generate explode(features#24564), [0], false, [features#24127]\n",
      "                     +- Project [features#24564]\n",
      "                        +- Filter ((size(features#24564, true) > 0) AND isnotnull(features#24564))\n",
      "                           +- Relation [crs#24563,features#24564,name#24565,type#24566] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#24143, Average Income#24345, Crimes per Person#24558]\n",
      "   +- SortMergeJoin [COMM#24143], [COMM#24574], Inner\n",
      "      :- Sort [COMM#24143 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#24143], functions=[sum(total_income#24323), sum(POP_2010#24278L)], output=[COMM#24143, Average Income#24345], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#24143, 1000), ENSURE_REQUIREMENTS, [plan_id=5716]\n",
      "      :        +- HashAggregate(keys=[COMM#24143], functions=[partial_sum(total_income#24323), partial_sum(POP_2010#24278L)], output=[COMM#24143, sum#24624, sum#24626L], schema specialized)\n",
      "      :           +- Project [COMM#24143, POP_2010#24278L, (Estimated Median Income#24306 * cast(POP_2010#24278L as float)) AS total_income#24323]\n",
      "      :              +- BroadcastHashJoin [Zip Code#24300], [ZCTA10#24160], Inner, BuildLeft, false\n",
      "      :                 :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=5711]\n",
      "      :                 :  +- Project [Zip Code#24300, cast(regexp_replace(Estimated Median Income#24302, [$,], , 1) as float) AS Estimated Median Income#24306]\n",
      "      :                 :     +- Filter isnotnull(Zip Code#24300)\n",
      "      :                 :        +- FileScan csv [Zip Code#24300,Estimated Median Income#24302] Batched: false, DataFilters: [isnotnull(Zip Code#24300)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "      :                 +- HashAggregate(keys=[COMM#24143, ZCTA10#24160], functions=[sum(POP_2010#24270L)], output=[COMM#24143, ZCTA10#24160, POP_2010#24278L], schema specialized)\n",
      "      :                    +- Exchange hashpartitioning(COMM#24143, ZCTA10#24160, 1000), ENSURE_REQUIREMENTS, [plan_id=5708]\n",
      "      :                       +- HashAggregate(keys=[COMM#24143, ZCTA10#24160], functions=[partial_sum(POP_2010#24270L)], output=[COMM#24143, ZCTA10#24160, sum#24628L], schema specialized)\n",
      "      :                          +- Project [features#24127.properties.COMM AS COMM#24143, coalesce(features#24127.properties.POP_2010, 0) AS POP_2010#24270L, features#24127.properties.ZCTA10 AS ZCTA10#24160]\n",
      "      :                             +- Filter (isnotnull(features#24127.properties.ZCTA10) AND isnotnull(features#24127.properties.COMM))\n",
      "      :                                +- Generate explode(features#24119), false, [features#24127]\n",
      "      :                                   +- Filter ((size(features#24119, true) > 0) AND isnotnull(features#24119))\n",
      "      :                                      +- FileScan geojson [features#24119] Batched: false, DataFilters: [(size(features#24119, true) > 0), isnotnull(features#24119)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- Sort [COMM#24574 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#24574], functions=[count(1), sum(POP_2010#24583L)], output=[COMM#24574, Crimes per Person#24558], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#24574, 1000), ENSURE_REQUIREMENTS, [plan_id=5804]\n",
      "               +- HashAggregate(keys=[COMM#24574], functions=[partial_count(1), partial_sum(POP_2010#24583L)], output=[COMM#24574, count#24630L, sum#24632L], schema specialized)\n",
      "                  +- Project [COMM#24574, POP_2010#24583L]\n",
      "                     +- RangeJoin point#24406: geometry, geometry#24130: geometry, WITHIN\n",
      "                        :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS point#24406]\n",
      "                        :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "                        :     +- FileScan parquet [LAT#24376,LON#24377] Batched: true, DataFilters: [isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                        +- Project [features#24127.properties.COMM AS COMM#24574, features#24127.properties.POP_2010 AS POP_2010#24583L, features#24127.geometry AS geometry#24130]\n",
      "                           +- Filter ((isnotnull(features#24127.geometry) AND isnotnull(features#24127.properties.COMM)) AND bloomfilter#24676 of [bf24676 COMM#24143 estimatedNumRows=294857] filtering [features#24127.properties.COMM])\n",
      "                              :  +- GenerateBloomFilter bf24676, 294857, 0, false, false, false, true, [id=#5798]\n",
      "                              :     +- OutputAdapter [COMM#24143, ZCTA10#24160, sum#24628L]\n",
      "                              :        +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                              :           +- Exchange hashpartitioning(COMM#24143, ZCTA10#24160, 1000), ENSURE_REQUIREMENTS, [plan_id=5795]\n",
      "                              :              +- HashAggregate(keys=[COMM#24143, ZCTA10#24160], functions=[partial_sum(POP_2010#24270L)], output=[COMM#24143, ZCTA10#24160, sum#24628L], schema specialized)\n",
      "                              :                 +- Project [features#24127.properties.COMM AS COMM#24143, coalesce(features#24127.properties.POP_2010, 0) AS POP_2010#24270L, features#24127.properties.ZCTA10 AS ZCTA10#24160]\n",
      "                              :                    +- Filter (isnotnull(features#24127.properties.ZCTA10) AND isnotnull(features#24127.properties.COMM))\n",
      "                              :                       +- Generate explode(features#24119), false, [features#24127]\n",
      "                              :                          +- Filter ((size(features#24119, true) > 0) AND isnotnull(features#24119))\n",
      "                              :                             +- FileScan geojson [features#24119] Batched: false, DataFilters: [(size(features#24119, true) > 0), isnotnull(features#24119)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "                              +- Generate explode(features#24564), false, [features#24127]\n",
      "                                 +- Filter ((size(features#24564, true) > 0) AND isnotnull(features#24564))\n",
      "                                    +- FileScan geojson [features#24564] Batched: false, DataFilters: [(size(features#24564, true) > 0), isnotnull(features#24564)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:..."
     ]
    }
   ],
   "source": [
    "res.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8899b63d-5913-44ab-bcb2-c5debeecf66b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White| 90|\n",
      "|Hispanic/Latin/Me...| 76|\n",
      "|             Unknown| 51|\n",
      "|               Other| 40|\n",
      "|               Black| 33|\n",
      "|                NULL| 28|\n",
      "|         Other Asian|  6|\n",
      "|             Chinese|  1|\n",
      "|            Filipino|  1|\n",
      "|          Vietnamese|  1|\n",
      "|            Japanese|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 41.66 seconds"
     ]
    }
   ],
   "source": [
    "# query 4 conf 1\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Santa Clarita\",\"Lancaster\",\"Rancho Palos Verdes\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "    \n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 4 conf 2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Santa Clarita\",\"Lancaster\",\"Rancho Palos Verdes\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "    \n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c56370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 4 conf 2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .appName(\"Query 4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "    \n",
    "start_time=time.time()\n",
    "\n",
    "#geojson handling\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "#the communities we want\n",
    "target_comm_values = [\"Santa Clarita\",\"Lancaster\",\"Rancho Palos Verdes\"]\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\\\n",
    "            .filter(F.col(\"COMM\").isin(target_comm_values))\n",
    "\n",
    "#Crime data handling in the areas of interest\n",
    "dataframe = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .select(\"Vict Descent\",\"LAT\",\"LON\")\n",
    "    \n",
    "#Creation of geometry type column\n",
    "dataframe = dataframe\\\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\",\"LAT\"))\n",
    "\n",
    "#keep data related to the specific areas\n",
    "dataframe = dataframe\\\n",
    "    .join(flattened_df, ST_Within(dataframe.geom, flattened_df.geometry), \"inner\")\\\n",
    "    .select(\"Vict Descent\")\n",
    "\n",
    "#Ethnic-Race dataframe handling\n",
    "ethnic_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Vict Descent right\")\n",
    "\n",
    "#Conversion of ethnic codes to full Name of Race and count for each group the number of victs\n",
    "result_df = dataframe \\\n",
    "    .join(ethnic_df, dataframe[\"Vict Descent\"] == ethnic_df[\"Vict Descent right\"], how=\"left\")\\\n",
    "    .withColumn(\"Vict Descent\", F.col(\"Vict Descent Full\"))\\\n",
    "    .select(\"Vict Descent\")\\\n",
    "    .withColumnRenamed(\"Vict Descent\", \"Victim Descent\")\\\n",
    "    .groupBy(\"Victim Descent\").agg(F.count(\"*\").alias(\"#\"))\\\n",
    "    .orderBy(\"#\",ascending=False)\n",
    "    \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e7c4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----+\n",
      "|        Division|      avg_distance|   #|\n",
      "+----------------+------------------+----+\n",
      "|         TOPANGA| 3097.041046676431|7199|\n",
      "| NORTH HOLLYWOOD|2789.6757914709897|7174|\n",
      "|WEST LOS ANGELES|2988.6934728031933|7101|\n",
      "|      HOLLENBECK| 5051.412468047077|7014|\n",
      "|         PACIFIC| 2644.958011210727|6455|\n",
      "|        WILSHIRE|2280.9372680604174|6421|\n",
      "|        VAN NUYS| 2496.780848087917|6378|\n",
      "|     WEST VALLEY|3009.1054804543173|5865|\n",
      "|          HARBOR|2801.4499340320704|5583|\n",
      "|         MISSION|2408.4567916898773|5328|\n",
      "|        FOOTHILL|   2302.8236985538|5265|\n",
      "|      DEVONSHIRE| 2532.616141824519|4594|\n",
      "|       SOUTHWEST| 1832.859973967432|4578|\n",
      "|       HOLLYWOOD|2735.1101302923275|3858|\n",
      "|         OLYMPIC| 1653.605771356625|3815|\n",
      "|       SOUTHEAST|2353.4705692131356|3691|\n",
      "|       NORTHEAST| 3697.289880077716|3590|\n",
      "|         RAMPART|1320.5912012440954|3058|\n",
      "|     77TH STREET|1332.6590954181906|2927|\n",
      "|          NEWTON|1547.3572308802582|2868|\n",
      "+----------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 174.86 seconds"
     ]
    }
   ],
   "source": [
    "# query 5 conf 1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .appName(\"Query 5\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#Precint data handling\n",
    "precincts_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True)\\\n",
    "    .withColumnRenamed(\"DIVISION\",\"Division\")\\\n",
    "    .withColumn(\"geom\", ST_Point(F.col(\"Y\"),F.col(\"X\")))\\\n",
    "    .select(\"Division\",\"geom\")\n",
    "\n",
    "\n",
    "#Crime data handling\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group35/main_dataset_parquet\") \\\n",
    "    .withColumn(\"crime_geom\", ST_Point(F.col(\"LAT\"),F.col(\"LON\")))\\\n",
    "    .select(\"crime_geom\")\n",
    "\n",
    "#Calculating the distance of each case with each precinct\n",
    "joined_df = precincts_df.crossJoin(crime_df) \\\n",
    "    .withColumn(\"distance\", ST_DistanceSphere(col(\"geom\"), F.col(\"crime_geom\")))\n",
    "\n",
    "#Finding the closest precinct to each crime\n",
    "window_spec = Window.partitionBy(\"crime_geom\").orderBy(F.col(\"distance\"))\n",
    "\n",
    "closest_division_df = joined_df.withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"Division\", \"crime_geom\", \"distance\") \n",
    "    \n",
    "result_df = closest_division_df.groupBy(\"Division\") \\\n",
    "    .agg(\n",
    "        F.avg(\"distance\").alias(\"avg_distance\"),\n",
    "        F.count(\"crime_geom\").alias(\"#\"),\n",
    "    )\\\n",
    "    .orderBy(\"#\",ascending=False)    \n",
    " \n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05781cc-5230-44b0-b848-9cf78b72f0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
